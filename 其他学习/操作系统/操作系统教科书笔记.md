# 进程的描述与控制

## 进程的描述

### 进程的定义

进程由程序段、相关数据段、PCB三部分组成一个进程实体。将这个实体简称为进程。

### 进程的基本状态以及互相转换

进程具有就绪态(Ready)，运行态(Running)，阻塞态(Block)三种状态。

- 就绪态：指进程已经得到了除了CPU外所有必要的资源，等待获得CPU后开始运行。系统中若有多个处于就绪态的进程，则还需要将他们按照一定规律排成一个队列。队列是就绪队列。
- 运行态：从就绪态分得CPU后进入运行态，进程处于执行状态。当分配到的CPU时间片完成后，重新回到就绪态。
- 阻塞态：在执行态时进程发生IO请求或者申请缓冲区失败等情况时，进入阻塞态。当请求完成或者错误处理后，回到就绪态。

考虑得更加周全一点，还可以加入创建态和终止态。

- 创建态：指进程创建指令发出后，建立PCB，申请相关资源的过程。当创建态的进程被操作系统调度器许可执行了，那么就可以进入就绪态了。
- 终止态：同样的，进程在运行态运行完成后，还需要一些额外的时间用来回收资源，清空PCB等。这个过程是终止态。

### 挂起操作和进程转换

有时进程的外部，比如用户或者父进程等，会想让进程暂时停止不要继续运行。此时可以发起挂起操作。此时进程若处于运行态，则会暂停运行；若处于就绪态，则其不会进入就绪队列接受调度。挂起的逆向操作是激活。

引入挂起操作之后，在三态互相转换的关系中，可以新加入额外的两个状态：

- 静止就绪态：将就绪态或者运行态的进程挂起后，进程进入静止就绪态。激活静止就绪态的进程会进入（活动）就绪态。
- 静止阻塞态：（活动）阻塞态通过挂起得到静止阻塞态，反过来激活静止阻塞态得到（活动）阻塞态。

### 进程管理的数据结构

操作系统中为了统一管理各种资源，会有一个类似于管理中心的地方。这个地方又被分成了好几块用于管理不同的资源。通常包括了内存表、设备表、文件表和进程表。进程表中维护了各个进程的信息，而这个信息，具体来说就是进程控制块（PCB）。操作系统通过PCB感知进程并指挥进程、与进程联系。

一个进程的PCB主要包含了下列内容：

- 进程标识符
-  处理器状态，也称为处理器上下文。基本上保存的是各种寄存器在进程当前进度的中间值。这些寄存器包括了通用寄存器、指令计数器、程序状态字、用户栈指针等。正因为有了这些信息，才能保证进程可以在下一次运行时从断点续上。
- 进程调度信息，比如进程状态，进程优先级，事件（进程阻塞原因）等
- 进程控制信息，比如程序和数据段的地址，进程同步机制，资源清单等

### PCB在进程表中的组织方式

- 顺序表：最简单的办法，将所有PCB表按顺序放在一个列表中。这样的话每次搜索PCB都可能扫描全表。
- 链表：进程状态中提到，进程可能有很多种状态，某个状态可能还有个队列。要把这方面信息也维护在进程表中，就需要更复杂一点的数据结构了。一种方案是链接，具体来说，进程表中维护各个队列的队首指针，指向某个PCB。而某个PCB会指向队列中的下一个PCB。PCB们本身可以还是存在一个线性表中，但是从进程表的不同队列指针出发，就可以区分处于不同状态的进程。
- 索引：进程表中还可以根据不同队列，直接维护队列中各个PCB的指针。这样就不需要PCB之间的互相连接了。

## 进程控制

### OS内核

为了防止重要信息被破坏以及提升运行效率，OS会将一些与硬件紧密相关的模块或者常用的模块安排在靠近硬件的层次。这些程序及其数据常驻内存中，称为OS内核。

内核主要有下列功能：

- 中断处理、时钟管理、原语操作等支撑功能
- 进程管理、存储器管理、设备管理、等资源管理功能

光区分了内核还不够，为了让其他进程不要破坏内核的程序以及数据，CPU运行时被分为内核态和用户态。处于内核态运行的程序有最高权限，可以访问硬件、内核所属的内存等。而用户态的程序只允许访问部分内存，不允许直接访问硬件。

## 进程同步

### 进程同步的基本概念

注意区分进程同步和进程通信这两个概念的区别。

进程同步是指一种机制，其对多个进程在执行次序上进行协调，从而使其可以以一定规律有序地共享系统资源。

#### 制约关系

两个进程之间若存在制约关系，则说明他们不能自由地同时运行。这种制约关系分成间接制约和直接制约。间接制约是指A和B两进程需要互斥地访问同一个资源，这就导致两者不能同时运行。直接制约指B的运行需要A的输入，则两者不能同时运行。

#### 临界资源与临界区

多个进程对某个硬件资源（打印机、磁带机）或者软件资源（某个变量等）需要互斥地进行访问，且访问时不允许别的进程进行抢占。这些资源称之为临界资源。一个进程的代码中，需要访问临界资源的部分称之为临界区。

所谓的同步，就是指限制所有进程中，只有一个进程能进入临界区。

### 信号量机制

实现同步的方式有硬件也有软件的。信号量就是一个通过软件实现同步的典型代表。

最早信号量是一个整型值（也称整型信号量），用于抽象系统中某些特定的可供分配的资源总数。进程需要用到资源时，通过对信号量`-1`来实现对资源的获取。显然当资源耗尽时信号量小于等于0，此时若再有进程想要申请资源，只能被阻塞或者等待。进程执行完成后，让信号量`+1`返还相关资源。

上面提到的让信号量`-1`和`+1`的操作，都是原子操作。因此保证了多进程对信号量操作的互斥性。

若信号量的初始化值是1，即全局只有一个资源的情况，这也被称为互斥信号量。

#### AND型信号量

很多时候，进程可能需要多个资源都到位之后才能运行。此时如果给每个资源都分配一个信号量的话，很可能导致死锁。一个解决办法是，规定所有资源必须全部满足可获取状态，才获取资源开始运行，并且修改信号量。运行完成后又全部一起返还资源。这样，多个资源就可以视为一个资源，一起获取一起返还。

这种信号量称为AND信号量。



信号量还有其他一些变种比如信号量集。此外还有将信号量以及其他一些必要的数据给包装成一个数据结构的管程等。这里就不多说了

## 进程通信

在保证进程的同步和互斥中，理论上进程已经可以通过诸如信号量等中介进行通信了。但是这种通信的效率比较低。而高效率的进程间通信主要有以下几种：

### 进程间通信类型

- 共享内存：在内存中划一块专用于共享内存通信的地盘。各进程都可以申请到一种一部分，将其附加到进程自身内存中，然后对其进行读写。读写完成后归还其到共享内存。共享内存是最快的进程间通信方法。
- 管道：管道本质上是个文件，连接了一个读进程和一个写进程进行通信。两个进程之间互斥地读写管道内容。管道还可细分为无名管道（内存中的文件，仅供父子进程间通信）和有名管道（FIFO文件，供任意进程间通信）。
- 消息传递系统
  - 消息队列
- 客户端-服务端系统
  - 套接字 （可基于网络和文件）
  - 远程过程、方法调用

## 线程

在OS中引入进程，以进程为资源分配和调度运行的基本单位，解决了多个程序并发运行的问题。而引入线程，是为了进一步减少多程序并发时的时空开销的问题。可以说这两个引入，一个是从0到1，一个是从1到更好。

一个进程，从创建，到运行时进行CPU的上下文切换，到销毁时回收资源，都会发生时空损耗。从纯功利主义的角度来所，这些都属于不必要的额外的损耗。为了降低这种损耗，引入了线程这种更加轻量级的概念。

### 线程与进程比较

- 进程和线程都可以作为调度的基本单位。但是进程作为调度的基本单位时，在调度时必然要进行CPU上下文切换，很浪费
- 进程和线程都可并发执行
- 进程和线程都可拥有资源，但进程占大头。另一方面，同一个进程内的多个线程，还可以共享进程内比较大的资源比如文件IO，其他内存内容等
- 线程的创建，销毁，切换也都需要系统开销，但是比相应的进程开销小得多。比如Solaris OS中，线程创建速度是进程创建速度的30倍，切换速度是5倍。

### 线程状态和TCB

和进程很类似，线程也有三种主要状态：就绪、运行、阻塞。其互相转换关系和进程也一样。

和进程有PCB类似，线程有TCB，其中内容也很相似，主要包括了：线程标识符、寄存器、运行状态、优先级、线程专有存储区、堆栈指针等。

### 多线程OS中的进程属性

在引入了线程之后，就要对进程的部分概念进行一些修正了（毕竟现在所有计算机都是多线程的了）。

在多线程系统中，进程仍然是可拥有资源的基本单位，特指那些比较大的比如文件、设备等资源。而线程的并发更为广泛应用，传统的进程运行方式已经被称为单线程运行。一个进程至少有一个主线程。

而进程已经不再是可执行的实体。线程才是可独立运行的基本单位。通常说某个进程在执行，是指其某个线程正在被执行。另一方面，对进程的操作会反映到其所有线程上。比如对进程挂起，其所有线程也都挂起。

# CPU调度与死锁

CPU调度其实可以分成好多层级。最高层的有作业调度（Job），中层有内存调度，低层的是进程调度。这里主要聚焦于进程调度的内容。

## 进程调度

进程调度的主要任务有三个：

- 保存CPU的现场信息
- 按照某种算法选取下一个进程
- 将CPU分配给该进程

### 进程调度的基本方式（Mode）

- 非抢占方式：一旦将CPU分配给某个进程后，直到进程完成或者被阻塞等情况时，才将CPU收回分配给其他进程
- 抢占式：将某个正在执行的程序按照特定原则暂停，然后将其CPU收回，分配给其他进程。

### 各类进程调度算法

- 先来先服务算法（FCFS）、短作业优先算法（SJF）

  ```
  这俩算法其实最开始是作业调度层面的算法，但是也可以用于进程调度。需要注意这两种算法都是非抢占式的算法。当一个进程开始处理之后就会运行到其结束为止。
  简单来说，FCFS就是从进程的就绪队列中按序挨个拿出进程，进行执行。执行到其结束后继续处理下一个。是最简单的处理逻辑。
  SJF算法是按照进程预计需要用时进行排序，优先处理预计用时较短的进程。这样可以使总等待时间较小。
  以上两种算法，前者只考虑进程的等待时间，后者只考虑进程的运行时间，都比较极端化。
  ```

- 轮转调度算法：

  ```
  分时系统中最简单的调度算法。采用最公平的分配方式，让就绪队列中每个进程运行一个时间片，然后暂停其运行将CPU分给队列中下一个进程。
  这个算法的关键在于时间片设置成多大。可以想象，如果时间片过小，就会耗费大量时间在切换进程上，而如果时间片过大，所有进程都在一个时间片内完成运行，则算法退化成FCFS算法。
  ```

- 优先级调度算法

  ```
  轮转调度中，假设所有进程的优先级都相同，但是实际上并不是这样。所以我们会考虑将优先级也纳入调度算法的考虑中来。
  此时还可分成非抢占式和抢占式两种方式。非抢占式的算法，一旦将CPU分配给当前最高优先级进程后，即使过程中出现更高优先级的，也不变化。
  相对，抢占式算法在这种情况下会暂停当前进程，转而分配CPU给更高优先级的进程。
  ```

- 多队列调度算法

  ```
  以上所有算法默认都只有一个就绪队列。而多队列算法可以设置有多个就绪队列，每个队列又可以设置不同的算法以及优先度。
  灵活安排，从而可以实现更加高效的调度。
  ```

  多队列调度算法是一个比较泛泛的概念，下面说其中一个最佳实践：

  - 多级反馈队列

    ```
    多级反馈队列算法设置有多个就绪队列。每个队列被分配一个优先级，第一个队列优先级最高，第二次之，以此类推。每个队列内采取轮转调度算法，并且优先级越高的队列，设置的时间片越小。
    当新进程进入内存后，默认将其放入第一个队列中等待执行。轮到他时拿去执行，如果在一个时间片内这个进程没能执行完成，则暂停执行并将此进程放入第二个队列的队尾，等待执行。以此类推，直到设置的最后一个队列。最后一个队列以普通的轮转调度算法执行。
    对于不同队列间的调度，采取优先级调度算法。即，只有第一队列中目前是空的，才去第二队列队首取进程执行，以此类推。
    ```

    

## 死锁

死锁总是发生在对临界资源的使用时。临界资源，定义是需要互斥访问，并且不可以被抢占的资源，比如打印机、队列、信号量等。

最经典的死锁的模式，是A和B分别对X和Y两个临界资源发起请求。不同的是A以XY的顺序发起，B以YX的顺序发起，导致互相等对方释放前一个资源，从而死锁。上述模式也可以总结称为 **“竞争不可抢占性资源引起死锁”**。

另一种死锁的模式是，加入A和B两个进程分别要求某种相同的资源各三个。然而这种资源系统里总共就只有4个。如果AB同时发出资源申请，很可能会出现，A获得两个，B获得两个的情况。此时两者都还没有满足获得三个资源的条件，因此不会运行，然而系统资源也没了，互相之间就死锁了。这种情况下，本应想定资源即使不够，也应该至少满足一个进程的运行，这样等进程运行结束后让出了资源可供另一个进程使用。这种模式成为 **“竞争消耗性资源引起死锁”**。

死锁的定义: 一组进程中，每个进程都在等待由改组进程中其他进程才能引发的事件重回就绪态，那么这组进程是死锁的。

### 产生死锁的必要条件

1. 互斥条件

   ```
   进程对所分配的资源进行排他性的使用。
   ```

2. 请求和保持条件

   ```
   进程在使用到某个排他性资源后，要继续请求另一个资源，且这个过程中，对已有的排他性资源进行保持不放。
   ```

3. 不可抢占条件

   ```
   上述已经使用的排他性资源，不能被其他进程抢占。
   ```

4. 循环等待条件

   ```
   发生死锁时，若将资源、进程之间的互相关系以有向图的形式画出来，必然存在一个循环链。
   ```

### 预防死锁

预防死锁，是指一切程序开始运行前，通过追加一些限制条件，破坏死锁四个必要条件中的一个或几个，从而预防运行过程中产生死锁的情况。针对四个条件不同，有不同的预防策略如：

- 破坏“请求和保持条件”

  ```
  破坏请求和保持条件，实际上就是不允许进程在请求新资源的同时持有不可抢占的资源。解决这个问题有两种思路，
  第一，要求进程在开始运行前申请好全部需要的资源，否则不允许运行。这样就不会有持有一些资源，申请另一些资源的情况了；
  第二，进程在申请新资源之前，将已经使用完成的旧资源全部回收处理。
  第二种思路比第一种更高效因为第一种在一开始就要占据全部资源，可能会造成浪费。
  ```

- 破坏不可抢占条件

  ```
  一句话就是允许资源被抢占。准确来说，只有当某个进程尝试申请新资源没有立即成功时，就将其目前持有的资源设置为可抢占。自然，这个资源很快就会被别的进程抢占去。
  ```

- 破坏循环等待条件

  ```
  本质上就是破坏资源-进程图中的环。具体是这么做的：
  对于所有资源，我们将其从小到大编号。每个资源有了编号之后，我们规定，任何进程，都必须先申请较低编号的资源，再申请较高编号的资源。
  若某进程持有较高编号（如12）的资源时，想要申请较低编号（如5）的资源时，其必须先把12号资源给释放，然后再从头申请5号和12号资源。
  有了这个限制，资源-进程图就不会成环。
  ```

### 避免死锁

避免死锁看起来字面意思和预防死锁差不多，两者指两种不同的防止死锁出现的方案。预防死锁更多的是在程序运行前追加限制条件。而避免死锁更多的，是指在程序运行过程中动态地判断某些资源能不能分配，分配了之后会不会引起不安全状态，从而防止死锁出现。

避免死锁更多的是用在 竞争消耗性资源引起的死锁 的解决上。这里首先定义一个概念叫做系统的安全状态：

- 安全状态：指当前进程和系统资源的情况下，可以找到一个进程的排列，使得现有的系统资源可以通过这个排列依次分配资源进行进程运行。期间不会出现死锁直到所有进程结束。相对的，不安全状态就是指一个这样的排列都找不到。

避免死锁的算法称为银行家算法。

#### 银行家算法

简单来说，银行家算法要求每个新进程进入系统时，声明运行过程中需要的各种资源的最大数目。当进程对某资源发起请求时，系统会结合该进程目前持有的资源判断，系统是否有足够的资源满足该进程。只有是的情况，才允许分配资源给进程。

如此就避免了因 竞争消耗性资源而引起的死锁。

银行家算法的具体内容这里就不写了，可以参看书本。

### 死锁的检测

正如前面提到的，验证循环等待条件的方法，就是将当前的进程-资源有向图画出来。然后探索这个图看有没有环就可以知道是否存在死锁了。

> 顺便一提，检测有向图 中是否有环，是一个拓扑排序问题。可以用基于DFS的拓扑排序方法来解决。

### 死锁的解除

解除死锁的方法也很简单。从两个方面考虑，第一，解放出新的足够的资源，从而使得死锁解除。第二，终止进程，打破循环链解除。

对于后者，还可选择一次性终止整个死锁组的进程，或者一个个终止。一次性终止所有进程，是个很暴力的解决办法。。一个个终止稍微温和一点，但是仍然可能会带来比较大的损失。

# 存储器管理

## 存储器的层次结构

存储器泛指计算机上所有具有数据存储功能的部件。存储器的访问速度与其最大容量、价格之间有一个trade-off，因此不同速容比的访问器也有不同的用途。常见的从访问速度从高到低，最大容量从小到大的顺序有：

```
寄存器（CPU寄存器）
主存（高速缓存、主存储器、磁盘缓存）
辅存（磁盘、可移动存储介质）
```

寄存器和主存在掉电后回丢失其中保存的信息，而辅存的信息是持久化的。另外，CPU寄存器和主存储器也被称为可执行存储器。与辅存等非可执行存储相比，对可执行存储的访问不必通过IO设备，因此比要通过IO设备的辅存访问快3个数量级。

本章主要介绍寄存器、主存这些掉电后丢失信息的存储器。至于磁盘等的管理在后面。

### 各类存储器简介

- 主存储器

  ```
  即内存，CPU从内存中读取指令，并把计算结果输出给内存。
  因为CPU的执行指令的速度比访问内存的速度要快很多，为了调和这个矛盾，又引入了寄存器和高速缓存
  内存嘛，通常大小在几百M到几个G之间。
  ```

- 寄存器

  ```
  寄存器具有和CPU处理同样快的速度，因此访问很快，可以和CPU协调工作。
  寄存器是很小很小的，通常是32、64位。相对的可能会设置很多个寄存器。
  ```

- 高速缓存

  ```
  高速缓存介于寄存器和主存之间。主要用于备份主存中常用的数据，从而提升主存与CPU之间通信的效率。
  高速缓存的大小通常在几十K到几个M间，比寄存器要大很多，但比主存小很多。
  为了进一步提升效率，现在通常都会设有多级的高速缓存，越高级的高速缓存速度快容量小。套娃。
  ```

- 磁盘缓存

  ```
  和高速缓存调和了CPU计算速度与主存访问速度的矛盾一样，磁盘缓存用来调和主存访问速度与磁盘访问速度之间的矛盾。
  需要注意的是，磁盘缓存不是真实的存储器而是一个概念。磁盘缓存通常是在主存中特别划出一部分空间，用来做磁盘缓存，保存一些磁盘过来的信息。
  ```

## 连续分配 存储管理方式

连续分配方式是存储管理方式的一种。其特点是将一片连续的内存空间分配给某个进程的代码和数据。具体的还可分为如下四种算法的分配方式

- 单一连续分配：将整个可以分配的内存分配给一个进程。显然只适用于单用户单任务的操作系统中
- 固定分区分配：将整个可以分配的内存提前按照一定规则划分成数个区域，每个区域针对某用户的某任务进行单一连续分配。虽然支持了多用户多任务系统，但是十分死板。
- 动态分区分配：根据进程的实际需要，可以动态地进行内存分配工作。这个下面会重点介绍。
- 动态可重定位分区分配：当分配碎片过多时，将已经分配空间进行归纳整理，比如将其全部都整理到低址区，从而可以空出大片连续的高址区。然而这么做的问题是已经分配的部分因为绝对地址的改变，导致无法正常读取程序和数据。此时就需要进行一次动态的重定位，修正其绝对地址到新地址上。

### 动态分区分配具体算法

动态分区分配的前提是要知道当前内存还有哪些空间是可以被分配的，于是这就要求我们实时维护一个空闲分区表。这个表记录了所有空闲分区的 1.大小 2.起始地址。

而其具体算法可以分为两类。基于顺序搜索的分配算法和基于索引搜索的分配算法。先来说说基于顺序搜索的：

- 首次适应（FF）算法

  ```
  在空闲分区表从低址到高址依次扫描，碰到的第一个足够容下进程的空闲分区时将其分配给进程。
  需要注意如果空闲分区比进程要求的空间大，那么剩余空间会遗留下来。
  FF算法的特点是每次尝试分配都会从低址开始扫描，因此其倾向于使用低址区域的空闲分区。
  这也导致可能的问题：低址区域的内存会越来越碎片化，且后期找空闲分区会花费较长时间因为低址处无地可用。
  ```

- 循环首次适应（NF）算法

  ```
  为了避免FF中出现的碎片化空间的问题，NF中规定本次扫描是从上次扫描到的空闲分区的下一个分区开始。
  由于不是每次都从头扫描，可以避免碎片化空间过于集中的问题。
  ```

- 最佳适应（BF）算法

  ```
  我们总是选取当前空闲分区中能够满足进程要求且最小的分区分配给进程。
  这么做乍一看好像挺合理的，但是实际上每次都选择最佳适应空闲分区的话，也会有最大的风险不断产生很小的无法利用的碎片空间。
  并且为了实时知晓维护分区的大小比较情况，此算法比较耗费时间。
  ```

- 最坏适应（WF）算法

  ```
  和BF相反，总是选用空闲分区中最大的来分配给进程。
  这种算法可以避免过多的碎片空间，并且每次分配只要看最大的空闲分区即可，所以算法耗时上比较好一些。
  ```

上述四种是基本的，基于顺序搜索的动态分区分配算法。从实践上来说，一般还是最简单直白的首次适应算法具有最佳性能。

基于顺序搜索的动态分区分配算法通常适用于较小系统。当内存较大时，空闲分区表本来就很大，会导致顺序搜索的效率下降。此时用索引搜索是一个更好的办法。下面继续介绍几个基于索引搜索的分配算法。

- 快速适应（Quick Fit）算法

  ```
  事先将所有空闲分区按照大小规格分类，对每类分区单独安排一个空闲分区表。于是就有了一个 规格：分区 的索引表。
  借助这个索引表，对于新来的进程可以快速地为其找到合适的分区并分配。
  ```

- 伙伴系统

  ```
  略
  ```

- 哈希算法

  ```
  哈希和QF也差不多，只不过这里不规定死的大小规格，视内存当前具体情况，把所有空闲分区大小的可能，都用 空闲分区大小：起始地址 类似的形式用一个哈希表整合起来。
  ```

  

## 对换（Swapping）

将内存中暂时不能、不用运行的程序以及其数据换出到外存上，腾出内存空间，从而将已经具备运行条件的程序和数据从外存交换到内存中。

按照对换操作的基本单位，可以将对换分成 整体对换（指一次对换针对一整个进程）或者 部分对换（一次对换只针对一个页面等部分单位）。这章主要介绍的是整体对换。

### 对换空间的管理

在要进行对换的系统中，磁盘上都会划出一块对换区。这片对换区就是用来进行对换的外存空间。由于进入这个区域的内容都不会长久驻留且访问频率很高，因此这个区域应该要实现高效的访问而不是空间利用率的提升。

因此，对换区通常采用 “连续分配方式”（注意，虽然上面介绍过的连续分配方式是针对内存的，但是要知道内存和磁盘都是存储器，本质是一样的，所以可以沿用在这里。

对换，不论换哪个方向，其本质都是从一片存储区域中找到一个合适的区域来存放进程内容。因此之前介绍到的所有动态分区匹配的算法也都适用于此。

## 离散分配 —— 分页存储管理方式

上面提到的存储管理方式主要是连续分配存储管理方式。可以感觉到，这种方式的一个很大的缺点，就是可能会引起很多碎片空间，碎片空间是无法使用的，因此会影响有效空间使用率。

相对的，离散分配的存储管理方式看起来似乎更加灵活。根据离散分配的单位大小不同，将离散分配管理方式大概分为 分页、分段、页段三种。这节主要讲讲分页的管理方式。

### 分页存储管理的基本方法

分页，指一方面将进程的地址空间分成若干个固定页面大小的部分，通常这个大小是1K~8K。另一方面，将内存也分成同样大小的页框，或者叫块。进程的任意一页都可以放到内存的任意一块中去。

在分页体系下，可以通过页号P和偏移量W来确定一个地址。假设页长是L，则有`A = P * L + W`。

另一方面，如果知道了逻辑地址A，也可以快速找到其页号和页内地址：`P = A // L`, `W = A % L`。

分页管理，是一种离散型的管理方式。这也就意味着针对一个进程的每个页，我们都得知道其处于内存的哪个物理块中。为了维护这方面信息，系统给每个进程又开辟了一个页表。页表，就是 一个进程页的页号 对应到 内存中块的块地址 的映射关系。

### 地址变换机构

所谓的地址变换机构就是一个将用户地址空间中的逻辑地址变换成一个内存中的物理地址的模块。在分页进行存储管理的情况下，一个页放进一个内存块之后两者是完全对应的，因此只要找到两者的起址即可。换言之，地址变换机构是一个将进程地址空间内页号转化为内存某个块的块号的模块。这个机构具体的工作其实也很简单，只要知道进程整个页表的起始地址，这样把某个页号转化为物理块号，只要加上这个起始地址就行了。

为了保证快速的寻址，通常采用寄存器作为这个机构的载体。当然寄存器很贵，全局只能安排一个寄存器。于是，进程的页表起始地址和其页表长度，平时只保存在其PCB中，当开始运行时将这两个量加载到地址变换寄存器中。

接下来拿到一个逻辑地址后，只要读取寄存器，取出起始地址，加上逻辑地址，就是物理地址了。当然这过程中还需要校验，逻辑地址不能超过页表长度，否则越界，会访问到不安全的物理地址。这也是为什么要把页表长度也加载到寄存器中的原因。

#### 使用快表寻址

利用页表寻址的过程，系统需要访问两次内存。第一次，访问页表，用逻辑页号计算物理块号。第二次采取访问物理块，得到想要的数据。

而所谓的块表，就是在这个过程中加入一个名为快表的LRU缓存。拿到逻辑页号后，先去缓存里找是否有相关记录记录了其对应的块号。如果有，直接使用即可。没有才再去访问页表查找相应的物理块号。

快表通常是一个寄存器（如果读取速度和内存一样慢，那就没有意义了），因为是寄存器所以很小，通常只能存放512条页号→块号的记录。然而这还挺够用的，因为计算机数据访问的局限性（或者叫二八定律，80%的访问总是落到20%的热门数据上），因此可以加速。因为快表是一个LRU缓存，所以当表满的时候会把最老的没人用过的记录给清除掉。

加入快表之后，寻址会有一个命中率的概念。命中率指，在泛泛的寻址过程中，一个逻辑页号可以再快表中找到对应物理块号的大致概率。代码数据复用率越高，热门数据越热门，命中率也就越高，从而整体CPU访问内存速度会加快很多。

### 多级页表

现在操作系统大多都是32或者64位，说明其中一个进程理论上最大可以支持`2^32 ~ 2^64`字节的地址空间。即使一个内存也设置为4K（即`2^12`），除法做一下，一个32位进程的页表长度仍可能达到1M。

和很多类似情况一样，这种时候可以考虑多级页表。比如第一级页表中保存的并非页号与内存块号之间的逻辑，而是页的页号。此时页有一级页和二级页（分页）。此时每个逻辑页号就有三个部分了：一级页页号，二级页页号以及页内地址。凭借前两个页号就可以定位到一个物理块。这么一来，就把长页表给通过二级结构压缩，检索的时候就快了。

## 分段 和 段页式存储管理方式

具体内容先略了…

总之先记住，所谓的段，本质上和页是差不多的，也是将大块内存以小粒度进行保存，从而避免碎片化等问题。只不过这里的段由程序员自己定义管理；而页是系统自身进行管理，对程序员透明。

