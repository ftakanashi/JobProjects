# 进程的描述与控制

## 进程的描述

### 进程的定义

进程由程序段、相关数据段、PCB三部分组成一个进程实体。将这个实体简称为进程。

### 进程的基本状态以及互相转换

进程具有就绪态(Ready)，运行态(Running)，阻塞态(Block)三种状态。

- 就绪态：指进程已经得到了除了CPU外所有必要的资源，等待获得CPU后开始运行。系统中若有多个处于就绪态的进程，则还需要将他们按照一定规律排成一个队列。队列是就绪队列。
- 运行态：从就绪态分得CPU后进入运行态，进程处于执行状态。当分配到的CPU时间片完成后，重新回到就绪态。
- 阻塞态：在执行态时进程发生IO请求或者申请缓冲区失败等情况时，进入阻塞态。当请求完成或者错误处理后，回到就绪态。

考虑得更加周全一点，还可以加入创建态和终止态。

- 创建态：指进程创建指令发出后，建立PCB，申请相关资源的过程。当创建态的进程被操作系统调度器许可执行了，那么就可以进入就绪态了。
- 终止态：同样的，进程在运行态运行完成后，还需要一些额外的时间用来回收资源，清空PCB等。这个过程是终止态。

### 挂起操作和进程转换

有时进程的外部，比如用户或者父进程等，会想让进程暂时停止不要继续运行。此时可以发起挂起操作。此时进程若处于运行态，则会暂停运行；若处于就绪态，则其不会进入就绪队列接受调度。挂起的逆向操作是激活。

引入挂起操作之后，在三态互相转换的关系中，可以新加入额外的两个状态：

- 静止就绪态：将就绪态或者运行态的进程挂起后，进程进入静止就绪态。激活静止就绪态的进程会进入（活动）就绪态。
- 静止阻塞态：（活动）阻塞态通过挂起得到静止阻塞态，反过来激活静止阻塞态得到（活动）阻塞态。

### 进程管理的数据结构

操作系统中为了统一管理各种资源，会有一个类似于管理中心的地方。这个地方又被分成了好几块用于管理不同的资源。通常包括了内存表、设备表、文件表和进程表。进程表中维护了各个进程的信息，而这个信息，具体来说就是进程控制块（PCB）。操作系统通过PCB感知进程并指挥进程、与进程联系。

一个进程的PCB主要包含了下列内容：

- 进程标识符
-  处理器状态，也称为处理器上下文。基本上保存的是各种寄存器在进程当前进度的中间值。这些寄存器包括了通用寄存器、指令计数器、程序状态字、用户栈指针等。正因为有了这些信息，才能保证进程可以在下一次运行时从断点续上。
- 进程调度信息，比如进程状态，进程优先级，事件（进程阻塞原因）等
- 进程控制信息，比如程序和数据段的地址，进程同步机制，资源清单等

### PCB在进程表中的组织方式

- 顺序表：最简单的办法，将所有PCB表按顺序放在一个列表中。这样的话每次搜索PCB都可能扫描全表。
- 链表：进程状态中提到，进程可能有很多种状态，某个状态可能还有个队列。要把这方面信息也维护在进程表中，就需要更复杂一点的数据结构了。一种方案是链接，具体来说，进程表中维护各个队列的队首指针，指向某个PCB。而某个PCB会指向队列中的下一个PCB。PCB们本身可以还是存在一个线性表中，但是从进程表的不同队列指针出发，就可以区分处于不同状态的进程。
- 索引：进程表中还可以根据不同队列，直接维护队列中各个PCB的指针。这样就不需要PCB之间的互相连接了。

## 进程控制

### OS内核

为了防止重要信息被破坏以及提升运行效率，OS会将一些与硬件紧密相关的模块或者常用的模块安排在靠近硬件的层次。这些程序及其数据常驻内存中，称为OS内核。

内核主要有下列功能：

- 中断处理、时钟管理、原语操作等支撑功能
- 进程管理、存储器管理、设备管理、等资源管理功能

光区分了内核还不够，为了让其他进程不要破坏内核的程序以及数据，CPU运行时被分为内核态和用户态。处于内核态运行的程序有最高权限，可以访问硬件、内核所属的内存等。而用户态的程序只允许访问部分内存，不允许直接访问硬件。

## 进程同步

### 进程同步的基本概念

注意区分进程同步和进程通信这两个概念的区别。

进程同步是指一种机制，其对多个进程在执行次序上进行协调，从而使其可以以一定规律有序地共享系统资源。

#### 制约关系

两个进程之间若存在制约关系，则说明他们不能自由地同时运行。这种制约关系分成间接制约和直接制约。间接制约是指A和B两进程需要互斥地访问同一个资源，这就导致两者不能同时运行。直接制约指B的运行需要A的输入，则两者不能同时运行。

#### 临界资源与临界区

多个进程对某个硬件资源（打印机、磁带机）或者软件资源（某个变量等）需要互斥地进行访问，且访问时不允许别的进程进行抢占。这些资源称之为临界资源。一个进程的代码中，需要访问临界资源的部分称之为临界区。

所谓的同步，就是指限制所有进程中，只有一个进程能进入临界区。

### 信号量机制

实现同步的方式有硬件也有软件的。信号量就是一个通过软件实现同步的典型代表。

最早信号量是一个整型值（也称整型信号量），用于抽象系统中某些特定的可供分配的资源总数。进程需要用到资源时，通过对信号量`-1`来实现对资源的获取。显然当资源耗尽时信号量小于等于0，此时若再有进程想要申请资源，只能被阻塞或者等待。进程执行完成后，让信号量`+1`返还相关资源。

上面提到的让信号量`-1`和`+1`的操作，都是原子操作。因此保证了多进程对信号量操作的互斥性。

若信号量的初始化值是1，即全局只有一个资源的情况，这也被称为互斥信号量。

#### AND型信号量

很多时候，进程可能需要多个资源都到位之后才能运行。此时如果给每个资源都分配一个信号量的话，很可能导致死锁。一个解决办法是，规定所有资源必须全部满足可获取状态，才获取资源开始运行，并且修改信号量。运行完成后又全部一起返还资源。这样，多个资源就可以视为一个资源，一起获取一起返还。

这种信号量称为AND信号量。



信号量还有其他一些变种比如信号量集。此外还有将信号量以及其他一些必要的数据给包装成一个数据结构的管程等。这里就不多说了

## 进程通信

在保证进程的同步和互斥中，理论上进程已经可以通过诸如信号量等中介进行通信了。但是这种通信的效率比较低。而高效率的进程间通信主要有以下几种：

### 进程间通信类型

- 共享内存：在内存中划一块专用于共享内存通信的地盘。各进程都可以申请到一种一部分，将其附加到进程自身内存中，然后对其进行读写。读写完成后归还其到共享内存。共享内存是最快的进程间通信方法。
- 管道：管道本质上是个文件，连接了一个读进程和一个写进程进行通信。两个进程之间互斥地读写管道内容。管道还可细分为无名管道（内存中的文件，仅供父子进程间通信）和有名管道（FIFO文件，供任意进程间通信）。
- 消息传递系统
  - 消息队列
- 客户端-服务端系统
  - 套接字 （可基于网络和文件）
  - 远程过程、方法调用

## 线程

在OS中引入进程，以进程为资源分配和调度运行的基本单位，解决了多个程序并发运行的问题。而引入线程，是为了进一步减少多程序并发时的时空开销的问题。可以说这两个引入，一个是从0到1，一个是从1到更好。

一个进程，从创建，到运行时进行CPU的上下文切换，到销毁时回收资源，都会发生时空损耗。从纯功利主义的角度来所，这些都属于不必要的额外的损耗。为了降低这种损耗，引入了线程这种更加轻量级的概念。

### 线程与进程比较

- 进程和线程都可以作为调度的基本单位。但是进程作为调度的基本单位时，在调度时必然要进行CPU上下文切换，很浪费
- 进程和线程都可并发执行
- 进程和线程都可拥有资源，但进程占大头。另一方面，同一个进程内的多个线程，还可以共享进程内比较大的资源比如文件IO，其他内存内容等
- 线程的创建，销毁，切换也都需要系统开销，但是比相应的进程开销小得多。比如Solaris OS中，线程创建速度是进程创建速度的30倍，切换速度是5倍。

### 线程状态和TCB

和进程很类似，线程也有三种主要状态：就绪、运行、阻塞。其互相转换关系和进程也一样。

和进程有PCB类似，线程有TCB，其中内容也很相似，主要包括了：线程标识符、寄存器、运行状态、优先级、线程专有存储区、堆栈指针等。

### 多线程OS中的进程属性

在引入了线程之后，就要对进程的部分概念进行一些修正了（毕竟现在所有计算机都是多线程的了）。

在多线程系统中，进程仍然是可拥有资源的基本单位，特指那些比较大的比如文件、设备等资源。而线程的并发更为广泛应用，传统的进程运行方式已经被称为单线程运行。一个进程至少有一个主线程。

而进程已经不再是可执行的实体。线程才是可独立运行的基本单位。通常说某个进程在执行，是指其某个线程正在被执行。另一方面，对进程的操作会反映到其所有线程上。比如对进程挂起，其所有线程也都挂起。

# CPU调度与死锁

CPU调度其实可以分成好多层级。最高层的有作业调度（Job），中层有内存调度，低层的是进程调度。这里主要聚焦于进程调度的内容。

## 进程调度

进程调度的主要任务有三个：

- 保存CPU的现场信息
- 按照某种算法选取下一个进程
- 将CPU分配给该进程

### 进程调度的基本方式（Mode）

- 非抢占方式：一旦将CPU分配给某个进程后，直到进程完成或者被阻塞等情况时，才将CPU收回分配给其他进程
- 抢占式：将某个正在执行的程序按照特定原则暂停，然后将其CPU收回，分配给其他进程。

### 各类进程调度算法

- 先来先服务算法（FCFS）、短作业优先算法（SJF）

  ```
  这俩算法其实最开始是作业调度层面的算法，但是也可以用于进程调度。需要注意这两种算法都是非抢占式的算法。当一个进程开始处理之后就会运行到其结束为止。
  简单来说，FCFS就是从进程的就绪队列中按序挨个拿出进程，进行执行。执行到其结束后继续处理下一个。是最简单的处理逻辑。
  SJF算法是按照进程预计需要用时进行排序，优先处理预计用时较短的进程。这样可以使总等待时间较小。
  以上两种算法，前者只考虑进程的等待时间，后者只考虑进程的运行时间，都比较极端化。
  ```

- 轮转调度算法：

  ```
  分时系统中最简单的调度算法。采用最公平的分配方式，让就绪队列中每个进程运行一个时间片，然后暂停其运行将CPU分给队列中下一个进程。
  这个算法的关键在于时间片设置成多大。可以想象，如果时间片过小，就会耗费大量时间在切换进程上，而如果时间片过大，所有进程都在一个时间片内完成运行，则算法退化成FCFS算法。
  ```

- 优先级调度算法

  ```
  轮转调度中，假设所有进程的优先级都相同，但是实际上并不是这样。所以我们会考虑将优先级也纳入调度算法的考虑中来。
  此时还可分成非抢占式和抢占式两种方式。非抢占式的算法，一旦将CPU分配给当前最高优先级进程后，即使过程中出现更高优先级的，也不变化。
  相对，抢占式算法在这种情况下会暂停当前进程，转而分配CPU给更高优先级的进程。
  ```

- 多队列调度算法

  ```
  以上所有算法默认都只有一个就绪队列。而多队列算法可以设置有多个就绪队列，每个队列又可以设置不同的算法以及优先度。
  灵活安排，从而可以实现更加高效的调度。
  ```

  多队列调度算法是一个比较泛泛的概念，下面说其中一个最佳实践：

  - 多级反馈队列

    ```
    多级反馈队列算法设置有多个就绪队列。每个队列被分配一个优先级，第一个队列优先级最高，第二次之，以此类推。每个队列内采取轮转调度算法，并且优先级越高的队列，设置的时间片越小。
    当新进程进入内存后，默认将其放入第一个队列中等待执行。轮到他时拿去执行，如果在一个时间片内这个进程没能执行完成，则暂停执行并将此进程放入第二个队列的队尾，等待执行。以此类推，直到设置的最后一个队列。最后一个队列以普通的轮转调度算法执行。
    对于不同队列间的调度，采取优先级调度算法。即，只有第一队列中目前是空的，才去第二队列队首取进程执行，以此类推。
    ```

    

## 死锁

死锁总是发生在对临界资源的使用时。临界资源，定义是需要互斥访问，并且不可以被抢占的资源，比如打印机、队列、信号量等。

最经典的死锁的模式，是A和B分别对X和Y两个临界资源发起请求。不同的是A以XY的顺序发起，B以YX的顺序发起，导致互相等对方释放前一个资源，从而死锁。上述模式也可以总结称为 **“竞争不可抢占性资源引起死锁”**。

另一种死锁的模式是，加入A和B两个进程分别要求某种相同的资源各三个。然而这种资源系统里总共就只有4个。如果AB同时发出资源申请，很可能会出现，A获得两个，B获得两个的情况。此时两者都还没有满足获得三个资源的条件，因此不会运行，然而系统资源也没了，互相之间就死锁了。这种情况下，本应想定资源即使不够，也应该至少满足一个进程的运行，这样等进程运行结束后让出了资源可供另一个进程使用。这种模式成为 **“竞争消耗性资源引起死锁”**。

死锁的定义: 一组进程中，每个进程都在等待由改组进程中其他进程才能引发的事件重回就绪态，那么这组进程是死锁的。

### 产生死锁的必要条件

1. 互斥条件

   ```
   进程对所分配的资源进行排他性的使用。
   ```

2. 请求和保持条件

   ```
   进程在使用到某个排他性资源后，要继续请求另一个资源，且这个过程中，对已有的排他性资源进行保持不放。
   ```

3. 不可抢占条件

   ```
   上述已经使用的排他性资源，不能被其他进程抢占。
   ```

4. 循环等待条件

   ```
   发生死锁时，若将资源、进程之间的互相关系以有向图的形式画出来，必然存在一个循环链。
   ```

### 预防死锁

预防死锁，是指一切程序开始运行前，通过追加一些限制条件，破坏死锁四个必要条件中的一个或几个，从而预防运行过程中产生死锁的情况。针对四个条件不同，有不同的预防策略如：

- 破坏“请求和保持条件”

  ```
  破坏请求和保持条件，实际上就是不允许进程在请求新资源的同时持有不可抢占的资源。解决这个问题有两种思路，
  第一，要求进程在开始运行前申请好全部需要的资源，否则不允许运行。这样就不会有持有一些资源，申请另一些资源的情况了；
  第二，进程在申请新资源之前，将已经使用完成的旧资源全部回收处理。
  第二种思路比第一种更高效因为第一种在一开始就要占据全部资源，可能会造成浪费。
  ```

- 破坏不可抢占条件

  ```
  一句话就是允许资源被抢占。准确来说，只有当某个进程尝试申请新资源没有立即成功时，就将其目前持有的资源设置为可抢占。自然，这个资源很快就会被别的进程抢占去。
  ```

- 破坏循环等待条件

  ```
  本质上就是破坏资源-进程图中的环。具体是这么做的：
  对于所有资源，我们将其从小到大编号。每个资源有了编号之后，我们规定，任何进程，都必须先申请较低编号的资源，再申请较高编号的资源。
  若某进程持有较高编号（如12）的资源时，想要申请较低编号（如5）的资源时，其必须先把12号资源给释放，然后再从头申请5号和12号资源。
  有了这个限制，资源-进程图就不会成环。
  ```

### 避免死锁

避免死锁看起来字面意思和预防死锁差不多，两者指两种不同的防止死锁出现的方案。预防死锁更多的是在程序运行前追加限制条件。而避免死锁更多的，是指在程序运行过程中动态地判断某些资源能不能分配，分配了之后会不会引起不安全状态，从而防止死锁出现。

避免死锁更多的是用在 竞争消耗性资源引起的死锁 的解决上。这里首先定义一个概念叫做系统的安全状态：

- 安全状态：指当前进程和系统资源的情况下，可以找到一个进程的排列，使得现有的系统资源可以通过这个排列依次分配资源进行进程运行。期间不会出现死锁直到所有进程结束。相对的，不安全状态就是指一个这样的排列都找不到。

避免死锁的算法称为银行家算法。

#### 银行家算法

简单来说，银行家算法要求每个新进程进入系统时，声明运行过程中需要的各种资源的最大数目。当进程对某资源发起请求时，系统会结合该进程目前持有的资源判断，系统是否有足够的资源满足该进程。只有是的情况，才允许分配资源给进程。

如此就避免了因 竞争消耗性资源而引起的死锁。

银行家算法的具体内容这里就不写了，可以参看书本。

### 死锁的检测

正如前面提到的，验证循环等待条件的方法，就是将当前的进程-资源有向图画出来。然后探索这个图看有没有环就可以知道是否存在死锁了。

> 顺便一提，检测有向图 中是否有环，是一个拓扑排序问题。可以用基于DFS的拓扑排序方法来解决。

### 死锁的解除

解除死锁的方法也很简单。从两个方面考虑，第一，解放出新的足够的资源，从而使得死锁解除。第二，终止进程，打破循环链解除。

对于后者，还可选择一次性终止整个死锁组的进程，或者一个个终止。一次性终止所有进程，是个很暴力的解决办法。。一个个终止稍微温和一点，但是仍然可能会带来比较大的损失。

# 存储器管理

## 存储器的层次结构

存储器泛指计算机上所有具有数据存储功能的部件。存储器的访问速度与其最大容量、价格之间有一个trade-off，因此不同速容比的访问器也有不同的用途。常见的从访问速度从高到低，最大容量从小到大的顺序有：

```
寄存器（CPU寄存器）
主存（高速缓存、主存储器、磁盘缓存）
辅存（磁盘、可移动存储介质）
```

寄存器和主存在掉电后回丢失其中保存的信息，而辅存的信息是持久化的。另外，CPU寄存器和主存储器也被称为可执行存储器。与辅存等非可执行存储相比，对可执行存储的访问不必通过IO设备，因此比要通过IO设备的辅存访问快3个数量级。

本章主要介绍寄存器、主存这些掉电后丢失信息的存储器。至于磁盘等的管理在后面。

### 各类存储器简介

- 主存储器

  ```
  即内存，CPU从内存中读取指令，并把计算结果输出给内存。
  因为CPU的执行指令的速度比访问内存的速度要快很多，为了调和这个矛盾，又引入了寄存器和高速缓存
  内存嘛，通常大小在几百M到几个G之间。
  ```

- 寄存器

  ```
  寄存器具有和CPU处理同样快的速度，因此访问很快，可以和CPU协调工作。
  寄存器是很小很小的，通常是32、64位。相对的可能会设置很多个寄存器。
  ```

- 高速缓存

  ```
  高速缓存介于寄存器和主存之间。主要用于备份主存中常用的数据，从而提升主存与CPU之间通信的效率。
  高速缓存的大小通常在几十K到几个M间，比寄存器要大很多，但比主存小很多。
  为了进一步提升效率，现在通常都会设有多级的高速缓存，越高级的高速缓存速度快容量小。套娃。
  ```

- 磁盘缓存

  ```
  和高速缓存调和了CPU计算速度与主存访问速度的矛盾一样，磁盘缓存用来调和主存访问速度与磁盘访问速度之间的矛盾。
  需要注意的是，磁盘缓存不是真实的存储器而是一个概念。磁盘缓存通常是在主存中特别划出一部分空间，用来做磁盘缓存，保存一些磁盘过来的信息。
  ```

## 连续分配 存储管理方式

连续分配方式是存储管理方式的一种。其特点是将一片连续的内存空间分配给某个进程的代码和数据。具体的还可分为如下四种算法的分配方式

- 单一连续分配：将整个可以分配的内存分配给一个进程。显然只适用于单用户单任务的操作系统中
- 固定分区分配：将整个可以分配的内存提前按照一定规则划分成数个区域，每个区域针对某用户的某任务进行单一连续分配。虽然支持了多用户多任务系统，但是十分死板。
- 动态分区分配：根据进程的实际需要，可以动态地进行内存分配工作。这个下面会重点介绍。
- 动态可重定位分区分配：当分配碎片过多时，将已经分配空间进行归纳整理，比如将其全部都整理到低址区，从而可以空出大片连续的高址区。然而这么做的问题是已经分配的部分因为绝对地址的改变，导致无法正常读取程序和数据。此时就需要进行一次动态的重定位，修正其绝对地址到新地址上。

### 动态分区分配具体算法

动态分区分配的前提是要知道当前内存还有哪些空间是可以被分配的，于是这就要求我们实时维护一个空闲分区表。这个表记录了所有空闲分区的 1.大小 2.起始地址。

而其具体算法可以分为两类。基于顺序搜索的分配算法和基于索引搜索的分配算法。先来说说基于顺序搜索的：

- 首次适应（FF）算法

  ```
  在空闲分区表从低址到高址依次扫描，碰到的第一个足够容下进程的空闲分区时将其分配给进程。
  需要注意如果空闲分区比进程要求的空间大，那么剩余空间会遗留下来。
  FF算法的特点是每次尝试分配都会从低址开始扫描，因此其倾向于使用低址区域的空闲分区。
  这也导致可能的问题：低址区域的内存会越来越碎片化，且后期找空闲分区会花费较长时间因为低址处无地可用。
  ```

- 循环首次适应（NF）算法

  ```
  为了避免FF中出现的碎片化空间的问题，NF中规定本次扫描是从上次扫描到的空闲分区的下一个分区开始。
  由于不是每次都从头扫描，可以避免碎片化空间过于集中的问题。
  ```

- 最佳适应（BF）算法

  ```
  我们总是选取当前空闲分区中能够满足进程要求且最小的分区分配给进程。
  这么做乍一看好像挺合理的，但是实际上每次都选择最佳适应空闲分区的话，也会有最大的风险不断产生很小的无法利用的碎片空间。
  并且为了实时知晓维护分区的大小比较情况，此算法比较耗费时间。
  ```

- 最坏适应（WF）算法

  ```
  和BF相反，总是选用空闲分区中最大的来分配给进程。
  这种算法可以避免过多的碎片空间，并且每次分配只要看最大的空闲分区即可，所以算法耗时上比较好一些。
  ```

上述四种是基本的，基于顺序搜索的动态分区分配算法。从实践上来说，一般还是最简单直白的首次适应算法具有最佳性能。

基于顺序搜索的动态分区分配算法通常适用于较小系统。当内存较大时，空闲分区表本来就很大，会导致顺序搜索的效率下降。此时用索引搜索是一个更好的办法。下面继续介绍几个基于索引搜索的分配算法。

- 快速适应（Quick Fit）算法

  ```
  事先将所有空闲分区按照大小规格分类，对每类分区单独安排一个空闲分区表。于是就有了一个 规格：分区 的索引表。
  借助这个索引表，对于新来的进程可以快速地为其找到合适的分区并分配。
  ```

- 伙伴系统

  ```
  略
  ```

- 哈希算法

  ```
  哈希和QF也差不多，只不过这里不规定死的大小规格，视内存当前具体情况，把所有空闲分区大小的可能，都用 空闲分区大小：起始地址 类似的形式用一个哈希表整合起来。
  ```

  

## 对换（Swapping）

将内存中暂时不能、不用运行的程序以及其数据换出到外存上，腾出内存空间，从而将已经具备运行条件的程序和数据从外存交换到内存中。

按照对换操作的基本单位，可以将对换分成 整体对换（指一次对换针对一整个进程）或者 部分对换（一次对换只针对一个页面等部分单位）。这章主要介绍的是整体对换。

### 对换空间的管理

在要进行对换的系统中，磁盘上都会划出一块对换区。这片对换区就是用来进行对换的外存空间。由于进入这个区域的内容都不会长久驻留且访问频率很高，因此这个区域应该要实现高效的访问而不是空间利用率的提升。

因此，对换区通常采用 “连续分配方式”（注意，虽然上面介绍过的连续分配方式是针对内存的，但是要知道内存和磁盘都是存储器，本质是一样的，所以可以沿用在这里。

对换，不论换哪个方向，其本质都是从一片存储区域中找到一个合适的区域来存放进程内容。因此之前介绍到的所有动态分区匹配的算法也都适用于此。

## 离散分配 —— 分页存储管理方式

上面提到的存储管理方式主要是连续分配存储管理方式。可以感觉到，这种方式的一个很大的缺点，就是可能会引起很多碎片空间，碎片空间是无法使用的，因此会影响有效空间使用率。

相对的，离散分配的存储管理方式看起来似乎更加灵活。根据离散分配的单位大小不同，将离散分配管理方式大概分为 分页、分段、页段三种。这节主要讲讲分页的管理方式。

### 分页存储管理的基本方法

分页，指一方面将进程的地址空间分成若干个固定页面大小的部分，通常这个大小是1K~8K。另一方面，将内存也分成同样大小的页框，或者叫块。进程的任意一页都可以放到内存的任意一块中去。

在分页体系下，可以通过页号P和偏移量W来确定一个地址。假设页长是L，则有`A = P * L + W`。

另一方面，如果知道了逻辑地址A，也可以快速找到其页号和页内地址：`P = A // L`, `W = A % L`。

分页管理，是一种离散型的管理方式。这也就意味着针对一个进程的每个页，我们都得知道其处于内存的哪个物理块中。为了维护这方面信息，系统给每个进程又开辟了一个页表。页表，就是 一个进程页的页号 对应到 内存中块的块地址 的映射关系。

### 地址变换机构

所谓的地址变换机构就是一个将用户地址空间中的逻辑地址变换成一个内存中的物理地址的模块。在分页进行存储管理的情况下，一个页放进一个内存块之后两者是完全对应的，因此只要找到两者的起址即可。换言之，地址变换机构是一个将进程地址空间内页号转化为内存某个块的块号的模块。这个机构具体的工作其实也很简单，只要知道进程整个页表的起始地址，这样把某个页号转化为物理块号，只要加上这个起始地址就行了。

为了保证快速的寻址，通常采用寄存器作为这个机构的载体。当然寄存器很贵，全局只能安排一个寄存器。于是，进程的页表起始地址和其页表长度，平时只保存在其PCB中，当开始运行时将这两个量加载到地址变换寄存器中。

接下来拿到一个逻辑地址后，只要读取寄存器，取出起始地址，加上逻辑地址，就是物理地址了。当然这过程中还需要校验，逻辑地址不能超过页表长度，否则越界，会访问到不安全的物理地址。这也是为什么要把页表长度也加载到寄存器中的原因。

#### 使用快表寻址

利用页表寻址的过程，系统需要访问两次内存。第一次，访问页表，用逻辑页号计算物理块号。第二次采取访问物理块，得到想要的数据。

而所谓的块表，就是在这个过程中加入一个名为快表的LRU缓存。拿到逻辑页号后，先去缓存里找是否有相关记录记录了其对应的块号。如果有，直接使用即可。没有才再去访问页表查找相应的物理块号。

快表通常是一个寄存器（如果读取速度和内存一样慢，那就没有意义了），因为是寄存器所以很小，通常只能存放512条页号→块号的记录。然而这还挺够用的，因为计算机数据访问的局限性（或者叫二八定律，80%的访问总是落到20%的热门数据上），因此可以加速。因为快表是一个LRU缓存，所以当表满的时候会把最老的没人用过的记录给清除掉。

加入快表之后，寻址会有一个命中率的概念。命中率指，在泛泛的寻址过程中，一个逻辑页号可以再快表中找到对应物理块号的大致概率。代码数据复用率越高，热门数据越热门，命中率也就越高，从而整体CPU访问内存速度会加快很多。

### 多级页表

现在操作系统大多都是32或者64位，说明其中一个进程理论上最大可以支持`2^32 ~ 2^64`字节的地址空间。即使一个内存也设置为4K（即`2^12`），除法做一下，一个32位进程的页表长度仍可能达到1M。

和很多类似情况一样，这种时候可以考虑多级页表。比如第一级页表中保存的并非页号与内存块号之间的逻辑，而是页的页号。此时页有一级页和二级页（分页）。此时每个逻辑页号就有三个部分了：一级页页号，二级页页号以及页内地址。凭借前两个页号就可以定位到一个物理块。这么一来，就把长页表给通过二级结构压缩，检索的时候就快了。

## 分段 和 段页式存储管理方式

具体内容先略了…

总之先记住，所谓的段，本质上和页是差不多的，也是将大块内存以小粒度进行保存，从而避免碎片化等问题。只不过这里的段由程序员自己定义管理；而页是系统自身进行管理，对程序员透明。

# 虚拟存储器

## 概述

以上提到的所有内存管理方式，有一个大前提，就是要求将一个进程的整个地址空间全部都放入内存，然后才开始运行进程。

但是如果物理内存不足而进程地址空间又很大的话，会导致进程无法运行。

当然，物理上增大内存可行，而更灵活的方法就是用虚拟化方法，虚拟地扩大当前现有的内存。

简言之，用时间换空间。进程运行不一定需要整个地址空间中的所有信息，于是我们可以只将当前进程要用的内容加载到内存，运行进程；而其要求新内容时，将用完的内容换出内存，加载新的内容进来，就可以保证进程在小内存空间中无问题地运行。

### 虚拟存储器的运行原理

首先，虚拟存储器必然建立在分页存储管理方式的基础上。如果是连续分配管理方式，考虑到进程所有内容都有可能被用到，终究还是要分配足够容纳整个进程的空间，就无法做到虚拟化。

具体的，程序运行之前，可能仅将进程的一部分内存页加载到物理内存中，先行开始运行。运行过程中，如果进程发现需要访问的页不在内存中，此时称为发生缺页，于是进程将向OS发起 “缺页中断请求”。OS接到请求后，利用页置换计数，将相关页加载入内存。如果内存此时已经充满，那么还需要将内存中一些页面给调到磁盘上，腾出内存空间以调入新内存页。

以上过程不仅可以让一个大进程在小内存中运行，也可以使同样内存中运行更多的进程。

### 虚拟存储器的定义与特征

> 这个“虚拟存储器”，很容易叫成虚拟内存，但是Windows系统中的虚拟内存更像是Linux中的Swapping机制，所以为了不混淆，这里继续使用虚拟存储器这个叫法。

虚拟存储器，并不是一个实际客观存在的存储器，而是集合了主存、辅存、具有请求调入和置换功能，借此可以从逻辑上对内存容量进行扩充的存储器系统。

按照基于的存储系统是分页还是分段的，又可以分成两类。这里以分页式的作为例子进行说明。

## 请求分页存储管理方式

分页式的虚拟存储器学名 请求分页系统，在基本的分页存储管理方式上发展而来。注意这里以及下文中的的“请求”二字是一个定语而不是动词，用来形容这个分页不是一般的分页而是带有调入、置换功能的分页体系。

### 硬件要求

请求分页系统的硬件要求有：内存、外存、请求页表、缺页中断机构、地址变换机构。

#### 请求页表

我们知道，普通的页表是进程的内存页与物理内存块之间地址的映射。因此一行只有页号+物理块号两个字段。而请求页表，因为要考虑与外存换页的功能，这里还有额外的四个字段，分别是

- 状态位P

  仅一位，指示该页是否在内存中

- 访问位A

  用于记录该页多久没有被访问。可以作为置换算法的参考。

- 修改位M

  记录该页这次在进入内存后是否被修改了。通常外存会保存该页的一个副本，如果一个页进入内存后始终没修改，那么就没必要将其原封不动地写回去，直接保留副本即可。这样可以减少和磁盘交互的时间。

- 外存地址

  保存该页在外存的副本的地址。供调入内存时参考。

#### 缺页中断机构

当进程需要访问的页面不在内存中时，就会触发缺页中断，请求OS将相关页调入内存。

和一般的中断相比，缺页中断当然也需要保存CPU信息，分析原因，调入中断处理程序。不同的是，缺页中断可以在指令执行过程中产生，而一般中断通常是在指令完成后才检查有没有中断请求。此外，即便是一条指令内部，也有可能发生多次中断请求。

#### 地址变换机构

请求分页模式下的地址变换更加复杂，因为要根据页是否在内存中以及是否在快表中等多个条件，制定不同的变换策略。大致过程是这样的：

拿到页号后，先检索快表看是否命中，

- 若命中，则直接访问相关物理块，修改页内的访问位字段。
  - 若这个指令是写指令，则在写完成后还需要修改 修改位 字段的信息。

- 若没有命中，说明需要到内存中去检索页表，但该页有可能不在内存中，因此先检查该页 状态位。
  - 若该页已经在内存中，则更新快表，继续访问即可。
  - 若不在内存中，则触发缺页中断。此时OS去外存中找相关页，找到后还要看内存是否满了
    - 如果满了则根据置换算法选择一页换出，换出后记得后续更新一些内容比如该页的状态位以及页表等。
    - 如果没满，则直接启动IO，将外存的页加载到内存，加载结束后更新页表，更新快表。

总的来说，页表中记录的页必然是已经在内存中的，而快表中的页必然是在页表中的。

### 请求分页存储管理方式中的内存分配

在一般的分页存储管理方式中，系统默认分配给一个进程足够的内存空间。所以也不存在什么分配策略的问题。然而在请求分页体系中，就有很多细节需要明确。

首先，要明确保证一个进程能够运行的最小的物理块数。通常最简单的直接寻址方式工作的系统，进程只需要两个物理块，一个存指令，一个存数据，就可以运行下去（虽然换页非常频繁地发生）。

其次，要确定内存该如何分配。
总体来说，针对一个进程，分配给他固定的内存还是可变的内存、当缺页发生时置换的页是局部的还是全局的，根据这两个维度可以衍生出几种不同的策略。

- 固定分配局部置换

  ```
  固定分配，即将定值个内存块分配给一个进程，之后进程运行过程中，属于此进程的内存块个数不再变化。
  局部置换，即缺页时，OS只考虑将属于当前进程范围内的某个页置换出去，调入新的页。因为只考虑本进程范围内，所以称为局部置换。
  显然，固定分配的坏处是很难把握到底分配块给这个进程才合适。如果太少，则频繁发生缺页中断；若太多，则又降低了内存中进程的个数。
  ```

- 可变分配全局置换

  ```
  可变分配，即进程运行过程中，会根据其需要，动态地增减分配给其的块数。
  全局置换，即缺页时，OS考虑从全局所有页中找到合适的块来接收新页。这个块可能本身就是空闲的，那么相当于是给本进程增容了；或者是别的进程使用中的页，那么相当于给本进程增容但给其他进程减容。
  ```

- 可变分配局部置换

  ```
  可变分配和局部置换的概念上面都说了。
  具体的，这种策略下，缺页时优先考虑进行进程内的局部置换。当缺页频率上升局部置换频繁发生时，系统将额外的块追加分配给该进程改善其内存紧缺的情况。
  ```

最后，不同进程间如何分配有限的物理块也存在一个策略问题。
常见的策略是 1. 平均分配  2. 按比例分配  3. 按优先级分配

### 请求分页存储管理方式中的页面调入

在何时调入页面：分为预调页和请求调页两种策略。预调页默认某个页面被用到后其后面的一些页很有可能在接下来的执行中用到，所以一次性将多个连续页面调入，减少磁盘IO。

调入页过程：

```
当进程要访问某页，发现其状态位是0（不在内存中），于是向CPU发出缺页中断。
中断前先保存CPU环境，分析中断原因，调用中断处理程序。中断处理程序查找页表中相关页信息，得知其外存地址，然后尝试将其调入内存。
调入内存分为内存充足（直接调入）和内存不足（使用置换算法将一个老页出去，置换成新页）两种情况。
调入页面后，还需要更新页表，更新快表。
```

### 缺页率

设程序运行过程中，每次访问页面时，若成功访问到的次数是S，发生缺页的次数是F，那么缺页率就是`F / (F+S)`。影响缺页率的因素有如下几点：
页面大小，进程分配所得的物理块数目，页面置换算法，程序本身（较好地体现程序局部性，即存在明显的热门数据，热门代码的程序缺页率低）

## 页面置换算法

缺页发生，且内存中并无空闲块，此时就要发生页面置换，即将已经存在于内存中的某个页面置换到外存，将要用的页面置换进来。而如何选择那个已经存在内存中的页面，就要靠页面置换算法来决定了。

一个不合适的页面置换算法会引发这种现象：刚开始内外存分别有A、B两个页面，下一个时刻，AB互换变成了BA，再下一个时刻，发现又要用A了，于是再将A置换回来，再次变成AB。如此，频繁地置换页面，浪费大量计算资源。这种现象也被称为页面置换的“抖动”。

下面介绍一下各个算法

- 最佳置换算法

  ```
  需要指出，最佳置换算法是一种理论算法，目前还无法实现，是页面置换算法性能的天花板，同时也是用于评价其他实际算法的标杆。
  最佳置换算法假设在某一时刻我们可以识别出内存中已经存在的所有页中，在未来最长时间内不会再次被使用的页面，将其作为被置换页面置换出去。
  
  显然，目前的技术水平尚无法确定到底哪个页面在未来最长时间不会被使用，因此是个理论算法。
  ```

- 先进先出（FIFO）算法

  ```
  顾名思义，这是将内存看做是一个页的队伍，当要挑一个祭天的时候，就挑最早进来的那个。
  ```

- 最近最久（LRU）未使用算法

  ```
  LRU算法是什么样的就不说了，从思考上来说，这是一种将 “最近的过去” 用来近似 “未来”，从而逼近最佳置换算法的手段。
  另外，为了实现LRU，还需要硬件支持。硬件可以是寄存器或者栈。
  ```

- 最少使用（LFU）置换算法

  ```
  顾名思义，该算法将过去一段时间内访问次数最少的页作为祭品。
  具体的，该算法和LRU一样需要硬件支持。他要对每个页安排一个寄存器。该寄存器用来统计一个极短时间间隔内页面是否被访问。
  所有寄存器的值每隔一段时间t后右移，最后经过n个t后寄存器中值最小的那个页面，就是过去n*t时间内，使用最少的页。
  
  能够实现LRU的一套硬件，也可以拿来实现LFU，两者很相似。
  ```

- 简单Clock置换算法（也称NRU算法）

  ```
  要求额外的硬件使得LRU并没有那么容易实现。而Clock置换算法是一种不需额外硬件的LRU近似算法。
  该算法中，利用请求页面体系中，页表中某个页的 访问位 。
  具体原理是，安排一个指针从头到尾再从头，循环地扫描所有页。我们知道，当某个页被进程访问后，这个页的访问位置1。
  而Clock指针扫描到一个页，若其访问位是1，则说明最近有进程使用过他，因此给予其留在内存的机会，不过将访问位置0，并继续向下扫描。
  相反，若某个页访问位是0，则说明在上次置换之后，这个页最近一段时间没人用过，因此可以将其祭天。
  
  这种算法也是找了最近未使用页，所以称为NRU算法。
  ```

- 改进型Clock置换算法

  ```
  我们知道，对于某个被置换出去的页来说，如果 修改位 是1的话，说明其滞留内存期间被修改，因此置换出去之后需要重写外存中这个页的副本；否则不用。
  那么显然，优先考虑置换那些未被修改过的页出去是比较合理的。于是有这么一个策略：根据 访问位A和修改位M两者的值不同将页分类：
  1. A=0, M=0
  2. A=0, M=1
  3. A=1, M=0
  4. A=1, M=1
  对这四种，优先扫描第一种页，若有祭天。扫描过程中不改变任何。
  第二轮，扫描第二种页，若有祭天。扫描过程中将所有扫描过的页的A置0。
  第三轮，将所有页A置0，重复第一轮寻找第一种页的扫描，若有祭天。扫描过程中不改变任何。
  第四轮，若前三轮都没有找到合适的页，则第四轮重复第二轮扫描，即找第二类页。理论证明，此时一定一定可以找到一个页祭天了。
  
  改进型Clock和简单Clock相比，由于需要多轮扫描性能上会差一点。
  ```



综合考虑以上各种页面置换算法后，改进型Clock虽然性能略弱，但是效能不错，是一个比较好的综合选择。

## 抖动与工作集

### 抖动

在请求分页存储管理方式下，一个固定内存的系统，其CPU的利用率随着内存中同时存在的进程数上升而呈现先上升再下降的趋势，即存在一个使得CPU效率最大化的 “合适进程数”。

而在进程数很大的时候，CPU的利用率接近0。这就是因为系统发生了上面说过的，因为过于频繁的页面置换而引起的抖动。
抖动的根本原因就是，分配给每个进程的物理内存块数过少，导致进程为了继续运行不得不频繁地进行页面置换，而页面置换又是需要进行磁盘IO操作，因此过于频繁的置换导致系统几乎就在空转。

### 工作集和抖动预防

因为程序对数据和代码访问都具有局部性。因此，在某一个时刻接下来的一段时间内，要访问的页面可能是相对比较集中的。于是我们就可以将这部分未来访问可能性很高的页面集合，称为工作集。实际上，我们无法得知未来的情况，所以和页面置换算法中的最佳置换与LRU之间关系类似，采用过去的一段时间来近似估计未来。

换言之，过去一段时间内（称为窗口时间）的被访问的多个页整合成工作集。那么工作集有什么用呢？将其融入到页面置换的算法中，就可以减小抖动出现的可能性。除了工作集，还有如下这些方法可以预防抖动发生：

- 采取局部置换策略：在可变分配局部置换的内存分配策略下，当某个进程发生抖动后，因为局部置换使得其可置换的页都是自己的页，所以一个进程发生抖动不会“传染”其他进程。在全局置换策略下则有这种风险。
- 使用L=S准则调节缺页率
- 暂停部分进程以腾出空间

