> https://github.com/Fsiyuetian/kubernetes 狠不戳

# 【旧】~~容器与虚拟机~~

容器和虚拟机都是用来做环境隔离的基本单位。通常我们都把容器看做一种轻型的虚拟机，因为容器和虚拟机的架构区别如下：

![img](https://upload-images.jianshu.io/upload_images/12979420-a562cd670f2b8b02?imageMogr2/auto-orient/strip|imageView2/2/w/529/format/webp)

不论是用什么隔离体系，最终我们的目的都是要让程序利用硬件进行计算。而利用硬件都要通过操作系统内核。
这里，虚拟机的做法，是带起一个自己的操作系统，通过自身操作系统的内核去访问硬件。而硬件本身也不是直接暴露给虚拟机OS，而是通过虚拟机管理软件，即上图中的hypervisor进行了虚拟化。
另一方面，容器的做法是所有容器共享宿主机操作系统的内核，从而访问硬件。因此容器内部不需要自己的OS。容器通过Docker Engine管理程序与宿主机OS内核打交道。

开启一个新虚拟机时，需要加载庞大的虚拟机的操作系统，同时虚拟机运行时其操作系统肯定也要运行，因此会占据较多的CPU、内存资源。
而启动一个容器很快，往往只要几毫秒即可。容器通过Namespace技术将不同容器的各种资源隔离开，但由于容器专注于应用代码本身，没有多余的操作系统等，因此占用资源也不多。

# 容器

> 容器这个概念的出现其实比Docker要早。基于Linux内部的一些技术如Namespace以及Cgroup等，可以实现进程之间运行环境的隔离，就是容器。由于基于Linux实现，也称Linux Container，简称LXC。
>
> 而Docker只不过是在外围开发了更多功能，让容器在工业上得以方便快速地应用。

## 容器与虚拟机

我们经常把容器看做是一个轻型虚拟机。一句话来解释两者的区别：
虚拟机从操作系统层面开始虚拟化。虚拟机管理软件对硬件资源进行虚拟化并提供给虚拟机使用。
容器则是从运行环境层面进行虚拟化， 容器管理软件如Docker引擎对操作系统直接进行虚拟化并提供给容器使用，换言之多个容器共用同一个操作系统内核。

如图：

![img](https://upload-images.jianshu.io/upload_images/12979420-a562cd670f2b8b02?imageMogr2/auto-orient/strip|imageView2/2/w/529/format/webp)

由于架构上的不同，容器和虚拟机在运行使用上差别也很大：

| 比较点           | 虚拟机                                                       | 容器                                                         |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 系统资源利用效率 | 低，因为要额外运行维护一个OS，相当一部分资源被浪费在非核心任务上。<br />虚拟机内部的进程用硬件运行时还需要通过一层层虚拟化软件的检查和调配，更是拖慢了运行速度。 | 高                                                           |
| 启动时间         | 慢                                                           | 快，容器启动只需利用内核做好环境隔离与资源限制即可启动，无需启动操作系统 |
| 隔离性           | 高，完全的OS级别的隔离                                       | 低，有些共通的东西无法隔离                                   |

而结合Docker实现的其他技术，容器还具有易迁移、易维护扩展等特性。下面细说。
上面描述看起来容器好像吊打VM。但是实际上两者并不是哪个更先进，而只是两个各有用途的东西。

比如容器因为没有虚拟化OS，所以容器依赖于宿主机的OS内核。若我想在Linux上跑Windows的程序，那么显然容器是无论如何也做不到了，这时候还是得使用VM从OS层面进行虚拟化。
再比如相较于虚拟机，容器的隔离级别没有那么高，有许多资源实际上并没有做到隔离。比如系统时间，在一个容器中通过系统调用修改系统时间，会导致宿主机以及该机上所有其他容器的时间也被修改。

## ⭐️虚拟化技术

因为容器共享一个OS，可以想象，==容器的本质应该就是一个OS上的特殊进程而已。而这个进程的目的，是为其下属的子进程们，提供一个独立的运行环境==。那么怎么样才算独立的运行环境？大概要实现下面两个维度的隔离：

- 文件系统的隔离。如`chroot`命令或者配置sftp服务器时那样，可以让某个进程的可以看见的最上层根目录是实际系统中某个非根目录，从而限制了该进程能访问的文件系统的范围。
- 资源隔离。资源隔离又分两个角度来看。一是资源可视性，不同进程能够看到的资源不同，看不到的自然就使用不了；二是资源限制，对进程可以使用的资源，能让他使用多少。

==Docker通过了UnionFS技术实现了容器间的文件系统的隔离。又用了Namespace和Cgroup技术分别实现了资源隔离的上述两个角度。
除了这三个，还通过容器引擎，实现了对容器的综合管理。==
以上提到的四大技术，就是Docker核心的四个虚拟化技术了。下面来细说。

### Cgroup

> https://zhuanlan.zhihu.com/p/127312824

Cgroup最初的设计目的，是先将进程分组，然后将各个不同组的进程可以使用的资源进行限制，限制的角度包括CPU使用个数/使用率、内存使用空间/使用率、IO吞吐量、带宽等等。由于容器本质上是一个特殊的进程及其所有子进程。那么将这些视为一个进程组并通过Cgroup加以限制，就可以做到Docker的通过Cgroup对资源使用进行限制的目的。

一般而言，有了限制，我们肯定期望其在超出限制的时候做一些事情，因此Cgroup还具有 监控进程组的资源使用情况 以及 对进程组做出控制操作如挂起、重启等。

Cgroup中一个重要的概念称为子系统。不同的子系统对不同种类的资源作出限制。
每个子系统都对应到系统中某个目录文件下，通常是`/sys/fs/cgroup/`中的一些目录。可以通过`mount -t cgroup`命令进行查看。
子系统列表如下：

<img src="https://img2020.cnblogs.com/blog/1977753/202004/1977753-20200412224046852-870716976.png" alt="img" style="zoom:50%;" />

#### docker容器与Cgroup的关联性

以cpu子系统为例，该子系统位于`/sys/fs/cgroup/cpu`。如果服务器装了docker，其中会有一个名为docker的子目录。代表其中维护这docker子组的一些信息。

如果有正在运行的容器，则docker子目录下还会有容器ID作为名称的子目录，代表其中维护着docker子组下的该容器子组的一些信息，也就是容器内所有进程形成的组了。
其中的各个文件则是这个子系统用来进行资源控制的参数内容。这些文件不是真实存在磁盘上的，因此大小是0。其中有个叫做`tasks`的文件保存了这个进程组内（容器内）所有进程映射到宿主机上的PID，算是指出了这个进程组的范围吧。

```
# 在 /sys/fs/cgroup/cpu/docker/ee61...(容器名) 中
-rw-r--r-- 1 root root 0  8月 14 21:38 cgroup.clone_children
-rw-r--r-- 1 root root 0  8月 15 15:55 cgroup.procs
-rw-r--r-- 1 root root 0  8月 14 21:38 cpu.cfs_period_us
-rw-r--r-- 1 root root 0  8月 14 21:38 cpu.cfs_quota_us
-rw-r--r-- 1 root root 0  8月 14 21:38 cpu.shares
-r--r--r-- 1 root root 0  8月 14 21:38 cpu.stat
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.stat
-rw-r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage_all
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage_percpu
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage_percpu_sys
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage_percpu_user
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage_sys
-r--r--r-- 1 root root 0  8月 14 21:38 cpuacct.usage_user
-rw-r--r-- 1 root root 0  8月 14 21:38 notify_on_release
-rw-r--r-- 1 root root 0  8月 14 21:38 tasks
```

举个实操例子，比如`cpu.cfs_period_us`是对CPU整体使用率限制的参数，默认是`100000`（单位是`us`，即100毫秒）。如果将其改成`10000`（即10毫秒），那么再去容器中开启CPU密集进程，此时会发现，其CPU使用率最大最大不会超过10%，这也就是Cgroup在起作用了。这个改动，或者说`cpu.cfs_period_us`的意思是，每100毫秒的CPU时间，当前进程组最多只能使用10毫秒。

启动容器`docker run`时，只要带上参数诸如`--cpu-period=100000`，`--cpu-quota=20000`等，就可以自动为容器添加由Cgroup提供的限制。

#### 【旧】~~Cgroup具体机制与使用~~

Cgroup机制中有以下这些比较重要的概念。

- 进程（task），或者叫任务，即系统中的一个进程。
- 控制组（control group），按照某种标准划分的进程组，指出了限定量的系统资源供组内进程使用。
- 层级（hierarchy），多个控制组之间以层级的方式组织。可以将其理解为一棵树，子控制组会继承父进程组的一些属性。
- 子系统（subsystem），子系统指出一类特定的需要控制的系统资源。将一个子系统attach到某个层级上后，该层级（树）中所有控制组使用子系统相关资源时都会受到其限制。

上述概念有如下逻辑关系

- 进程，即使在开始运行后，也可以根据需要被移动到不同的控制组。
- 父进程创建子进程时，子进程自动被加入到父进程所属的控制组中。后续子进程可以根据需要再被移动到其他组里。
- 一个进程可以是多个控制组的成员，但这多个控制组必须分属于不同的层级。
- 一个层级可以被attach多个子系统，一个子系统最多只能attach到一个层级。

### Namespace

Namespace也是Linux内核自带的一个机制。Namespace的基本生效单位是进程。==其作用是，将原来一个整体的系统（比如网络资源、用户系统、进程系统等），分成若干个互相独立的部分，使互相的进程之间不影响对方==。换言之，一个进程只能感知到相同Namespace下的其他进程，而不能感知到以外的。

Linux内核中实现的Namespace总共有下面这几种：

| Namespace | 作用                                    |
| --------- | --------------------------------------- |
| IPC       | 隔离进程通信的共享内存以POSIX消息队列等 |
| Network   | 隔离网络资源                            |
| Mount     | 隔离文件系统挂载点，即文件系统隔离      |
| PID       | 隔离进程PID，即隔离进程系统             |
| UTS       | 隔离主机以及域名                        |
| User      | 隔离用户以及用户组，即隔离用户系统      |

一个进程可以同时属于若干个不同类型的Namespace。对于多个同一类型的Namespace，一个进程只能从属于其中的一个。
以上这些进程 - Namespace的对应关系，可以在`/proc/<pid>/ns`中查看，如：

```
lrwxrwxrwx 1 root root 0  8月 15 17:09 cgroup -> 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 ipc -> 'ipc:[4026533159]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 mnt -> 'mnt:[4026533157]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 net -> 'net:[4026533162]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 pid -> 'pid:[4026533160]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 pid_for_children -> 'pid:[4026533160]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 user -> 'user:[4026531837]'
lrwxrwxrwx 1 root root 0  8月 15 17:09 uts -> 'uts:[4026533158]'
```

软链指向内容括号中的数字是具体的Namespace编号。不同进程若指向的编号一样，那么就是指他们俩在同一个Namespace中。

至于Docker容器，显然每个容器都应该拥有独立的一套上述各种类型的Namespace，因此，容器启动时就会初始化一套上面所有的Namespace供容器中的进程使用。

需要指出，==Namespace机制从进程系统、网络系统、用户系统等角度做到了一定程度的资源隔离，但是也有一些资源没能隔离。比如时间，当某个容器修改了容器内部的时间，由于对时间并没有隔离，所以会导致宿主机以及其他容器也一并被改变。==

#### 容器高隔离级别的解决方案

在实际操作中，有时候会用一些技术诸如Seccomp，来实现对容器内部系统调用的识别和过滤操作。这样，可以在一定程度上解决上述“改时间”的问题。
当然，额外增加一层判断会拖累容器的性能，同时维护各种系统调用的可不可以使用的情况，也是一件累人的事。

另一个角度的解决方案，是干脆抛弃原来容器的基本原理，采用基于虚拟化或者独立内核技术实现的容器。这相当于是一个从容器向虚拟机的靠近，中庸之道了属于是。

## UnionFS

> https://zhuanlan.zhihu.com/p/115659428

在Linux启动的时候，初始化文件系统时，最开始会建立一个叫做rootfs的文件系统。这个系统内容与磁盘中整个文件系统一样，但是只有读权限。之后，Linux会将rootfs的权限修改为读写，于是文件系统初始化就完成了。

而在Docker中借鉴了类似的思路。第一步先启动挂载一个只读的rootfs，这个文件系统内包含的是镜像的文件内容。之后，和Linux不同的是，容器中的rootfs一直保持只读状态，在此基础上容器内部新创建一个空的读写的文件系统。==只读部分的rootfs和可读写部分的文件系统，共同组成了联合文件系统，即UnionFS。==

<img src="/Users/wyzypa/Pictures/TyporaImages/Docker相关笔记.asset/image-20210722110433707.png" alt="image-20210722110433707" style="zoom:50%;" />

### UnionFS中的层

Docker中的镜像（也就是上图中的rootfs部分）基于层的概念进行构建。层可以简单理解为增量内容。一个镜像，从最基础的只包含最最基本的那些系统组件的状态，加入各种东西到适应各种应用场景的状态，中间每加一次东西，那些==**新增的**==，就会被以层的形式记录。

具体的层的实现方式等与Docker使用的存储驱动形式有关。按照较新的Overlay2为例。
在存储驱动的安装目录（默认是`/var/lib/docker/overlay2`）中，我们可以发现很多代表着一个个层的目录。而每个层目录下面都会有一些类似的子目录or文件，含义如下：

- `diff`子目录：==用于保存相比于所有下级的层，该层独有的文件（指新增的或者修改过的），并且维护的是这个文件的完整路径。==
- `lower`文件：以特定的格式记录了该层下方所有层的顺序关系。这部分内容中，层以短名展示。
- `link`文件：一个指向短名链接（位于overlay2下的`l`目录中）的软链接。

如上所述，每个层的目录名是一个一长串由16进制数组成的东西，这也是该层的一个代号，我叫他长名。与之相对的，为了展示起来好看一些，还有一个短名。比如上述的`lower`中都是用短名表示层。
长名和短名的对应关系在`overlay2/l`目录下维护。同时层内的`link`也是指向了这里面相应的短名链接。

![](https://pic4.zhimg.com/80/v2-f522452ff1c111dae6bc564e5b56f0b7_720w.jpg)

### 镜像层和容器层

如上所说，rootfs部分其实是对应了镜像，而镜像又是由层构成的，这些层称为镜像层。由于rootfs只读，所以可以说镜像层是只读的层。
另一方面，可读写部分的FS也是由层构成，但由于其可读写，为了区别，我们可以不严谨地称之为容器层。

用镜像启动一个容器后，再观察`overlay2`目录，会发现多了两个层。一个是名为`xxx`的容器层，另一个是名为`xxx-init`的容器init层。
看下两者的lower就可以知道，容器层位于容器init层上面，而init层下面才是镜像层们。

顶层容器层很好理解，是一个可读写的层。而init层其实是docker内部自动创建的一个，维护了镜像层中原来的`/etc/hosts`，`/etc/resolv.conf`等网络相关文件的层。
创建这个层的目的，是因为启动容器时为了将容器加入本机的docker子网，必然会要修改这些文件。但是这些修改并不是哪里都通用的，换言之如果你将容器commit成镜像分发，那么我们不希望写在这些文件中的本机docker子网相关配置也一并被打包。所以安排了这么一层init层用于保存这些文件原本的内容，commit时固化这些原生内容而非修改后的。

### 层的读与写

上面我们说了，==每个层的`diff`中其实只保存了相比之前新增的文件==。而多个层合在一起能够构建出一个镜像。那么如果相比于之前版本镜像，我删除或者修改了文件该怎么办？这就牵扯到了在Docker中，层的读写的规则。

读的问题很简单，如上所述，镜像层也是可读的。所以文件存在于镜像层就从镜像层读，当文件存在于容器层就从容器层读，若文件存在于两类层，则直接读取容器层，相当于屏蔽了镜像层的文件。总原则就是，高的层优先于低的层。（至于为什么会有一个文件在两种层的情况，下面细说

写的问题略复杂一些。以修改一个文件内容为例，试想，假如某个文件不存在于容器层，只存在于镜像层中，由于镜像层只读，肯定不能直接改。
实际上，Docker会将容器从镜像层复制到容器层，然后修改容器层的副本。这也是上面提到过的，一个文件在两种层的情况。此时很明显，为了让容器使用者能够获得最新的版本，这里肯定直接读取容器层，符合上述读的规则。这种先拷贝再修改的策略称为Copy on Write即写时复制。

写还有更细节的，比如除了修改文件，删除文件怎么办，删除目录怎么办，修改文件名怎么办，等等。对于层的读写规则，总结如下：

- 读
  - 文件位于容器层中，直接读容器层（若镜像层中也有同名文件，则其被屏蔽
  - 文件只位于镜像层中，则直接读镜像层
- 写（修改文件）
  - 文件位于容器层中，直接修改容器层
  - ==文件只位于镜像层中，将其完整复制到容器层，并在容器层修改==
- 写（删除文件/目录）
  - 文件/目录只位于容器层中，直接删除即可
  - ==文件/目录还位于镜像层中，无论其是否存在于容器层中，都会在容器层的相关路径下创建一个同名的dummy标记（若存在于容器层，则先删除再创建标记）。文件的标记称为whiteout文件，目录的标记称为opaque目录。==这两个东西，直接到`overlay2`中的层的目录下去`ls`可以看到，但是`cat`等操作无法读取，并且在容器中，由于这两者的特殊形式，引擎不会显示之，从而在容器使用者看来，觉得文件被删除了。
- 写（重命名）
  - 重命名基本视为 删除 + 新建 两步走。
  - 有些资料指出重命名目录时，只有原路径和目标路径都在容器层中才能成功。但是做了下实验并不是啊…

#### 对挂载卷的读写

稍微再多提一句。挂载卷的原理，是创建一个宿主机和容器进程能共同使用的文件系统。因为独立与UnionFS，所以在容器内部对挂载卷进行读写就是直接对磁盘进行读写。
另一方面，如果挂载卷内容被添加到镜像中，基于镜像启动容器，再到容器中修改相关文件，显然要遵循UnionFS的规则，即先把文件复制到容器层，再在容器层做修改。显然后者的效率以及开销比前者大。

#### 构建镜像时注意层级别的优化

如上述，一个层一旦被固化到镜像，就不会再被改变，而基于这样一个镜像创建更高层级的镜像时，这些文件也会被包括其中。

这就会导致，当一个镜像层数很多时，镜像会变得很大（比如一个大文件在低层中被创建，而后在高层被删除，但是此时文件仍然存在于低层中，而高层中只是一个whiteout文件）。
另外，镜像的绝对层数也是有限制的。通常和具体的docker版本、实现方式有关。比较常见的是127，即超过127层的镜像，docker是无法打包并分发的，需要注意。

因为以上原因，在构造镜像时必须注意，在满足需求的基础上尽可能地减少层数。比如能通过一个命令搞定的事情，尽量用一个`RUN`指令与分号完成，这样就只生成一个层。

# ⭐️容器的网络模式

> https://www.jianshu.com/p/22a7032bb7bd

docker容器总共有四种网络模式

| 网络模式           | 命令                      | 描述                                                         |
| ------------------ | ------------------------- | ------------------------------------------------------------ |
| host模式           | -net=host                 | 容器与宿主机共享网络Namespace                                |
| container模式      | -net=container:NAME_OR_ID | 容器与其他容器共享网络Namespace，如k8s的一个pod内的容器就是container模式，互相共享网络namespace |
| none模式           | -net=none                 | 容器自身有独立的网络Namespace，但需要手动对其进行配置        |
| 【默认】bridge模式 | -net=bridge               | 容器自身有独立的网络Namespace，将其通过一个宿主机统一指定的网关接入一个docker子网络。类似虚拟机的桥接模式。 |

- bridge模式（默认）
  
默认情况下，docker采用bridge模式进行容器网络配置。docker主进程启动时，虚拟一个叫做docker0的网卡，并以此建立一个docker子网。
  之后每个容器建立时，都从这个子网中分得一个IP。宿主机内的所有容器都处于该子网中。容器间通信时，docker0被视作交换机进行通信中继服务，容器向外网访问时，则将宿主机视作网关路由器进行访问。

  <img src="https://upload-images.jianshu.io/upload_images/13618762-f1643a51d313a889.png?imageMogr2/auto-orient/strip|imageView2/2/w/1083/format/webp" alt="img" style="zoom:50%;" />

- host模式
  
==通过host模式建立的容器，其进程、文件系统等用Namespace等技术和宿主机隔离开，但是不进行网络的隔离==。由于容器本身就是一个进程而已，所以其可以和宿主机的任何进程一样使用宿主机的网卡。换言之，如果docker容器监听端口，那么是直接在宿主机IP上监听，外部访问这个端口也就访问了容器内部；反过来，容器内部也可以利用宿主机的IP访问外部网络。
  
==host模式可以最大限度地提升网络传输效率，并且配置直接方便，但是安全性、隔离性方面不好。==
  
<img src="https://upload-images.jianshu.io/upload_images/13618762-a892da42b8ff9342.png?imageMogr2/auto-orient/strip|imageView2/2/w/698/format/webp" alt="img" style="zoom:50%;" />
  
- container模式
  ==container模式下，容器建立时需要指定另一个已经配置好网络的容器。新容器与该容器共享网络Namespace。==通常被指定的这个容器是桥接模式的，即处于宿主机建立的docker0网络中。新容器可以通过该容器的IP与外界交互信息。而两个容器之间通信则可以通过local网卡进行。

  显然，此时新容器与既存容器形成一个容器集群。因此k8s等集群服务中采用这种模式。

  ==与桥接模式的子网不同，container模式的子网的网关是一个容器，而不是宿主机本身。==

  <img src="https://upload-images.jianshu.io/upload_images/13618762-790a69a562a5b358.png?imageMogr2/auto-orient/strip|imageView2/2/w/695/format/webp" alt="img" style="zoom:50%;" />
  
- none模式
  none模式下建立起的新容器，只有一个lo网卡。即在网络上其不与宿主机或其他容器有关联。可以通过手动配置进行关联。

  none模式可以保证容器在网络方面的绝对安全和绝对隔离。

  <img src="https://upload-images.jianshu.io/upload_images/13618762-3fd41778faebcef5.png?imageMogr2/auto-orient/strip|imageView2/2/w/723/format/webp" alt="img" style="zoom:50%;" />



# 镜像

> https://zhuanlan.zhihu.com/p/92802421

## 镜像原理

本来想把镜像的层等概念的介绍放在这里，不过上章全讲完了。正如上所述，镜像本质上是一个个层的堆叠。每个层保存的是增量信息，其好处很多，比如我有一个`v1.0`的镜像，现在我想要拉`v1.1`镜像用，`1.1`与`1.0`相比，肯定差别没那么大，所以其实大多数层都可以共用，我可以用很少的时间，把增量层拉过来，就构建起了`v1.1`的镜像了。

镜像相关的命令就不多说了。总的入口是`docker images`和`docker image`。

### 基础镜像中的内容

创建docker镜像总是要基于一些OS层面的基础镜像如`Ubuntu`或者`CentOS`之类的。
但是这些基础镜像也并非保存了完整的OS（否则和VM没啥区别），其只保存OS包含的配置、文件、目录等，但不保存内核。
这也是说docker容器依赖OS内核的原因，Linux容器就没有办法在Windows的docker引擎上跑，因为其依赖的内核Windows提供不了。

## Dockerfile

Dockerfile是一个文本文件，但是其内写了一行行用于构建docker镜像的指令。
这里，构建是指基于一个基础镜像，再加上自己想要往里加的内容、配置、挂载卷、进入点等。通过命令`docker build`可以根据指定Dockerfile的指令构建镜像。

使用Dockerfile进行镜像构建时，每一条指令都会建立一个新的层，不论这个指令是否引入了新的文件。

常用的Dockerfile中的指令包括

| 指令                                      | 说明                                                         |
| ----------------------------------------- | ------------------------------------------------------------ |
| FROM 基础镜像                             | 指出该Dockerfile基于的基础镜像是什么                         |
| RUN 命令 或者 [命令, 参数1, 参数2, ...]   | 构建镜像过程中执行一些命令                                   |
| COPY [--chown=user:group] 源路径 目标路径 | 将本地某文件复制到镜像中                                     |
| ADD [--chown=user:group] 源路径 目标路径  | 将本地或者远程的文件复制到镜像中，如果文件是tar包还会自动解包 |
| ENV key1=value1 key2=value2 ...           | 设置镜像中的环境变量                                         |
| WORKDIR dir                               | 指定工作目录（必须是提前创建好的                             |
| USER user                                 | 切换执行后续命令的用户（必须是提前创建好的                   |
| CMD 命令 或者 [命令, 参数1, 参数2, ...]   | 指出 构建镜像完成后，用这个镜像启动容器时，启动的命令。即`docker run xxx(一堆参数) bash`里的那个`bash`。另，若`docker run`命令后面手动写了启动命令，那么以那个为准，CMD作废。<br />如果Dockerfile中有多个CMD，以最后一个为准。 |
| ENTRYPOINT [命令, 参数1, 参数2, ...]      | 和CMD很类似，只是<br />1. ENTRYPOINT指出的命令不会被`docker run`手动写的命令覆盖，而是将手动写的部分当做参数附加在ENTRYPOINT的后面。<br />2. ENTRYPOINT常和CMD结合起来使用。ENTRYPOINT作为固定参数，CMD指出动态参数 |

### 几个指令的辨析

- COPY和ADD

  两者区别在于COPY是只能将本地已有的文件拷贝到镜像中，而ADD可以添加远程URL指向的文件。
  此外ADD命令还能自动解开`.tar`文件。

- RUN和CMD

  RUN提示的命令是创建镜像时运行的，而CMD提示的命令是创建完镜像后，基于镜像启容器时运行的进入点命令。CMD常常和ENTRYPOINT配合起来使用。

- ENTRYPOINT和CMD
  1. ENTRYPOINT无法被`docker run`手动输入的命令覆盖，而CMD可以
  2. ENTRYPOINT用于指出定参，常和CMD配合来拼接命令。比如ENTRYPOINT写`["nginx", "-c"]`，而后CMD写`/etc/nginx.conf`之类的。结合起来就变成了完整命令`nginx -c /etc/nginx.conf`。
     CMD在不带ENTRYPOINT的时候，默认用`sh -c`作为前缀来执行相关命令。

# 数据卷

我们知道启动容器时`-v`参数可以绑定宿主机目录与容器内目录，即挂载数据卷。

`-v`参数的值中可以最后再加上`:ro`或者`:rw`这两个flag，分别标识只读挂载卷和读写挂载卷。如`-v test_con:/home/back:/root:ro`。
通常都会要指定`<宿主机路径>:<容器内路径>`，若没有前者，即`-v <容器内路径>`时，此时docker会先创建一个临时目录：`/var/lib/docker/volumns/[VOLUMN_ID]/_data`，将这个目录挂载到容器内。

当有容器A挂载了若干个数据卷，此时通过`docker run`启动容器B，可以指定参数`--volumns-from A`来挂载和A中一模一样的几个数据卷。
在创建很多容器进行容器集群创建的时候，给所有容器都加上这个参数，便可以让多个容器有共同的挂载卷。==这也是一种实现容器间通信的办法==。

## 数据卷部分原理

在对数据卷的原理进行说明前，需要再梳理补充一些容器文件系统方面的知识。

### 联合挂载

从一个镜像启一个容器的过程中，我们知道镜像提供了容器绝大多数文件内容。可镜像又是分成很多层，其内容分散在以目录为单位的各个层中。
而最终通过镜像启动的容器，这些内容都是统一呈现的，并不带层级关系。这里就用到了所谓的联合挂载技术。

联合挂载本身，和Cgroup等类似，是一个Linux实现的挂载技术，支持把不同目录下的文件合并在一起挂载如：

```shell
$ tree
.
├── A
│  ├── a
│  └── x
└── B
  ├── b
  └── x
$ mkdir C
$ mount -t aufs -o dirs=./A:./B none ./C
$ tree ./C
./C
├── a
├── b
└── x
```

上述就是将A和B两个不同目录下的内容给联合挂载到C中。此时看起来，文件a,b,x似乎是同级别的，但实际上他们来自不同出处。

类比到docker中，每个层其实就是上面的A或者B目录，通过将所有层都联合挂载将其整合成一个整体的容器文件系统。
这里联合挂载总共有两种实现方式。

第一种是传统的联合文件系统UnionFS，一个具体实现是叫aufs的东西，其做法是在`/var/lib/docker/aufs/mnt`中建立一个容器专用目录，然后将镜像层与容器层联合挂载到这个目录下，形成一个整体。

第二种则是`overlay2`的做法，更准确地说，其实`overlay2`的联合挂载形成的应该称作overlayFS而非UnionFS了。其直接在`/var/lib/docker/overlay2`中的各个层里维护了层级关系，并且在启动容器时逐级挂载，形成统一文件系统。

aufs和overlay比较，后者由于不需要经过一个额外的联合挂载点目录作为中介，整体更快，性能更好。

### chroot

不论是通过aufs也好，overlay也好，得到一个完整的文件系统后，还没完。实际上这部分工作可以认为是创建了一个mount namespace。但尚未生效。
mount namespace相比于其他的，更特殊一些。因为它需要通过`chroot`来激活。

`chroot`作为一个命令，也是Linux上已经有的东西了。其作用就是让一个进程将某个指定目录作为根目录，换言之进程只能看到该目录及其下属内容。比如SFTP服务器限制可见目录内容就用到了这个技术。
而事实上，mount namespace或者说文件系统的隔离机制本来就是从`chroot`发展而来的。

这里 ，不严谨地描述一下mount namespace生效的过程就是，先通过联合挂载（在某个挂载点下）构建起一个完整的容器用文件系统，然后再用`chroot`或者其他有类似效果的系统调用将容器进程的根目录设置到那个挂载点下。这样容器的mount namespace，或者说文件系统隔离就生效了。

### 数据卷挂载的时机

综合以上两节，一个容器的mount namespace是这样产生的：

1. 启动容器初始化进程
2. 通过联合挂载，将镜像层、容器层统一整合成一个容器用的文件系统
3. 通过`chroot`或者其他类似功能的系统调用，将容器进程的根目录设置为相关目录，从而实现了文件系统的隔离

在第2步之后，第3步之前，此时容器进程还可以看到宿主机的所有文件，同时也已经建立起了自己的mount namespace。
若有数据卷需要挂载，docker会在这个时间点上操作。即mount namespace建立之后，chroot之前，数据卷被挂载。

至于为什么要在这个时间点，主要是因为此时chroot还未执行，所以容器看得见外面的文件内容；但mount namespace已经建立，宿主机此时已经无法感知容器内部进行了数据卷的挂载。而==后者保证了commit容器成镜像的时候，不会将挂载卷也一并固化到镜像中，因为commit是宿主机层面的行为但宿主机根本不知道有数据卷的存在。==

### 数据卷挂载原理

当docker试图将一个外部的目录如`/home`作为数据卷挂载到一个容器中的目录如`/test`上的时候，实际上是先在联合挂载点中创建`/test`目录（如果已经存在那么原内容会被覆盖），接着采用绑定挂载的方法，将`/home`挂载到容器内的`/test`上。

==注意，这个`/test`目录在容器中，属于最上层的容器层，容器可对其直接读写。这也符合我们对挂载数据卷后，能在数据卷上直接进行读写的期望相符合。==

#### 绑定挂载

上面说了很多挂载，挂载。但其实严格来说，其并不是常见的狭义的挂载。
一般意义上的挂载，我们通常想象成将一个磁盘设备挂载到某个目录，从而可以通过这个目录访问磁盘内容。

而上面提到的所有挂载都更加广义一些，指的是若将一个目录A挂载到目录B，那么可以通过进入目录B访问到目录A中的内容。
这就是Linux中说的绑定挂载（bind mount）。可以通过`mount --bind`来进行绑定挂载的操作。

绑定挂载的原理，其实就是修改了被挂载目录，其指针指向的inode：

<img src="/Users/wyzypa/Pictures/TyporaImages/Docker相关笔记.asset/95c957b3c2813bb70eb784b8d1daedc6.png" alt="img" style="zoom: 50%;" />

而原来的那个`inode_2`也没有消失，只不过没有指针指向他了，这也是为什么说原内容是被隐藏了。

### commit容器时数据卷如何处理

正如上面所说，进行数据卷挂载操作时，mount namespace已经建立，所以宿主机并不会得知某个外部的目录被挂载进了容器里。
换言之，在宿主机看来，容器内部的相关目录应该一直保持着原样（如果容器中原来没有相关目录，那么新建容器的流程是先建立空目录再挂载，此时宿主机看来相关目录一直是空）。

而又因为commit是一个宿主机层面的行为，所以自然commit的时候不会把宿主机感知不到的数据卷一并固化到镜像中。（对于新建空目录的情况，那个空目录还是会被固化到镜像中）

同样的道理，删除容器等操作，也不会影响到数据卷的独立和完整。

### 挂载卷已有数据的情况 - copyData功能

一个很有意思的问题是，如果启动容器时我将宿主机中的目录A以数据卷的形式挂载到容器中的目录B，根据A、B具体内容不同，docker会有什么样的行为表现？

- A中有数据，无B或B为空目录。这种情况最简单，无脑将A挂载到B即可。若一开始没有B目录，那么docker还会创建一个新的空B目录作为挂载点。
- A中有数据，B中也有数据。按照一般对于挂载的理解，此时B中的数据应该都会被隐藏，而接收A的挂载。但是也有例外，据说可以通过设置某些参数后，使得A、B两边内容都展现出来，原理是将B中的内容复制一份到A中。但是尚未证实。
- A中无数据，B中有数据。这种情况和上一种类似，按一般理解挂载后的B就是空的，但是也有说法说可以保留。

以上除了第一种情况，其他两种情况的后半个说法，都有待证实。



## 数据卷其他

### 数据卷与一般文件的读写差异

在容器内，写入挂载卷和写入非挂载卷目录的性能是有区别的。
写入挂载卷，由于和宿主机的目录直接同步，因此会直接写磁盘。
写入非挂载卷目录，根据上面说过的 rootfs + 可读写fs 的架构，首先要将相关文件从rootfs中复制到可读写fs中，再进行修改。因此步骤比直接写挂载卷多，因此性能稍差一些。

# 其他