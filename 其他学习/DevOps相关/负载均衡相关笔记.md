# 负载均衡工具 - Nginx

> https://www.cnblogs.com/gongjingyun123--/p/11424424.html
> https://www.cnblogs.com/gongjingyun123--/p/11410429.html

## 基本信息

Nginx是一个开源的、轻量化、微核心的HTTP/代理/邮件服务器软件。其特点是==占用内存少，并发能力强，扩展力强，稳定性好等==。
在诸多Nginx的功能中，主要用的还是其作为HTTP和代理服务器的时候。

- HTTP服务器Nginx

  ```
  可以作为一个静态文件服务器（向客户端的请求返回静态页面信息或者是JS，CSS这种当代网页上需要的静态文件）
  另一方面，如果Nginx想要作为动态网页的服务器，那么必须配合后端的CGI程序进行动态的页面渲染。
  总之，Nginx只负责传输数据而不负责生产数据。
  ```

- 代理服务器Nginx

  ```
  一般，我们将整个外网互联网看做是一个具有丰富资源的大海。而处于互联网中的一台小web服务器显然只是一条小河。小河流向大海。
  因此，从web服务器向外访问，顺着流向称为正向代理；外边网络的主机代理访问到web服务器，是逆着流向的，称为反向代理。
  Nginx可以做正向或者反向代理。在反向代理的场景下，Nginx还能做负载均衡（分发流量）和地址重定向（转发流量）。
  ```

- 动静分离中的静态服务器

  ```
  上面说到，Nginx负责传输数据。显然，将动态数据和静态数据分开，静态数据用Nginx提供，动态数据则用其他更加适合的应用服务器提供，就可以做到动静内容的解耦。
  ```

  

## Nginx的系统架构

![](https://pic1.zhimg.com/80/v2-f566edf999525f675698691a39e68218_1440w.jpg)

上图是一个基本的nginx系统架构图。每一个worker是nginx中最关键的部件。其内部代码存在一个循环，不断地接收、读取、写入套接字并以此与客户端进行通信。

## Nginx的四/七层负载均衡

Nginx的负载均衡，或者说转发，可以工作在两种层面上。四层和七层。

顾名思义，四层转发只会将网络数据拆包到第四层，解析出其IP以及端口。然后根据配置要求转发到特定的后端服务器上。一个四层负载均衡（转发）的配置案例：

```nginx
stream {
    log_format  proxy '$remote_addr $remote_port - [$time_local] $status $protocol '
                      '"$upstream_addr" "$upstream_bytes_sent" "$upstream_connect_time"' ;
    access_log /var/log/nginx/proxy.log proxy;

    upstream ssh_7 {
            server 10.0.0.7:22;
    }
    upstream mysql_group {
            server 10.0.0.51:3306;
    				server 10.0.0.52:3306;
    }
    server {
            listen 5555;
            proxy_connect_timeout 3s;
            proxy_timeout 300s;
            proxy_pass ssh_7;
    }

    server {
            listen 6666;
            proxy_connect_timeout 3s;
            proxy_timeout 3s;
            proxy_pass mysql_group;
    }
}
```

监听项的总起是server，说明这是一个四层负载均衡。看配置就知道，每个server只关注listen了哪个端口，而对于更高层的信息并没有兴趣。按上面配置，当有人访问Nginx服务开启的5555端口时，会被转发到`10.0.0.7:22`；类似的，若访问6666端口，则会转发给`mysql_group`这个包含了两个mysql节点的组。至于组中选择哪个节点来承载，这个和选择的调度算法有关，这一方面下面再说。

与四层负载相对的，是七层负载。七层是应用层，应用层协议有很多，但是讲到Nginx的七层负载，通常是指基于HTTP协议的七层负载。下面是一个配置案例：

```nginx
upstream web_7_8 {
        server 172.16.1.7:80;
        server 172.16.1.8:80;
        check interval=3000 rise=2 fall=3 timeout=1000 type=tcp;
}

server  {
        listen 80;
        server_name zh.gjy.com;
        index index.php  index.html;

    location / {
        proxy_pass http://web_7_8;
        proxy_set_header Host $http_host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_http_version 1.1;
    }
    location /status {
        check_status;
    }

}
```

可以看到，这次负载均衡发生在server字段内部的location字段中。换言之，决定负载的因素不再是server层面，而是location指定的具体URL层面了。==既然要解析出URL进行判断，说明这就是一个七层负载了==。

### location匹配规则

> 一个可以简单验证nginx配置location优先级的网站：https://nginx.viraptor.info/

上面的配置中，一个请求可以根据其URL被分配到不同的location，从而被不同的worker逻辑处理。而这个分配过程，其实存在一个location的匹配规则问题。

通常，配置写成`location [pattern] {...`。而pattern就是匹配规则的核心。这里的pattern又可以细分为一下几种类型：

| 形式                                                   | 说明（优先级从高到低）                     |
| ------------------------------------------------------ | ------------------------------------------ |
| = /xxx                                                 | 完全匹配、优先级最高                       |
| ^~ /xxx                                                | 优先前缀匹配，同优先级长度长的pattern优先  |
| ~  /xxx 或者  ~* /xxx (后者表示不考虑大小写的正则匹配) | 正则匹配，同优先级配置在前的优先           |
| /xxx                                                   | 常规前缀匹配，同优先级长度长的pattern优先  |
| /                                                      | 通用匹配，当上面所有规则都无法匹配时走这条 |

当发来一个Query，Nginx的匹配处理逻辑是这样的：

1. 首先，判断有没有“完全匹配”，若有直接返回。
2. 其次，==扫描所有“优先前缀匹配”和“普通前缀匹配”，若有可以匹配的，则记录下那个pattern，若有多个，则记录pattern最长的那个（注意，只是记录，还不返回）。若该记录是“优先前缀匹配”，则直接返回==。
3. 而后，扫描所有正则匹配。扫描的顺序为正则匹配pattern从上至下定义的顺序。若发现有匹配，则直接返回。
4. 再后，若步骤2中有记录的pattern（到这个时间点，这个pattern肯定是“常规前缀匹配”的），返回。
5. 最后，到这步，说明前面的匹配全部失败，因此走通用匹配。

举个例子，若配置的location规则如下：

```
1. location / {
2. location ~ ^/a/ {
3. location ^~ /a/ {
4. location ^~ /a/b/ {
5. location ~ .txt$ {
6. location /a/b/c.txt {
7. location = /a/b/c.txt {
8. location /a/b/ {
```

假设发来的Query的URI是`/a/b/c.txt`，按照上面的规则，首先当然是匹配了“完全匹配”的7。
若注释掉7，则接着会进行规则的第二步，发现最长的是6，而6是“常规前缀匹配”，因此继续做第三步正则匹配，正则匹配发现2能匹配，因此匹配2。若此时再注释掉2，则落到5。

以此类推，如果再注释掉5，那就返回6了，因为6是“*前缀匹配”中最长的。接着是4，接着是8，接着是3，最后是1。

总结一下，上面8条，按照逐个注释的做法，匹配的顺序是`7 2 5 6 4 8 3 1`。

## Nginx调度算法

Nginx也有和LVS类似的几种调度算法：

- 轮询（RR）
- 加权轮询（WRR）
- IP哈希：按来访IP的哈希值计算分配worker，可以使同一个IP的访问总是落在同一个worker上
- URL哈希：七层负载时可用，将访问的URL作为依据计算哈希值，决定分配的worker
- 最小连接（LC）：分配给当前连接数最小的worker

以上是一些Nginx自带的实现了的调度算法，还可以通过安装一些其他第三方模块，来使用更加复杂的策略。

具体的，调度算法在Nginx配置文件的`upstream`配置块中配置。下面是一个例子：

```nginx
upstream web_group{
  192.168.0.1:80 weight=5;
  192.168.0.2:80;
}
```

默认什么算法都不指定时，使用RR且默认所有weight都是1。把其中的某些weight显式修改后，显然就变成了WRR。如果想用其他算法，可以如下在配置中指出：

```nginx
upstream web_group{
  ip_hash;
  192.168.0.1:80;
  192.168.0.2:80;
}
```

### 调度时Nginx后端worker的辅助状态

调度算法的执行，除了Nginx本身，另一方面不得不考虑的重要因素是后端各个worker的状态。
Nginx配置支持手动将某些worker置为一些特殊状态。处于特殊状态的worker节点将被Nginx特别对待，从而实现更加灵活的调度。
具体的，可以为worker手动设置这些状态：

| 状态                    | 描述                                                         |
| ----------------------- | ------------------------------------------------------------ |
| down                    | 该worker不参与Nginx调度，相当于注释了这个worker              |
| backup                  | 预留备份服务器。优先调度其他worker，当其他worker都不行才启用他 |
| max_fails, fail_timeout | 允许请求失败的次数。请求失败达到指定次数后就将服务器置状态为down一段时间，时间长度由fail_timeout指定 |
| max_conn                | 指定最大连接数                                               |

配置案例：

```nginx
upstream load_pass {
    server 10.0.0.7:80 down;
    server 10.0.0.8:80 backup;
    server 10.0.0.9:80 max_fails=1 fail_timeout=10s;
}
```

上述配置中，`10.0.0.7`不会被分配任何请求因为其是down。
`10.0.0.9`则是一旦请求失败一次就将其置down，持续10秒。
这段时间内，因为没有其他选项，所以`10.0.0.8`将被启用。

# 负载均衡工具-LVS

> https://blog.csdn.net/weixin_40470303/article/details/80541639

## 基本信息

LVS全称Linux Virtual Server。曾经是一个开源的负载均衡项目，目前已经标准化并集成在了Linux内核中，换言之，目前LVS全程运行在内核态中。LVS工作时的架构图大概是这样的：

<img src="/Users/wyzypa/Pictures/TyporaImages/负载均衡相关笔记.asset/201806012200210.png" alt="img" style="zoom:50%;" />

外部的请求到来之后，LVS会根据其内部已经配置好的算法，进行请求的分发，从而实现负载均衡。

## LVS负载均衡基本原理

> https://www.cnblogs.com/xiaoyuxixi/p/12158489.html

上面说到LVS在Linux内核中实现。更确切地说，其是在Netfilter框架的基础上进行的一些进一步的改进。

### Netfilter

iptables作为Linux经典防火墙已经比较熟悉了。iptables中存在着诸如PREROUTING, INPUT, FORWARD, OUTPUT, POSTROUTING几个网络包传输时的节点，称之为链。这些概念隶属于一个内核中叫做Netfilter的框架。

Netfilter允许外部在各个链上进行hook函数的注册，即某个包到达某个链时调用什么函数处理。iptables只不过是一个用户态的工具，可以使得用户在各个链上注册一些简单的通过/拒绝的hook。整体的链路细节图如下：

<img src="/Users/wyzypa/Pictures/TyporaImages/负载均衡相关笔记.asset/1592465-20200106205401425-1544971350.png" alt="img" style="zoom:67%;" />

各个链的作用如下：

- PREROUTING ：刚刚进入网络层，还未进行路由查找的包，通过此处
  INPUT ：通过路由查找，确定发往本机的包，通过此处
  FORWARD ：经路由查找后，要转发的包，在POST_ROUTING之前
  OUTPUT ：从本机进程刚发出的包，通过此处
  POSTROUTING ：进入网络层已经经过路由查找，确定转发，将要离开本设备的包，通过此处

### LVS的工作地点

LVS，本质上是在INPUT链注册了一个叫做`ip_vs_in`的hook函数。整体流程如下：

<img src="/Users/wyzypa/Pictures/TyporaImages/负载均衡相关笔记.asset/1592465-20200106205434194-1257182793.png" alt="img" style="zoom:67%;" />

当外界发包到本机时，首先经过PREROUTING链，然后进行路由分析判断。如果发现是发往本机的，则将包放入INPUT链。

此时，IPVS hook函数会进一步检查包是否发送给LVS服务。若是，则调用相关的LVS处理函数拆包并将包头中的target ip与端口进行修改。修改完成后，直接将包放到POSTROUTING，准备发送给后端真实的服务器worker。

上述流程只是最简单的一个LVS接收请求并转发给后端worker的过程。实际操作过程中，还需要考虑很多因素比如返回客户端的包里，头部源IP应该写LVS的IP等等。这些涉及到了LVS的工作模式，下面会讲。

另外，参考文章给出了针对各个工作模式下链级别的LVS的操作原理，如果想细致了解，可以看。

## LVS的工作模式

### NAT模式

顾名思义，LVS在这种模式下起到NAT的作用。具体而言，这种模式下，LVS通过对网络包头部的IP、端口信息进行修改后转发的方式，实现其目的。

通常，NAT下的LVS有两块网卡，一块连接公网，另一块连接后端服务器集群局域网。公网网卡IP称为虚拟IP（VIP）。客户端请求发送到VIP之后，LVS根据内部的调度算法选出一台后台的工作服务器，确定其IP以及端口。然后将客户端发来的包的头部中IP和端口信息进行修改，并通过另一个网卡转发到局域网内。

当后端服务器处理完成后，将包返回给LVS。LVS再度修改头IP为自身的公网IP与端口，并返回给客户端。

==NAT模式下，一个很明显的缺点是，所有的网络请求、应答都要经过LVS中转处理==。当后端服务器较多，请求密度大时LVS可能成为系统性能的瓶颈。

### TUN模式

TUN（Tunneling）模式全称隧道模式。通常客户端的请求数据总是比服务器的应答数据少很多。因此，TUN的优化思路就是对于请求数据做负载均衡，而后端服务器处理完之后直接将应答数据返回客户端。

==这种模式要求每个后端服务器也有独立的与公网通信的能力，因此每个后端服务器都有自己的公网IP==。
另一方面，==为了能够在应答包中也带有LVS的IP信息（保证客户端知道这个包的来源），这里采用了一种叫做IP隧道的技术==。
IP隧道，简单来说就是在网络包的外面，保留原请求IP信息的基础上，再套一层请求IP。
本质上来说，LVS收到请求包后，不是像NAT中修改请求IP信息，而是额外套上一层后端服务器的IP。这样，后端服务器处理完返回应答数据时，应答IP写成LVS的IP而不是自己的，从而实现和客户端的通信。

<img src="/Users/wyzypa/Pictures/TyporaImages/负载均衡相关笔记.asset/20180601224445953.png" alt="img" style="zoom:50%;" />

### DR模式

DR（Direct Route）全称直接路由模式。上面TUN模式中使用了IP隧道技术，其主要目的就是让后端worker能够在返回应答包时，可以将LVS的VIP包括在包中，从而使得客户端无法感知到集群的存在。

而DR模式则是另一种实现该目的的办法。这里，将LVS和所有worker共享一个VIP。此时，为了保证指向VIP的请求总是能先到LVS而不是worker上，还需要将worker的VIP对应网卡的ARP应答机制关闭（理论上来说，不关闭ARP广播，直接将请求导向worker进行处理返回，并不影响业务逻辑，但是这会导致链路上的ARP表混乱）。==而LVS和worker间则通过二层mac直接通信，即LVS直接将请求包在二层的目的mac地址改为指定的worker的mac地址==。

总的来说，请求先到达LVS，然后LVS根据算法计算选择一个worker。然后修改帧的目的mac地址，通过二层直接将数据传给worker。
worker应答后，直接将应答包返回给客户端。由于worker也有和LVS相同的VIP地址，所以无需额外的处理，客户端可以直接识别。

<img src="/Users/wyzypa/Pictures/TyporaImages/负载均衡相关笔记.asset/20180601230414826.png" alt="img" style="zoom:50%;" />



上面对三种工作模式做了最简单的流程描述。如果需要在链级别对各个模式做进一步的了解，可以参考这篇文章：

> https://www.cnblogs.com/xiaoyuxixi/p/12158489.html

## LVS的worker调度算法

无论LVS使用的是上面介绍的哪种工作模式，其都需要使用一套算法选择出一个worker来进行请求的处理。这章就介绍LVS中实现的若干种调度算法。

- 轮询（RR）： 不解释
- 加权轮询（WRR）：不解释
- 最小连接（LC）：LVS中维护一个各个worker活跃连接数的表。将请求分配给当前连接数最小的worker
- 加权最小连接（WLC）：将请求分配给连接数尽量小、权重尽量高的服务器
- 基于局部的最小连接（LBLC）
  常用于Cache集群。为了提升Cache命中率，LVS内部维护一个请求IP与worker间LRU的对应关系，将请求分发到该IP最近访问过的worker上。
  若该worker已经满负荷，或者不存在了，则再根据最小连接算法选择一台服务器。

- 散列（hash）
  LVS内部维护一个哈希表，记录了请求源IP与worker的对应关系。
  请求到来时分发给其对应的worker，若worker满负荷，则返回空（即暂时不处理请求）
- 最短延迟期望（SED）
  基于WLC算法。比如ABC三个机器的权重是1,2,3，而当前各个机器的连接数也恰好是1,2,3。此时WLC算法认为三个机器差不多，都可以分配。
  而SED算法计算`(当前连接数 + 1) / 权重`作为接收新请求需要耗时的期望值。选取期望值最小的。
  比如上述例子，经过SED算法算出ABC三个机器的期望值分别是`2, 1.5, 1.333`，因此选择C作为worker。

## LVS负载均衡的特点

看完上面的叙述差不多就知道了，LVS的负载均衡，其主要改动的是三层网络包的头（甚至是二层帧的头），所以严格来说LVS工作在第三层。
相比较下，Nginx的负载均衡通常采用配置proxy_pass以及upstream服务集群来进行负载，有时还会根据URL的不同来转发到不同的worker。因为涉及到URL的解析，所以不可避免地要解析到应用层，换言之，这种模式下的Nginx是七层负载均衡。

如此，三层、二层的基于LVS的负载均衡，自然是比七层的Nginx负载均衡效率高多了。

# 基于DNS的负载均衡

在网络篇笔记中，提到过DNS也可以用于作为负载均衡。这里详细说一说。
首先，基于DNS的负载均衡原理并不复杂，当你有多个worker服务器对应着多个不同的IP，而对于用户来说我只访问一个域名。因此在DNS层面，可以将域名对应其多个服务器，并且在确定DNS解析结果时将流量分发到不同的worker上即可。

使用基于DNS的负载均衡，好处在于不需要额外的负载均衡组件，且实现起来十分方便。
通常，大型服务会将worker部署在不同地域的机房里做到异地多活。而地理区域的远近此时就会比较影响响应速度了。此时常用DNS负载均衡将请求分发到尽量靠近用户地理的worker，提升响应速度。

当然，DNS负载均衡也有很多缺点，比如

- 如果worker服务器有变动如增减或者改了IP，DNS服务器上的信息更新会有时滞
- DNS负载均衡的调度算法基本只有RR方式，无法灵活地分配负载
- DNS的解析列表，虽然根据不同供应商而不同，但通常都有上限。比如阿里云DNS只支持配置10个不同的后端IP，因此集群规模受限。

也是因为上述缺点，实际上的生产中，DNS负载均衡并不常用。用了也通常是上面所述的那种，针对地理距离优化时用到。

DNS是个应用层协议，所以理论上DNS负载均衡也是一个七层的负载均衡。

# 其他

## 硬件负载均衡

上面说的，无论是用Nginx，LVS还是基于DNS，这些方案都是基于软件进行的负载均衡。除了软件，还有硬件负载均衡方案。
硬件负载均衡需要的硬件，比较经典的有两种，`F5`和`A10`。只要通过网线将后端的worker服务器与这些设备相连后，之后就不用管了，硬件会把所有工作都做稳妥。

硬件负载均衡的优点是

- 功能性能强大，可以在各层实行负载均衡，并且因为是直接上硬件，所以性能超过任何软件负载均衡
- 稳定性高
- 自带一些安全措施比如防止DDos攻击等

而硬件负载均衡最大的缺点就是一个字，贵。所以通常只有要求很严格且不差钱，比如银行之类的公司，才会使用硬件负载均衡方案。

## 负载均衡注意点