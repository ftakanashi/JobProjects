> 这是读《逆袭进大厂》（InterviewGuide）的笔记。计网篇。

# ⭐️OSI的七/四层模型

现代网络通信通常将网络以分层模型建模。分层的好处是层之间隔离性、灵活性、易于维护、促进标准化等。

七层模型从上到下以及各个层级下的代表性协议分别是：

```
应用层     HTTP, FTP, DNS, TELNET, DHCP, POP3...
表示层     JPEG, ASCII...
会话层
传输层     TCP, UDP
网络层     IP, ICMP...
数据链路层   MAC, ARP, RARP...
物理层
```

将上三层合并可以成为新的“应用层”，将下两层合并可以成为“网络接口层”，每一层数据单位的名字是:

```
应用层
传输层				段（segment）
网络层				包（package）
网络接口层		 帧（frame）、比特流
```

两者的关系：四层是七层的一种简化，也是实际使用如TCP/IP协议族下的模型。七层模型是理想化理论模型。

## 每层作用和设备

 ![image-20210706105927695](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂 计算机网络.asset/image-20210706105927695.png)

## 每层的工作状态

TCP/IP五层模型中，应用层工作在用户态，传输层及以下工作在系统态。

# DNS基本

**DNS是应用层协议，基于UDP进行传输。**DNS就是做域名解析的协议。

## DNS多级缓存与查询过程

解析过程中会涉及很多缓存内容，若缓存有命中，则可以不去DNS服务器请求解析，加快解析速度。缓存包括了：

```
浏览器缓存
系统缓存（如/etc/hosts等）
路由器缓存
ISP服务器缓存
根域名服务器缓存
顶级域名服务器缓存
主域名服务器缓存
```

当试图解析一个域名时，系统从上至下依次去访问这些缓存。前两个很好理解，就是计算机本地带有的一些缓存信息。

第三个路由器缓存，是指路由器本身通常会带有一些域名解析记录，从而路由器本身可以作为局域网内的一个DNS服务器。对局域网内的DNS服务器发起解析请求，只需要ARP解析其IP成物理地址，然后再局域网内发起访问即可。

若局域网内DNS没有找到解析记录，则转到ISP服务器，这是指网络接入商提供的一个DNS解析服务器。通常一般的DNS解析到这里大概率能找到答案。若还是没解决，则ISP服务器将会和国际上通用的DNS服务器进行联系。现在假设访问的地址是`mail.cctv.com`。这个地址可以分成三个层次：`mail.cctv.com`是三级域名、`cctv.com`是二级域名、`.com`是顶级域名。

第一步，ISP服务器先行联系根域名服务器。根域名服务器不直接返回解析结果（这和解析模式是迭代还是递归有关，这里以迭代为例说明），而是返回一个顶级域名服务器地址，比如这例中的负责`.com`系列顶级域名的服务器。
第二步，ISP服务器再请求顶级域名服务器，顶级域名服务器也不直接返回解析结果，而是返回一个负责了`mail.cctv.com`的主域名服务器地址。
第三步，请求主域名服务器，这次主域名服务器中有相关解析记录，返回给ISP服务器。ISP服务器将IP记录到本地缓存中，并且返回给用户。

## 递归解析与迭代解析

上述ISP服务器解析地址的过程中，其实存在两种方式。递归和迭代。

递归方式解析，指ISP服务器接到请求后，去请求更高层级的服务器A。而服务器A接到请求后若无法自己解析，再去递归地请求更更高层级的服务器B。如此一层层递归下去，直到找到后再一层层返回到ISP服务器。

迭代方式解析，指ISP服务器接到请求后，去请求更高层级的服务器A。A接到请求后知道接下来要去请求B，但是它不自己做而是将B告诉ISP服务器，意思是“你自己去找他”。如此，是ISP服务器跑好几个办事窗口最终找到记录，称为迭代方式。

## 基于DNS的负载均衡

一般负载均衡，在DNS层面是无法感知的，通常DNS都将域名解析到负载均衡器上。

而基于DNS的负载均衡是指，在DNS服务器上设置同一个域名解析到多个不同的IP地址，这些IP地址形成一个后端集群。当有大量访问来袭时，按访问顺序轮转返回解析结果，从而尽可能把多个请求平均分配到后端各个IP上。

这也就是基于DNS的负载均衡策略。

## DNS中的UDP和TCP

不同层级的DNS服务器之间，进行数据的交流和同步，基于TCP。因为TCP可靠且一次性可以传输的数据量大。

客户端向DNS服务器发起DNS解析请求时，因为通常只要一次性通信并且数据量不大，所以使用更快的UDP。

# ⭐HTTP基本

## HTTP的各类方法

前三种方法由HTTP/1.0定义，后来HTTP/1.1又追加定义了六种方法

| 方法    | 描述                                                         |
| ------- | ------------------------------------------------------------ |
| GET     | 请求指定信息的页面                                           |
| HEAD    | 类似GET，只是返回响应无响应体，只获取报头                    |
| POST    | 提交数据并请求服务端进行处理。POST会发两个包，一个包含头信息，一个包含数据信息 |
| PUT     | 提交数据（完整）覆盖原数据。                                 |
| PATCH   | 提交数据（部分字段）覆盖原数据。                             |
| DELETE  | 请求从服务器删除指定页面。                                   |
| OPTIONS | 允许客户端查看服务器当前性能。                               |
| TRACE   | 回显服务器收到的请求，主要用于测试和诊断                     |
| CONNECT | HTTP/1.1中预留给能将连接方式改为管道方式的代理服务器。       |

### GET和POST有什么区别

- 规范上来说，GET用来请求数据，POST用来修改数据
  - GET纯粹是读取数据，因此理论上是幂等的。基于此，==浏览器一般对失败的GET会自动重发==。如果你的逻辑用GET进行了数据增删改操作，可能就会引起重复操作的风险。因此还是要遵照规范进行操作。
- GET将请求参数直接合并在URL中，通常浏览器会对URL长度做限制（如2K，注意这是浏览器的限制而非HTTP本身限制，原因是处理长URL开销大且容易引入安全风险）因此GET能够提交的参数也受到限制；POST将请求放在HTTP的请求体中，理论上参数不受限制。
- GET产生一个数据包，将header和body一起发送。若服务端处理成功，则响应200；==POST产生两个数据包，先发送header，服务器响应100 continue，再发送body，服务器响应200==（注：这种行为不是HTTP协议规定，只是大多数实现是这样）。
- GET请求会被浏览器自动缓存，而POST默认情况下不自动缓存。
- POST方法和GET方法在安全性上并没有区别。虽然POST的数据没有写在明面URL上，但是其数据包并没有加密，只要截取，就能看到数据。要安全，只能用HTTPS

## HTTP中的缓存控制

为了加速响应，减少重复计算，HTTP通信中存在多种缓存机制。
离用户最近的，自然是浏览器的缓存。再远一点，代理服务器也可以有缓存。更远的，根据后台的架构，源服务器可能也会带有缓存，甚至后台就有专门进行缓存响应的缓存服务器。

在HTTP中使用/控制缓存主要通过HTTP报文中的`Cache-Control`字段。
在HTTP请求的报文头中，`Cache-Control`可以设置为`no-cache`，`no-store`等值（其他的不写了）。`no-cache`表示客户端要求服务端不直接使用缓存，而是请求源服务器获取最新数据。`no-store`表示客户端要求本次请求服务端不要保存任何数据进缓存。
在HTTP响应的报文头中，也有`Cache-Control`字段，这次比较值得注意的是其值可能是`public`和`private`。分别用来指示本次返回的缓存内容是多用户可见的，还是就当前用户可见。

> 更多HTTP缓存相关内容，可参考：https://blog.csdn.net/u012375924/article/details/82806617

## HTTP常见状态码

| 状态码 | 类别             | 类含义                             |
| ------ | ---------------- | ---------------------------------- |
| 1XX    | 信息性状态码     | 告知客户端接收了请求正在处理       |
| 2XX    | 成功状态码       | 告知客户端请求正常处理完成         |
| 3XX    | 重定向状态码     | 需要对客户端请求进行重定向再做处理 |
| 4XX    | 客户端错误状态码 | 因客户端请求的问题，无法处理请求   |
| 5XX    | 服务端错误状态码 | 因服务端内部的问题，无法处理请求   |

更具体的说说各个类别的代表性的码。

- 【100 Continue】收到请求的初始部分，要求客户端继续发送剩余部分。通常出现在比如客户端想上传一个大文件，正式上传前先发送一个请求问一下是否可行，当服务端认为可行就返回100，提示客户端可以。不行则可能返回【417 Expectation Failed】
- 【200 Success】成功，返回了数据
- 【204 No Content】成功，不返回任何数据
- 【301 Moved Permanently】资源被永久重定向到新路径，以后应该从新路径访问
- 【302 Found】资源正临时性被重定向到新路径
- 【400 Bad Request】客户端请求报文中有语法错误
- 【401 Unauthorized】客户端未提供HTTP认证信息。浏览器碰到401后通常会弹出用户名密码输入框
- 【403 Forbidden】该客户端对该资源的请求被禁止了。通常返回信息中会有更具体的禁止原因
- 【404 Not Found】请求的资源不存在
- 【405 Method Not Allowed】请求的资源不允许客户端请求的方法访问。
- 【500 Internal Server Error】服务器内部错误，通常是web程序bug了
- 【502 Bad Gateway】通常在有代理、网关服务器等情况下给出。表示其无法从上游的应用服务器获取到资源
- 【503 Service Unavailable】服务器停机维护中，无法提供服务

## HTTP报文头中常见字段

- Host: 目标主机，通常是域名或者IP
- Content-Length: 数据长度，通常出现在HTTP响应中
- Connection: HTTP/1.1以后默认都是keep-alive了，但是为了兼容老版本，通常显式地加上keep-alive值
- Accept：C端明确自己接收什么格式的内容，`*/*`为任意格式
- Content-Type：S端明确本次响应的数据格式内容，例：`text/html; charset=utf-8`
- Accept-Encoding：C端明确自己接收什么样的压缩方法，例：`gzip, deflate`
- Content-Encoding：S端明确本次响应的数据压缩方法，例：`gzip`

## Session和Cookie

HTTP协议是==无状态的==，这主要是为了让HTTP数据处理方便一些。HTTP/1.1引入了Cookie来辅助性地保存状态。
无状态，是指服务端和客户端之间无法识别对方。比如服务端接收到第一个请求并处理，若客户端发送第二个请求，服务端无法得知这个请求与上一个是同一个客户端发出的。

- Cookie

  Cookie本质是==服务器发送到用户浏览器并保存在本地的一小块数据==。之后，每次访问同一服务器时，浏览器会将这块数据一并带上。如此，服务器就可以通过Cookie中的信息识别客户端的身份，从而解决了无状态的问题。

  经常使用Cookie的场景有==会话状态管理（如登录状态等），个性化设置（个性化的主题界面等），浏览器行为跟踪==。值得注意的是，曾经Cookie常用于存储大量的客户端数据，后来随着发展，很多浏览器都单独推出了客户端数据存储功能，于是就不用Cookie来存储这么重量的信息了。

  Cookie根据有效期，可以分为会话Cookie（在关闭浏览器后失效并被删除）和持久Cookie（保存在磁盘中，在指定失效期前持久有效）。

- Session

  Session也是用于解决HTTP无状态这个缺点的。不同的是，==Session将要保存的信息以键值对的形式存放在服务端==。至于具体在哪里由架构者自己选择，通常可以存放在文件、内存，也可以存放在诸如Redis之类的软件中。

  以登录状态为例，用户发送登录请求后服务端进行校验，通过后将用户信息保存在Session中，并且生成一个Session ID，将其通过响应报文中的`Set-Cookie`头字段返回给客户端，并让客户端将这个ID保存在Cookie中。之后客户端再请求时，只要带上SessionID，服务端就可以识别客户端了。

  乍一看，Session本质上还是得要依赖于Cookie，感觉很鸡肋。实际上，Session机制下服务端和客户端之间交流的额外数据只有Session ID而没有实际的数据内容（which单纯Cookie机制中可能要传输），因此更加安全。当然，SessionID有可能会被盗用，应对这方面安全问题，通常服务端会比较频繁地更新session ID并且加二次验证之类的办法。

- 两者区别

  ```
  1. 本质上，保存位置不同，Cookie在客户端，Session在服务端
  2. 因为Session信息在服务端，更安全
  3. 因为Session信息在服务端，过多session影响服务器性能
  4. 因为Session信息在服务端，可以存储更加复杂的数据object，Cookie一般只存储ASCII码
  5. 因为Session信息再服务端，通常存储容量更大。Cookie大小、个数则受到浏览器限制
  ```



## HTTP各版本

目前，HTTP主要有了1.0, 1.1, 2.0, 3.0这些版本。简单记录下各个版本都有什么特点。

- HTTP 1.0

  最早的HTTP协议版本。很多方面都很拉胯

- HTTP 1.1

  引入了长连接以及异步请求。
  原来发送一个HTTP请求都要建立一次TCP连接，并且在第一个请求得到回应前第二个请求无法发出。
  现在一个TCP连接可以复用给多个HTTP请求，并且请求异步。

- HTTP 2.0

  2.0版本的HTTP协议自动基于HTTPS（包括了TLS协议在体系内）。
  报文头不再是每个请求必须有，头信息被存在缓存中，当发出头一样的请求，直接取缓存中数据。
  1.1中请求虽然异步了，但仍然是串行发送的。2.0中串行改为并行。
  增加了服务器推送机制，将客户端可能用到的静态文件提前推送给客户端。

- HTTP 3.0

  详细不说了，将HTTP基础的TCP改成了UDP（当然为了保证可靠传输，多加了不少东西）

# HTTPS基本

HTTPS就是HTTP + SSL/TLS。
简单说说两者的区别：HTTPS在HTTP外面套上一层安全壳，互相之间的传递的信息报文被加密从而不易被第三方读取。HTTPS需要额外的SSL证书和CA证书、端口通常是443。

## 什么是SSL/TLS

SSL是安全套接字层。他是用于加密和验证客户端-服务端通信数据的协议。其加密机制综合并用了不对称、对称加密两种方式。
TLS是SSL的另一种叫法（更准确的说是标准协会对SSL进行了标准化后得到的东西，其实两者只是同一事物的不同阶段的称呼。严格来说现在应该都叫TLS协议了，但是习惯上还是叫SSL）

### SSL的流程

1. 客户端发起SSL连接请求，协商加密算法、摘要算法等
2. 服务端发送公钥（S~pub~）给客户端
3. 客户端用S~pub~加密送通信的对称密钥C，并发送给服务端。这个密钥每个会话生成一次，也称会话密钥。
4. 服务端用私钥S~pri~对C进行解密，得到通信用对称密钥。
5. 之后，服务端和客户端对任何通信数据，都使用共同知晓的对称密钥C进行加密解密。
6. 客户端发送数据时，除了明文，还将明文的哈希值计算出来作为“摘要”，与明文共同加密传送。服务端收到后解密，并计算收到数据的摘要，比对客户端提供的摘要。两者一致则说明数据未受到篡改。

上面这个流程，并非绝对安全。比如有一个黑客劫持了2中的S~pub~然后将自己的公钥H~pub~发送给客户端。此时客户端并不知情， 就会将C用H~pub~加密发送给黑客，黑客经过解密，可以知道C的内容。然后再将其用S~pub~加密发送给真的服务端，黑客隐身。这么一来，黑客就能偷窥到之后服务端和客户端之间的通信了。

换言之，==若SSL中服务端的公钥被篡改，则还是会发生安全风险==。这个问题的解决办法是将公钥放进一个由第三方认证过的证书。可以这么理解：第三方是可信任的，而他会把上述可能被篡改的公钥本身 用第三方自己的私钥加密。作为用户，我们可以获取到第三方的公钥，只要这个公钥可以解密这段内容，那么这段内容一定可信，因此就一定是可信的，没有被篡改的。

## HTTPS的好处

- 加密（防窃听）：由会话密钥的加密实现
- 认证（防伪装）：由CA证书的背书实现
- 完整性保护（防篡改）：由摘要算法+摘要比对实现

# ⭐️TCP基本

TCP是最常用的传输层协议之一，与网络层的IP协议形成了实际上现实中互联网的基石TCP/IP协议组。

## TCP报文段内容

TCP段分为头部和数据部分。数据部分是指从上面应用层封装好传递下来的数据，而头部则是本层传输层额外包裹的一些信息。分析TCP协议的各种东西时，主要关注头部中的信息及其变化。头部数据大概长这样：

![](https://img-blog.csdn.net/20180717201939345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4OTUwMzE2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- 源端口、目的端口：顾名思义

- 序号(**seq**, 32bit)：主要标识本数据段在整个TCP数据流中的位置。通信建立时，seq初始化成一个随机数（Initial Sequence Number, ISN）。之后每个段的seq都是`初始值 + 已经发送的字节数`。这样就保证了段之间互相有序。

- 确认号(**ack**, 32bit)：接收方对发送方的TCP响应，为收到的段的序号+1

- 标志位(6bit)：这是六个1bit的开关位，如下：

  ```
  URG: 表示紧急指针是否生效
  ACK：表示确认号是否有效，即表示段是否是一个回应确认的数据段。事实上除了建立连接和断开时部分数据段，所有数据段这个位置都是1
  PSH：完整写法是PUSH，提示接收端立即从缓存中读取数据
  RST：要求对方重新连接（复位报文段）
  SYN：表示请求建立一个连接（连接报文段）
  FIN：表示关闭连接（断开报文段）
  ```

- 窗口 (16bit)：告知对方本机的缓冲区还有多少剩余。解决流量控制问题。
- 校验和(16bit)：用于检验报文段有无数据损坏。
- 紧急指针(urgent pointer)：指一个偏移量。序号+这个偏移量，是紧急数据的开始。紧急数据，指正常通信过程中，由于突发情况而想要高优先级传输给对方的信息。比如想要提前中断传输。

### 大小限制

TCP/IP协议族下的一个传输单元（IP数据包）最大大小称为**MTU (maximum transmission unit)**，通常由硬件决定，大概在比如1500字节。
除此之外还有一个最大分节大小 **MSS (maximum segment size)**，这个值是指一个IP数据包中数据部分最大的值。通常等于MTU减去TCP头部（20字节）和IP头部（20字节）两部分信息。

## TCP连接

### 三次握手

![](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/70.png)

- 第一次握手

  ```
  客户端发出TCP段，SYN=1，seq=初始化随机值x
  客户端状态：CLOSED 变为 SYN_SENT
  ```

- 第二次握手

  ```
  服务端发出TCP段，SYN=1，seq=另一个初始化随机值y，ACK=1，ack=x+1
  服务端状态：LISTEN 变为 SYN_RECV
  ```

- 第三次握手

  ```
  客户端发出TCP段，seq=x+1，ACK=1，ack=y+1
  客户端：SYN_SENT 变为 ESTABLISHED
  (收到后)服务端：SYN_RECV 变为 ESTABLISHED
  ```

#### 三次握手的意义

通俗的说，握手的意义在于 **双方都可以确认自己&对方的发送/接收功能正常**。

第一次握手成功，S可以确认：C发送功能正常、S接收功能正常。
第二次握手成功，C可以确认：S发送/接收功能正常，C发送/接收功能正常（因为第一次发送的包确实被S收到了，所以C发送正常）

此时C已经完全确认，可S还无法确认C的接收、S的发送功能是否正常。因此需要第三次握手。
第三次握手成功，S确认了C接收功能，S发送功能正常。

若不进行第三次握手，可能会出现以下情况：

```
C发送了第一个请求连接包a一段时间后无响应，于是发送第二个包b重连。b到达了S，于是CS间连接，并且正常通信完成。
结束通信后，a包终于到达了S，S进行回应。若只有两次握手，那么这个连接就建立了，而显然，C不会向S传送数据。
这么一来，S上就多了一个永远不会有数据，但是却一直开着的连接。
```

#### 三次握手如果异常怎么办

这里，异常指发出请求后一直没有回应的情况。

- 第一次握手异常
  即发出SYN包后服务端一直没回应。此时客户端会每隔数秒后重复尝试发送，在尝试过一定次数（Linux默认5次）后放弃连接并报超时错误。值得注意的是这里每次间隔的秒数还不一样，第一次是1秒，之后是呈指数级上升的。

- 第二次握手异常

  这也是SYN攻击中常见的套路了。和第一次握手异常类似，第二次握手异常即服务端的SYN+ACK包无响应时，服务端也是同样的尝试5次重传，时间间隔也是指数上升。5次后若还是失败，则放弃该连接，将其移出半连接队列。

- 第三次握手异常

  第三次握手发出后，服务端处于SYN_RECV而客户端已经处于ESTABLISHED。服务端这边，正如前面提到的那样，服务端不断尝试重传SYN+ACK包，但5次没反应后就舍弃了连接。因此大概一分钟后，服务端的SYN_RECV连接会被删除。

  另一方面，客户端如果在发出ACK包后无其他动作，根据TCP的保活机制，需要在相当长（2小时左右）一段时间后，TCP发现本连接已经死亡，从而断开连接回收资源。
  此外，正常来说客户端ACK发出后，会直接开始发数据了。然而发出的数据不会被已经关闭连接的服务端收到，因此就变成了普通的超时重传机制。通常Linux配置的超时重传是15次，即尝试15次后如果仍失败，则断开连接。

#### 三次握手其他

- 半/全连接队列：S端处于SYN_RECV的状态时，相关连接信息被放入一个称为“半连接队列”的队列中。而那些已经完成三次握手的连接放在“全连接队列”中。全连接队列满时，若C发起新的连接请求可能导致丢包。
- SYN-ACK包重传问题：第二次握手时，S发给C的包称为SYN-ACK包。这个包发出后若一定时间内没收到C的确认包，服务器会进行重传。重传次数超过系统设置上限时，将停止继续尝试并且放弃这个连接。
- 携带数据问题：三次握手中，第三次握手可以携带数据，因为C接收到第二次握手的包时就意味着连接已经建立。前两次握手则不允许携带数据，因为如果允许，那么就意味着接收方必然需要处理这些数据（至少得安排内存来保存），若我是攻击者，就可以发送大量握手包，来攻击对方的内存。

### 四次挥手

![](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/70-20210708093118050.png)

C或S端都可以主动发起关闭连接，从而进入四次挥手流程。以C主动结束连接为例：

- 第一次挥手

  ```
  客户端发出TCP段，FIN=1，seq=u
  客户端：ESTABLISHED 变为 FIN_WAIT_1
  ```

  服务端收到第一次挥手包后，通知应用程序进行连接关闭操作。异步地，发送第二次挥手的包。

- 第二次挥手

  ```
  服务端发出TCP段，seq=v，ACK=1，ack=u+1
  服务端：ESTABLISHED 变为 CLOSE_WAIT
  （收到后）客户端：FIN_WAIT_1 变为 FIN_WAIT_2
  ```

  进入FIN_WAIT_2状态后，C就没有了数据发送能力，但仍然保持数据接收能力，可以接收来自S最后发送的一些数据。

- 第三次挥手

  应用程序将连接关闭后，发生第三次挥手。

  ```
  服务端发出TCP段，FIN=1，seq=w，ACK=1，ack=u+1
  服务端：CLOSE_WAIT 变为 LAST_ACK
  ```

- 第四次挥手

  ```
  客户端发出TCP段，seq=u+1，ACK=1，ack=w+1
  客户端：FIN_WAIT_2 变为 TIME_WAIT，持续2MSL，TIME_WAIT 变为 CLOSED
  （收到后）服务端：LAST_ACK 变为 CLOSED
  ```

#### 四次挥手的意义

- 为什么不是三次是四次？

  四次挥手的包分别是FIN, ACK, FIN-ACK, ACK。和握手比较后发现多了第二个ACK包。
  这个现象的根本原因是，关闭连接需要进行进程处理一些后续工作，比如将缓冲区中剩余未发出的报文发完。
  因为这些工作需要一些时间，所以==暂且先给出一个ACK包回应，旨在说明“你的FIN包我收到了，现在正在尝试关闭连接”==。而真的关闭连接后，才能发送FIN-ACK，说“你的FIN包我收到了，我也可以关闭连接”。

  相对的，握手的时候服务端没有什么额外工作要做，所以在收到客户端的SYN包后直接可以回复SYN-ACK说明“你的SYN包我收到了，我也同意建立连接”。

- 2MSL是什么，有什么意义？

  > MSL全称Maximum Segment Lifetime，即报文最大生存时间
  >
  > 默认C和S间传递一个报文最久只能是这个时间上限。超过这个上限则认为丢包。

  设置2MSL等待时间的原因是防止下面这样的情况出现：当C发出第四次挥手包时，S处于LAST_ACK状态。若此包中途丢失了，作为S来说，因为没收到最后一个ACK包，所以需要尝试重发第三个挥手包。若没有2MSL等待机制，C此时早已CLOSED，所以不能回应。因此S将永远无法从LAST_ACK变为CLOSED。

  相反，有了2MSL之后，在C发出第四次挥手包后，如果真的丢包了，S会在1MSL后察觉并重发第三次挥手包。此包又会最长花1MSL时间到达C。即，若真的丢包了，只要给C端2MSL的额外时间，C就能知道丢包事实，并且重发第四次挥手包。当然，若重发了，则需要重置2MSL计时器。

  总体而言，2MSL机制是为了给C一段额外的时间，给予其重发第四次挥手包的机会，防止因为丢包导致S端无法CLOSED。

### TCP连接中HTTP的限制

- 一个TCP连接可以对应多个HTTP请求（只要HTTP开了keep-alive，那么TCP连接就不会立刻中断，因此可以收发多个HTTP请求）
- 对同一个Host，客户端理论上可以建立非常多的连接。但是考虑到实际的开销问题，实际实现中通常还是有限制的。比如在Chrome的实现中，规定了对一个Host最多只能建立6个TCP连接。
- 到HTTP/1.1为止，多个HTTP请求在一个TCP连接中只能按顺序逐个发送处理，不能混着发。

## TCP拥塞控制四大算法

> 资料：https://www.bilibili.com/video/BV1c4411d7jb?p=61

==需要指出的是，这节提到的拥塞控制四大算法并不是四个独立可以做到拥塞控制的算法，而是相辅相成，共同组成了TCP拥塞控制体系的四个算法。==

### 什么是拥塞

拥塞，就是指网络过于拥挤导致堵塞。一条网络链路中的资源，包括带宽，交换机等是有限的，但可以请求使用这些资源的传输数据是无限的，于是过多的传输数据和过少的资源可能会引起网络响应时间过长。响应过长又可能进一步引起重传等操作，导致网络更加拥挤，形成恶性循环。更有甚者，还可能引起“竞争消耗性资源的死锁”。

简单来说，就把网络链路想象成普通道路，传输数据就是一辆辆车。如果车过多就会堵车，而一堵车就会有车加塞干嘛的，导致路况进一步混乱，最终导致谁也无法通行。

### 一些基本概念

假定有C和S，C向S发送TCP段数据。具体的发送方式是==“逐轮发送”==。可以简单这样理解：C发送数据时，一轮发送若干个段，并期待收到S的对于这若干个段的若干个确认报文。这个一来一回的时间又被称为RTT (Round-Trip Time)。

于是问题来了，若干个到底是几个？这其实是由C端的一个变量==“拥塞窗口”（cogestion window，简称cwnd）==控制的。顾名思义，C端可以根据当前网络的拥塞情况来动态调整这个窗口，这也是TCP进行拥塞控制的核心所在。
显然，cwnd越大，一个RTT内能传的数据越多，传输效率越高。但是如果网络很拥挤而窗口又过大的话，那么可能会引起拥塞问题。

此外，TCP数据传输中通常有重传机制。即C发送一个段给S后，在重传超时时间后（RTO，Re-Transmission Timeout）仍未收到确认，则自动重传该数据段。称为==“自动重传机制”==。

### 慢开始 与 拥塞避免算法

下面正式开始讲拥塞控制算法。先讲前两个。

慢开始算法指是，最开始时不能对网络情况估计过于乐观，因此将cwnd初始化为1。同时我们还需要设置一个==慢开始阈值（slow start threshold, ssthresh）==比如16。随后开始传输。
我们将一个RTT内传输了`cwnd`个数据段，并且全部收到了正确的确认报文的情况，称为一个正常的传输轮次。
在慢开始算法阶段，每完成一个正常的传输轮次，`cwnd *= 2`，呈指数速度增大。

当`cwnd >= ssthresh`时，停止执行慢开始算法，转而执行拥塞避免算法。
拥塞避免算法阶段，每完成一个正常的传输轮次，`cwnd += 1`，呈线性速度增大。

上述两个阶段，cwnd都在不断增大。但显然，其不能无限增长下去，到一个比较大的值比如24的时候，C可能会发现轮次不正常，即丢包发生，无法收到完整的确认信息了。
此时，两个算法的策略是：<font color="red">将cwnd重置为1，并将当前cwnd值的一半设置为ssthresh。随后重新开始执行慢开始</font>。

慢开始，强调从1开始指数增加负荷，快速逼近网络的承载极限。拥塞避免强调线性增加负荷，谨慎逼近网络的承载极限。

慢开始&拥塞避免两个算法结合，形成了一个简单的拥塞控制体系。这也确实是TCP早期的（1988年左右）实现方案，称为TCP Tahoe。

### 快重传 与 快恢复

在更新的TCP实现中，在上面两种算法的基础上还引入了快重传和快恢复两种机制。

上面提到，当丢包发生，我们就认为轮次不正常，从而进行拥塞控制。但是在实际网络中，很多时候因为网络抖动会突发丢包现象。这种丢包是偶然的，并不意味网络拥挤。这么一来，直接将ssthresh腰斩，还将cwnd变成1重新开始“慢开始”，会损失很大一部分通信效率。

作为解决办法，在较为新的TCP实现中，引入了快重传机制。新的实现称为TCP Reno。

#### 快重传

上面的描述中，我们认为当C很久得不到发出报文的确认，就默认进行超时重传。而所谓的快重传是指不等超时，而是结合TCP的序列号进行尽快的重传。

假设C向S发送包，0，1，2号发送正常，都收到了S的确认。3号丢包，暂时无S确认。随后发送4号包成功。==按原有套路，S应该给出4号的确认信息。但是按照序号序列来说，S认为你应该给我3号但是却给了我4号，这是不正常的，于是不返回4号确认，而是继续返回一次2号的重复确认。==
同理，C继续发5号包，S仍然返回2号重复确认确认；C继续发6号包，S仍然返回2号的重复确认。此时C这端，==连续收到了三个2号重复确认==。这就是一个快重传的信号，C端此时直接将`2+1=3`号包进行重传。若这次成功，S端接收到了3号包，于是他会返回6号确认，提示C端，按照序列号，一直到6号包位置都成功接收了。以上过程是很快的，通常到这个时间点，仍然还未超出RTO时间。所以这种做法被称为“快重传”。

以上就是快重传机制。一言蔽之，<font color="red">当连续收到三个重复确认时触发快重传</font>。画个图大概长这样：

![image-20210708212926382](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210708212926382.png)

#### 快恢复

发生快重传后，执行快恢复算法策略。具体的，将ssthresh和cwnd两个都设置为当前cwnd的一半，然后开始执行拥塞避免算法，即线性增大cwnd。
有的快恢复实现也会将`cwnd += 3`之后再开始拥塞避免算法。因为发生快重传后，至少有三个包（发生重传的包以及其后两个正常到达S的包）已经离开网络，可以适当加大窗口。

总体来说，快重传+快恢复是对原来实现中发生重传就直接一下子打回解放前的做法的一种修正。兼具了效率和拥塞控制的做法。

综合了慢开始、拥塞避免、快重传、快恢复四种机制的发送轮次与窗口大小的示意图大致如下（图中包括了Tahoe和Reno两种机制）：

![image-20210708232934962](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210708232934962.png)

#### 为什么是三次重复确认后快重传？

应该意识到，快重传的触发条件应该是明确判断到了“丢包发生”，可是重复确认机制本质上是进行的“包到达顺序乱序”。
实际上，因为网络抖动问题很多时候会引起包到达的顺序异常，因此要确定一个合适的值。

实践证明，两次重复确认（即`n+1`和`n+2`号包比`n`提前到）的情况由网络抖动引起的概率还较大，而三次基本可以确认是真的丢包而不是网络抖动了。因此确立了“三次重复确认后快重传”的规则。

## TCP流量控制

### 原理与注意点

TCP通信中如果发送方发送速率过快而接收方接收处理数据过慢，就会导致接收方会大量堆积还没确认的包。这些包不多的话可以暂时放缓存里稍后确认，但缓存如果也满了，那么就只能丢包。造成了网络资源的浪费。

==流量控制，就是通过控制发送方的发送速率，来使收发双方处于动态平衡==。

具体的，接收方每接收一个包并且发送确认信息给发送方时，带上自己的缓冲区还有多少空间这一信息。这个信息也称为“接收窗口”。这部分信息存放在上面TCP头数据构成中提到过的“窗口”段。
发送方接到确认，就知道了接收方目前的接收窗口大小，从而调整自己的发射窗口大小，尽量去匹配接收方的接收能力。显然，当接收窗口变成0时，发送方最好不再发送，防止大量丢包。

注意以下几点：

- 以上两种窗口，其单位都和之前提到过的拥塞窗口一样，是MSS，或者简单理解成多少个TCP报文段。

- TCP是双工通信协议，发送/接收双方随时互换角色，因此双方都要有一个接收窗口值，一个发送窗口值，从而保证双方的发送效率都是合理的。
- 发送窗口值并不是和对方接收窗口值设置同样大小，通常考虑到对方告知当前缓冲区剩余大小会会立即开始处理缓冲区，腾出更多的空间来，发送窗口会设置得比对方接收窗口更大一些。

### 与拥塞控制的区别与联系

流量控制和拥塞控制是两回事。
拥塞控制是指对传输的整个链路进行评估，从而动态调整发送的数据量，保证传输效率。
而==流量控制只着眼于发送方和接收方两台主机，是点对点的控制==。
但是显然，两者都会参与到通信控制中，所以一般==对于一个主机而言，其真实发送窗口大小，应该是`min(流量控制中发送窗口大小, 拥塞窗口大小)`==。

## TCP粘包问题

> 参考： https://www.zhihu.com/question/20210025/answer/1744906223
> 业界对于“粘包”这个事情也有一些不同的看法。

TCP粘包，并不是一个世界通用的计数名词，而是国内业界的土话。根据上面那篇知乎回答，所谓的“粘包”，其实可以解释成两种现象。
下面先过一过一些基础概念，然后再来解释一下这件事。

### 面向字节流的TCP、封包与拆包

常说，TCP是面向字节流的，而UDP是面向报文段的。这就有些离谱，上面的描述中TCP发的不也是报文段吗？
其实是这样的：说TCP面向字节流的意思，其实是指TCP发送的实际数据，即TCP报文段的数据部分，是一个字节流。
换言之，只要发送缓冲区中有数据，TCP并不关心你这些数据包括几条消息，有多长之类的，反正就是看做一个连续的流，切1个单位的流出来，套上报文段头，就发出去了。==这个过程也叫做封包==。相应的，接收方的传输层对报文段去掉报头的过程就是拆包。

### 粘包的两种含义

第一种：
程序在应用层通过调用send，将一些数据放入TCP发送缓冲区中。然而TCP发送时不关心数据的边界，有可能截取上个数据的一部分与这个数据的一部分，组成一个包发送出去。此时接收方调用receive后，发现数据是混着的，不符合其对有明确消息边界的期望，因此称之为一个问题，叫粘包问题。但其实这个更应该叫“粘数据”问题吧。
但是实际上，既然你期望明确的消息边界，则可以尝试用UDP而非TCP，因为UDP是面向报文而不是流，send一次那就是send一次，不存在缓冲什么的。

这种含义的粘包问题要解决也十分简单，虽然不同消息在TCP层没有明确边界，但是可以在应用层对其进行分割。

第二种：
TCP一般默认开启nagle算法。这个算法在TCP包发出前进行检查，如果TCP包很小，那直接发出不如等后续几个小包都来了一起发出，从而减少网络中传输中的包的数量。可这导致有些包不能预期时间内到达接收端。所以有人称这个也是一个问题，叫做粘包问题。
这个问题其实也不是问题，据说在99%的情况下，nagle算法并不能导致有体感的延迟。要不然人家TCP实现也不能默认开启nagle。

解决办法也很简单，将nagle算法关闭即可。



## TCP可靠性

常说TCP是可靠的传输协议，UDP则是不可靠的。那么TCP到底可靠在哪？可以从下面几个维度来解释

- 确认机制：每个报文段都需要得到确认
- 重传机制：当超过RTO时间或者满足某些条件触发快重传之类的情况，TCP会自动重传，保证数据完整
- 数据校验：TCP头中如上所述，带有校验部分，供接收方检查报文是否损坏
- 流量控制：从点对点控制的角度，防止丢包率过高
- 拥塞控制：从全链路控制的角度，防止丢包率过高
- 合理的分块：我们知道TCP面向流，但是实际传输中将数据分割成以MSS为单位的段进行传输。结合重传等机制，加入某一个段传丢了，问题也不大只需要将那个块重传即可获得完整信息。另一方面，若是UDP，一次传输就要传一整个报文，一旦传丢整个都丢，整个都要重新传（虽然UDP协议都不要求要重传）

# UDP基本

说到TCP就不得不说UDP。一句话解释UDP，就是这是一个提供无连接，尽最大努力的数据传输服务，不保证数据传输的可靠性。
这一章主要着眼于TCP和UDP的不同进行UDP性质的讲述。

## UDP的特点及其与TCP的区别

| UDP                                              | TCP                                         |
| ------------------------------------------------ | ------------------------------------------- |
| 无连接                                           | 面向连接，通信前必须先连接                  |
| 不可靠，尽最大努力                               | 可靠传输，可靠性见前述                      |
| 面向报文：应用层给多长的报文，加个头之后直接发送 | 面向流：应用层给的报文按MTU分割、封包，发送 |
| 支持一对多、多对多通信                           | 只支持一对一通信                            |
| 无拥塞控制，适合实时应用如zoom等                 | 有拥塞控制                                  |

## 常见的基于UDP和TCP应用层协议

- 基于UDP的应用层协议有

  ```
  DNS（指客户端向DNS发送DNS查询请求时用UDP）
  SNMP：简单网络管理协议
  TFTP：简单文件传输协议
  ```

  可以看到，很多都有简单作为定语。说明UDP不可靠，但是对可靠性要求不高时可以使用，因为实现方便，效率高。

- 基于TCP的应用层协议

  ```
  HTTP
  FTP
  Telnet
  SMTP
  POP3
  ```

# ⭐️IP基本

## IP包头

IP的数据包有包头，其中有大量网络层控制信息。这个包头大概长这样：

![image-20210710113055091](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710113055091.png)

其中比较重要的有源IP，目标IP，协议号（比如1是ICMP，6是TCP等等）。TTL是一个计数器，包每经过一个路由器路由后都会减去1，直到归零时返回ICMP超时报文。这部分解释下面有。

## IP地址与其分类

众所周知，IPv4地址是一个32位二进制数。为了表示方便分成了四组0-255的数。理论上整个IPv4地址池可以支持2^32^约43亿个IP。

当然，整一个大网把这43亿IP都整合进去不显示，现实中需要的是更多个规模更小的网络。于是，IP地址被分为了网络号部分和主机号部分，两者各占32位中的一些。这么一来，网络的变种就很多很多了。早期科学家觉得IP资源还很丰富，为了方便管理，认为规定了以下标准来分类一些IP地址。

| 地址分类 | 描述                             | 范围                        | 网内最大主机数 |
| -------- | -------------------------------- | --------------------------- | -------------- |
| A类      | 分类号0，7位网络号，24位主机号   | 0.0.0.0 - 127.255.255.255   | 16777214       |
| B类      | 分类号10，14位网络号，16位主机号 | 128.0.0.0 - 191.255.255.255 | 65535          |
| C类      | 分类号110，21位网络号，8位主机号 | 192.0.0.0 - 223.255.255.255 | 256            |

整个地址域中还有部分未被分到这三类地址中的地址，是有他用或者目前未被启用的备用段。

后来人们发现，按照最初的标准进行IP地址分类并不是特别合理。尤其是因为网络/主机位数被定死了，导致网络的大小选择余地太小。所以就又找出了掩码这种机制，通过他来灵活地分配网络位和主机位。掩码也可以用`/xx`的形式表示，表示32位中前`xx`位是网络号，剩余是主机号。需要注意，使用掩码进行灵活网络划分只是对这43亿个地址进行的重新编排，使得每个地方都能尽量用上合适大小的网络，并不会增加地址的数量。

### 子网划分

笔试题中经常会碰到这类问题。子网划分，本质上，是对已有网络表示中，进一步从高位选择几个主机号作为子网号。而剩余的主机号仍作为主机号。显然，这个过程结束后，每个子网的掩码比原网络的掩码要更大一些。

以划分一个C类网络`192.168.1.0/24`为例。若我想将其划分成n个子网，我就必须要在现有的8个主机位中匀出`k`个作为子网号，并且满足`2^k >= n`。剩余的`8-k`个主机号，则构成每个子网最多的主机数是`2 ^ (8-k)`。
举个具体例子，比如我们想要分20个子网出来。显然，`k=5`，即我们需要额外的5个二进制位表示子网。于是此时子网掩码是`11111111.11111111.11111111.11111000`，即`255.255.255.248`。
而每个子网最大主机数是`2^3 = 8`。（严格来说，每个子网还应该去掉一个网络地址和一个广播地址，所以应该再减去2）

### 公/私有地址

为了进一步提升IP地址分配与网络构建的灵活性，在A/B/C三类地址中，还额外划分了一些私有地址，范围如下：

| 地址分类 | 私有地址范围                  |
| -------- | ----------------------------- |
| A类      | 10.0.0.0 - 10.255.255.255     |
| B类      | 172.16.0.0 - 172.31.255.255   |
| C类      | 192.168.0.0 - 192.168.255.255 |

私有地址是指，这个范围内的IP地址的分配和管理权在网络构建者手上。因此，不同人构建的不同网络中，这范围内的IP是有可能重复出现的。
相对的，不在这范围内的都叫做公有地址。公有地址在整个互联网唯一，由互联网地址分配机构统一管理。

## IP地址与路由

==IP地址中的网络号部分，主要的一个应用就是拿来进行路由==。

路由要用到路由器。路由器可以视作是拥有两块以上网卡的特殊主机。其两块网卡分别配置于不同的网络中。比如`192.168.1.0/24`和`192.168.2.0/24`间可能有一个路由器。其两块网卡的IP分别是`192.168.1.1/24`和`192.168.2.1/24`。这样他就作为一个桥梁，将两个网络粘合起来。

![image-20210710103720276](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710103720276.png)

所有主机和路由器在网络层都维护了一个路由表，路由表是`网络：下一跳`的对应。当一个IP包来到这里时，首先分析其目的IP地址的网络号部分。然后在路由表中寻找其下一跳。所谓下一跳，指当前机器所在的网络中的另一个主机地址，通常这也是一个路由器。

Linux系统中，可以通过`route -n`命令查看当前系统中维护的路由表。路由表的输出示例如下：![image-20210710114256310](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710114256310.png)
Destination和Genmask字段形成一个网络。对于一目标地址，我们逐条分析匹配，看目标地址是否是该行对应的网络，如果是，则将数据通过Iface字段指定的网卡，发送给Gateway指定的网关即路由器。
Gateway是`0.0.0.0`表示已经不需要网关，即目标主机与本机处于同一个局域网中，直接基于MAC地址发送即可。

最后一行网络是`0.0.0.0/0`，指代默认网关，即无法匹配路由表之前所有规则的包，将通过默认网关发出。后面的路由交给默认网关做。

## IP与MAC（ARP）

> 在说具体的事情之前，先来理清这么一个事：所谓的点对点通信，本质都是物理信号的广播。只不过信息中带有接收方的一个ID。所有主机都需要解析物理信号之后看这个ID是否与自己的相同，若相同则接收数据，从而实现点对点通信。
>
> 在计算机网络内部通信中，IP虽然也能扮演这个ID的角色，但是IP在网络层，每次主机解析都要从物理层跨过链路层到达网络层，开销很大。因此，计算机选择在链路层解决这个问题。链路层每个网卡都有mac地址，以这个mac地址作为ID。因为其最贴近物理层，解析很快，所以可以高效实现点对点通信。
>
> 用MAC来实现点对点通信的底层不仅仅因为其更靠近物理层，其包头结构也更简单，一个MAC的包头长这样：
>
> ![image-20210710115119441](/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710115119441.png)
>
> 协议类型基本也只有两种选择，IP协议或者ARP协议。设置为IP协议时表明这个MAC帧是一个普通的传输帧，设置为ARP协议时表示这个MAC帧是某主机发出的ARP请求/应答帧。

上述利用IP地址进行路由的过程，关键是在每一步都能找到目标主机所在的网络以及去往那个网络的下一跳。
可问题是，到达目标网络的入口的那个路由器后，数据该怎么进一步传输给目标主机呢？由于是最后一步的网络内点对点，所以可以利用MAC地址。

我们泛泛地知道，==ARP协议是将IP转换为MAC地址的协议==。更具体的，发送数据的主机会对当前网络内进行一个广播，称为==ARP请求==，要求查找目的IP地址的MAC地址是多少。此时匹配的那个主机会进行==ARP响应，将自己的MAC地址放进响应包返回==。
获得回应后的发送方，还会将IP-MAC对应关系缓存起来，下次直接用。在Linux中可以使用`arp -a`命令查看当前主机的所有ARP缓存。

基于ARP的传输是网络内的传输，本质上是广播寻址然后点对点传输。
实际上，在网际的路由器间传输时，两两路由器之间身处同一个网络，在这个网络内部，传输也还是基于ARP的。

#### RARP

顺便提一下RARP。第一个R是reverse的意思，顾名思义，这个协议和ARP相反，是拿着MAC找IP的协议。这个协议的应用场景大概是这样的：在一些物理地址固定的设备比如打印机等接入网络时，其可以向网路中已经存在的某RARP服务器发起RARP请求，其中带有该设备的物理MAC地址。
随后RARP服务器将会为其分配一个IP地址，并将MAC和IP的对应关系记录下来，并且返回告知该设备其被分配的IP是多少。

## IP与NAT

虽说IPv4有40多亿个地址，但是总归还是不够用的。于是就有了NAT技术。

一言以蔽之，NAT服务器可以将其公网IP借用给其管理的局域网中的主机，以访问外界网络。这么一来，局域网里的很多主机只需要一个IP就可以参与到互联网中了。

更具体的，借用IP这个行为并不仅仅是网络层，应该升华到传输层。（否则两个局域网内主机同时想访问外网怎么办）
换言之，局域网内主机借用到的，其实是NAT服务器的一个TCP端口，以此为跳板和外界进行信息交互。

和路由表、ARP中的IP-MAC对应表等缓存一样，NAT服务器会将`局域网内主机:端口`和`NAT端口`的对应关系缓存起来，以便下次进行NAT时可以迅速找到并建立NAT。

# ICMP协议

ICMP全称Internet Control Message Protocol，即互联网控制报文协议。

## IP与ICMP

在网络层基于IP协议进行数据传输的时候，会发生各种各样的故障比如数据包传丢了之类的。此时发出方自然希望能知道是因为什么原因引起了故障而无法将数据包传递到目的主机。有了ICMP之后，故障节点可以向发出方返回一个ICMP数据包，说明到底发生了什么事。
以上是ICMP的一个功能。知道了故障原因之后自然还希望能够调试排查，这也是ICMP可以做的。
总之，ICMP就像是IP协议的一个助手和工具包，可以用来诊断、排查IP传输问题。

ICMP的报文其实是和IP头一起组成一个IP包传输的。换言之，ICMP的报文代替了一个TCP段。ICMP报文中还带有ICMP头，其中包含了报文类型等信息。

## ICMP报文类型

上面说了，ICMP的两大功能是 1.向发送方报告传输故障  2.进行网络调试。这两个功能分别对应了两种ICMP报文类型，称为 “差错报文类型” 和 “查询报文类型”。

两大类型下又可以细分成各个具体的报文种类，每个种类都用一个数字来表示（这个数字被写在ICMP报文头中）。对应如下：

| 数字ID | 报文种类   | 报文类型     |
| ------ | ---------- | ------------ |
| 0      | 回送应答   | 查询报文类型 |
| 3      | 目标不可达 | 差错报文类型 |
| 4      | 原点抑制   | 差错报文类型 |
| 5      | 重定向     | 差错报文类型 |
| 8      | 回送请求   | 查询报文类型 |
| 11     | 超时       | 差错报文类型 |

### 查询报文类型

查询报文也称回送消息，分为回送请求和回送应答两个过程。发送方A向接收方B发出`8.回送请求`，若B可达，则返回`0. 回送应答`。这个过程结束后，认为从A可以达到B，说明两者间网络层面是互通的。注意，这里的A和B既可以是主机也可以是路由器。

### 差错报文类型

挑两个重要的说说

- 3.目标不可达

  目标不可达又细分为网络不可达、主机不可达、协议不可达、端口不可达、需分片但不能分片。
  目标地址的网络号本身就在路由中找不到，称网络不可达，错误代码为0
  网络可达但无法在目标网络中找到主机，称主机不可达，错误代码为1
  网络层的通信没问题了，但是目标主机的传输层禁止访问比如禁止了TCP协议而你发了TCP包过去，就是协议不可达，代码2
  传输层没有禁止，但是对方主机没有监听相关端口，端口不可达，代码3
  数据包到达路由器，要进入下一个网络时，由于网络的MTU可能不匹配， 导致包过大。通常路由器会在此时进行包分割，逐个传入下一网络，但若包头的“不可分片”位为True，则路由器直接将包丢弃并返回ICMP不可达差错报文，代码4

- 11.超时

  我们知道IP中有TTL字段，包每经过一个路由器，TTL就减1。当减到0时，这个包被原地丢弃，并且该节点向包的发出方发出一个ICMP超时差错报文。

## ping命令执行

说到ICMP就不得不提到ping了。显然ping命令使用的是查询类型ICMP报文。下面假设从`192.168.1.1`发起命令ping`192.168.1.2`，来看看这整个过程。

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710111729576.png" alt="image-20210710111729576" style="zoom:50%;" />

首先，在192.168.1.1中

1. 进行ICMP报文组装。ICMP头部中，报文类型设置为8，并设置一个序号（设置序号主要是为了连续发多个ICMP包时可以进行区别），另外为了计算RTT报文头上还会加上发送时间字段。
2. ICMP报文组装完成后IP协议在这个报文外面加装IP头。IP头主要就是源地址、目标地址、以及协议类型（具体是1，表示ICMP协议）。
3. 接下来，数据被送到链路层，再套上MAC协议的头。这个头主要是源MAC，目标MAC。目标MAC率先尝试在本级缓存中找，找不到的话则通过ARP协议发起查询。

接着这个包就被发送出去，经过交换机进行局域网内广播。接下来，从192.168.1.2的视角，接收到了这个包。解析到网络层后，发现这是一个ICMP包，于是构建一个ICMP回送响应消息报文。类型为0，序号等于请求的序号。再一层层套回IP头，MAC头，然后发送。

以上是局域网内的ping的工作流程。如果涉及跨网段的ping，则还需要经过网际路由器进行路由转发。这部分操作就是之前IP与路由中提到过的通过多个路由器进行跳跃的转发过程，就不赘述了。

## traceroute命令执行

traceroute命令是另一个利用了ICMP中差错类型报文的命令。其有多种用法

- 用法一：追踪到达某目标主机的沿途路由器

  其工作原理是这样的，开始时，将发出一个包裹了ICMP报文的IP数据包，并设置其TTL为1。我们知道IP包每经过一个路由器TTL会减1，而TTL归零时当前路由器会返回ICMP超时差错报文。所以，发送者只要解析这个超时报文的发起源，就能找到链路中的第一个路由器。

  同理，第二次，将TTL设置为2，就可以找到第二个路由器，以此类推直到到达目标主机。

  当然，有些公网的路由器被配置为对那些TTL归零的包不回应ICMP超时报文，那么我们也就无从知晓这些中间节点了。

- 用法二：确定MTU

  我们知道IP层一个数据包最大的size称为MTU，通常以太网中这个值是1500字节。但是路由途中，有些中途的网络其MTU可能不为1500。为了得知达到某个主机需要的合理的MTU是多少，可以采取如下策略：

  包一个大IP包并设置报头“不可分片”位为1，发送。如果图中某些路由器的网络，其MTU比较小，则会返回ICMP不可达差错报文，错误类型是需分片但未分片。这个响应中带有那个网络的MTU。因此下一次就包一个适应那个网络的MTU大小的值继续发包。以此类推直到目标主机，就知道了到达目标主机的MTU大小是多少了。

# ⭐️一次完整的HTTP请求

上面，从HTTP说到TCP，再说到IP，MAC，现在我们祭出常见面试题：“浏览器中打入URL到页面显示，发生了什么？”，顺着这几个协议一层层的思路，将这个过程细致说一遍。

## URL解析/HTTP报文组装

浏览器对URL首先要进行解析。注意这里的解析指单纯文本层面上的解析，而不是指域名解析。
URL解析主要关注这几个部分：协议、域名、文件路径、请求参数。
对URL解析完成后，将其中的信息组装成HTTP的报文头和消息体。因为输入URL的话必然是使用GET方法，所以这里消息体是空的，如果是其他比如POST方法，那么这个消息体中应该是需要上传的数据：

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710143300410.png" alt="image-20210710143300410" style="zoom:50%;" />

## DNS解析

有了HTTP报文后还知道要往什么地方发送。此时就要进行DNS解析了。具体做法在DNS那一章说了，简单那来说就是能用缓存就用缓存，缓存里没有的去找DNS服务器。DNS服务器的请求方式还有迭代和递归两种，其中迭代比较常见。

将HTTP报文中的域名部分提取出来，拿去做DNS解析，得到一个目标IP地址。

## TCP包组装

至此，所有行为都还是应用层行为，CPU运行状态也都是用户态。接下来，因为HTTP一般基于TCP，所以要对HTTP报文进行TCP封装，工作也进入了传输层。
这个过程由用户态的程序调用`socket`库中相关函数开始。socket库中的函数一开始运行，就进入了系统态。即，传输层开始往下的工作都是在系统态中进行的。

TCP封装，是指给从上层传递来的HTTP报文加上一个TCP头。这个头的结构在TCP章里讲过了。
值得注意的是，如果HTTP报文过大，那么TCP在这里其实是会对报文做一个切割的。之前提到过，TCP段刨去头部，剩下的最大长度是MSS，就以这个作为标准切割：

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710144104107.png" alt="image-20210710144104107" style="zoom:50%;" />

在加上TCP头部之后，TCP段的机构是这样的：

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710144158008.png" alt="image-20210710144158008" style="zoom:50%;" />

## IP包组装、路由表

IP包组装就是给从传输层传来的TCP段再套上一层IP头部信息。IP头的结构也在IP那章讲过了。这里需要注意，IP头部中有一个协议字段，通过一个数字表示上面传输层用的是什么协议。由于现在传输层用的是TCP，所以当前加的IP头，协议字段也应该是06，代表TCP。

网络层负责将数据传输到非本地局域网其他网络。根据IP头中的目标IP地址，结合本地的路由表，可以得出下一跳需要发送的主机是什么。通常是局域网的网关，如果目标IP已经在本局域网内，则网关在路由表中显示0.0.0.0，此时说明可以直达目标主机了。
Linux的路由表可以通过`route -n`查看。

加上IP头后，数据变成了IP数据包，长这样：

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710144612970.png" alt="image-20210710144612970" style="zoom:50%;" />

（下面HTTP报文部分因为太长，截图截不过来了）

##  MAC帧组装、ARP缓存

数据到链路层，给IP数据包再套一个MAC头。而且这个MAC头的信息非常简单直接，只有源MAC地址，目标MAC地址以及上层协议（这里因为是传输数据，所以是IP协议，ARP请求和回应的时候是ARP协议）。

源MAC地址写死在网卡里，很好获取。目标MAC地址是指上面网络层解析后得知的下一跳的MAC地址。因为下一跳肯定在局域网内，所以用ARP缓存查找其MAC地址。若缓存未命中则发出ARP请求广播，寻找MAC地址，更新缓存。Linux中的ARP缓存用`arp -a`查看。

组装上MAC头之后的帧长成这样：

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710145022307.png" alt="image-20210710145022307" style="zoom:50%;" />

## 物理层网卡送出

最后，得到的帧仍然是数字型号，由网卡驱动程序将其转换为电信号并通过网卡发送出去。
在这里，网卡会最后为帧前面加上一个分界符，后面加上一个帧校验序列（FCS）检查传输过程中帧是否损坏。
经过网卡处理后，被转化为电信号发送前的数据长这样：

<img src="/Users/wyzypa/Pictures/TyporaImages/逆袭进大厂计网篇笔记.asset/image-20210710151058438.png" alt="image-20210710151058438" style="zoom:50%;" />

## 交换机转发、交换机缓存

现在局域网通常都使用交换机作为基于MAC点对点通信的实现设备。交换机有多个网线接口，局域网内所有机器都接入在交换机上。

上面说到，网卡将电信号发出。而发向的目的地第一站就是交换机。交换机接收到信号之后先将其解析为数字信号，简单的检验一下FCS看传输时是否有损坏。接着，交换机会检查这个帧的目标MAC地址对应哪个网线接口，并通过那个接口进行发送，实现点对点通信。
那么交换机是怎么知道MAC地址和接口的对应关系的呢？还是缓存。

交换机内部维护了一个各个接入交换机网卡MAC地址和物理接口的对应关系的表。对于拿着MAC地址找接口的活儿，缓存命中直接用缓存，缓存没有则对除了源接口的所有接口进行广播，让对应MAC地址现身。当然别忘了更新缓存。

注意到，交换机本身需要有像主机的网卡那样，将数字信号和电信号互相转换的设备。但是另一方面，交换机并不需要MAC地址，他只是做了数据的转发。当然其内部还需要一定缓冲区，用来保存缓存等信息。

## 路由器

说了一大通，别忘了我们现在是在做一个HTTP请求。而HTTP服务器通常是在外网而不是在局域网内。所以上一步交换机转发数据，通常是将数据转发到了当前局域网的网关处，即路由器。

路由器接收电信号后解析成数字型号，分析MAC头之后又发现是发给自己的，所以接收这个包进行进一步解析。
此时，解析IP头，路由器可以根据IP头中指定的目标IP，以及其自身的路由表，进行下一跳的选择。

通常下一跳是当前路由器所在局域网的另一个网关，若网关是0.0.0.0则表示目标IP直接可达。不管哪种情况，路由器都应该知道了下一跳的IP（另个网关或者目标IP）。有了IP，通过ARP就可以知道其MAC，也就为这个数据包再次套上一个新MAC头，发送出去了。

在一次HTTP请求中，上面的不断寻找下一跳，转发到下一个网关的过程周而复始要重复好多次。最终，最后一个路由器将数据包传送到了目标主机上。
==在整个传输过程中，IP头的信息其实一直没有变化，每个路由器会解析IP头，查看其目标IP地址等信息，但是并不会修改。
相对的，MAC头每经过一个局域网就会被更改一次。因为其负责的是局域网内点对点通信==。

### 路由器与交换机

路由器又被称为3层交换机，两者的功能十分相似。不同的是，交换机工作在第二层，其网络接口没有MAC地址，所做的工作是纯粹的转发。
路由器的每个接口，其实和主机的网卡一样，既有MAC地址也有IP地址。因为路由器进行的中介工作比交换机复杂得多，其路由表也有可能随时动态变化，所以纯粹的转发无法满足需求。代替的方案，则是将路由器也视作网络内的一个主机，用其来做数据的接收和转发。

## 服务端与客户端的互动

数据包经历了千辛万苦，终于到达了服务端主机。服务端主机进行层层拆包，最终拆到了HTTP报文。
根据HTTP请求报文的要求，服务端将返回内容也组装成一个HTTP响应报文，不出所料的，这个报文也通过服务端的层层加套，最终变成一个电流信号。

网卡将信号发出，返回给客户端。客户端收到后，再次层层拆包，解析出HTTP响应报文。

最后，客户端的浏览器将报文中的内容渲染到页面上，形成一个网页。

至此，一次完整的HTTP请求完成。

# 网络攻击

## DDos攻击

DDos攻击指攻击者控制大量机器向我服务器发起TCP请求，我服务器回应，进行第二次握手，但请求方不再给第三次回应确认信息，导致服务器花费大量资源维持这写连接一段时间。

防范方法：
限制半连接队列的长度。

## SQL注入攻击

指在HTTP请求中，部分字段故意写成一些恶意SQL代码。因为后台要将这些内容编写成SQL从而去更新数据库，会一并执行这些恶意代码从而造成损失。

防范方法：
Web端进行有效性检验，限制输入长度等
服务端不要使用SQL拼接，对SQL中的特殊字符进行过滤等

## XSS攻击

XSS全称跨站脚本攻击。现在大多数网站都支持和用户之间的互动，即用户可以发布一些自己的内容，其他用户则可以浏览其相关网页看到其发布的内容。
XSS攻击就是指将恶意代码植入到用户可编辑内容中发布，从而让别的用户在浏览时浏览器被迫执行这些代码。
举个例子，假如我发布一条微博，内容为`<script> alert('surprise')</script>`。如果后台校验不严格，逻辑是直接将输入内容放到一个`<div></div>`中，并发布了出去。此时别的用户浏览我这条微博时，他们的浏览器会将这段`<div><script>xx</script></div>`解释为javascript执行，于是就达到了我的攻击目的。

防范方法：
对输入长度做限制，校验输入内容中的<,>等字符，将其转义。

## CSRF攻击

CSRF全称跨站点请求伪造。CSRF和Cookie相关，通常描述成攻击者盗用你的Cookie从而可以冒充你的身份做一些事。
而攻击者通常通过架设另一个危险站点，在危险站点中对被攻击网站发起访问，从而实现CSRF攻击。

举个例子，如果某银行网站转账方式以GET方法进行的，如GET`bank.com?toBankId=A&amount=1000`就是向A账号转账1000。
通常首次转账需要密码，而第二次以后为了方便，将密码以token的形式保存在Cookie中，这样就可以不输密码直接送token进行校验。
现在攻击者架设一个网站，其中有这样一段代码：`<img src="bank.com?toBankId=B&amount=1000" />`。
如果你在之前转账密码token没失效前打开了这个网站，浏览器自动执行去GET这个src，此时token有效所以校验通过，于是你就给攻击者的B账户转账了1000元。

防范方法：
token机制：在用户第一次看到页面的时候，给页面中埋藏一个隐藏token。要求用户提交数据时必须带上这个token才有效。因为CSRF攻击者可以跨站获取Cookie的使用权，但却无法跨站访问页面，因此无从知晓这个token
验证码：通过为请求设置一些只有人才能计算的验证码，来过滤掉由机器发起的CSRF攻击
referer识别：识别请求的referer，拒绝以别的不明站点为referer的请求。

# 其他

