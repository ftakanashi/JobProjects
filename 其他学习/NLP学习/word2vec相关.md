# word2vec
是一种工具，一种算法，而不是一个具体的模型。他试图通过神经网络的无监督学习，学习词向量。
==word2vec有两种模型架构，CBOW和skip-gram。
还有两种优化套路，分别是负采样和阶层softmax==。
下面来具体看一下。

## CBOW

CBOW就是词袋模型。在word2vec中，CBOW模型架构具体就是指通过给神经网络输入句子中某个单词的前`k`个单词和后`k`个单词，然后训练网络在输出端输出单词本身。

通常，网络中间安排一个长度比如256或者512的隐层，训练完成后，输入层参数矩阵 $W_{in}$ （size是`vocab * 512`之类的）就是我们要的word2vec了。

另外，因为输入是多个词，必然在经过embedding之后得到多个词向量，需要一个pooling层（比如mean pooling或者sum pooling之类的）将多个向量合成一个，再拿去传递给隐层。

## Skip-gram

Skip-gram和CBOW相反，输入的是一个目标词的词向量而输出的是其前`k`个和后`k`个词。
==注意，输出多个词并不意味着隐层-输出层的参数有多组，还是只有一组==。相当于我用一组参数尽量使前后`k`个词的softmax后的概率尽量大。因为我做翻译的，下意识觉得不选出个max概率的词来不行，但是并不是这样，因为这里的目的不是选出一个max概率词，而是得到embedding就行了。
从这个意义上来说，Skip-gram有点像是输出端是词袋的意思。

Skip-gram的输入侧因为只有一个单词经过embedding矩阵，得到词向量输入到隐层，因此不用pooling。相对的，其输入一次后可以得到`2k`个损失。

两种架构的示意图如下：

<img src="/Users/wyzypa/Pictures/TyporaImages/word2vec相关.asset/v2-1f525e2259edd9210fe470ec7fc218d4_r.jpg" alt="preview" style="zoom: 67%;" />

## CBOW和Skip-gram的比较

两者对比如下表：

|            | Skip-gram    | CBOW         |
| ---------- | ------------ | ------------ |
| 输入       | 一个中心词   | 多个上下文词 |
| 输出       | 多个上下文词 | 一个中心词   |
| 训练复杂度 | 高           | 低           |
| 训练时间   | 长           | 短           |
| 训练效果   | 好           | 稍差         |

是的，总体来说CBOW的效果比Skip-gram要差一些。
许多人是这样解释的：CBOW的一次正向传播只能获得一个损失，自然对embedding矩阵的更新也只有一次。而Skip-gram一次传播可以得到`2k`个损失，相应的embedding矩阵会被更新`2k`次，从这个意义看，Skip-gram训练次数更多。
还有一个形象的比喻，说Skip-gram是请了多个学科的家教一对一辅导一个学生。学生从不同老师那里学习到不同知识，但也更累；而CBOW则是大班化教学，一个老师带多个学生。

我个人认为上面解释也不太对，你怎么不说CBOW的时候更新了`2k`条embedding中的参数而Skip-gram只更新一条呢。
我觉得问题可能出在CBOW的pooling层上，一旦pooling，肯定会对原本的词的信息造成损失。

## 对word2vec的优化

> 参考这个系列的文章：https://www.cnblogs.com/pinard/p/7160330.html

考虑到vocab size很大的时候，不论是CBOW还是Skip-gram，输出层都会是一个诸如`512 * 100000`之类的矩阵，经过这个矩阵的计算后，其输出还要进行一次总score数量是`100000`的softmax操作。这两个计算，哪个看起来都不像是省油的灯。针对这两个计算花费较大的工作，分别提出了针对性的优化策略。

### 负采样

Negative Sampling是针对`512 * 100000`这个大矩阵的。因为输出端要计算100000个神经元的score，很消耗资源。一个很自然的思路就是能不能别计算那么多神经元呢？

我们知道，这里面100000个神经元，每个对应的其实都是vocab里面的一个单词。既然不计算全单词，那么计算哪些单词，这个选择就显得很重要了。首先，假设标签是 $w_i$ 这个单词，那么 $w_i$ 肯定得被纳入考虑范围吧，否则就都谈不上学习了。而对于其他词，一种比较合理的策略是，对vocab进行随机采样，对高频词以高概率采样。相当于是基于频率，对整个vocab做一些采样。

采样个数从5-50个甚至更多，可以自己定，但不论多少，可以明确的是这个采样的母集，是非正确答案的那些词，即负例。所以这个过程叫做负采样。

假设我们采样10个。此时，针对这个网络，我们就有了1个正例词和10个负例词。针对这11个词的每个词，都进行一次二元回归。即用`512 * 1`矩阵+Sigmoid将其计算成一个概率值，然后用Binary Cross Entropy计算损失并反向传播。

如此，训练过程中，在输出端我不用完整的计算100000个目标词的概率，只需要计算一个正例以及若干个负采样得到的负例词的概率即可。事实证明，负采样虽然大大缩小了损失计算的范围，但是效果却不差。

### 阶级化Softmax

Hierarchical Softmax的做法将整个隐层-输出层以及对输出层的softmax都给替换掉了。
具体的，这个优化根据vocab建立一个哈夫曼树（哈夫曼树是什么怎么建立就不说了…）。这个哈夫曼树的每个叶子节点自然是一个单词，==而每个非叶子节点，内含一个512维的参数向量和一个Sigmoid操作==。

和之前全连接层的`512 * n`的参数量相比，这里参数量只有`512 * logn`了。

进一步的，来看看如何进行计算。在输入层得到词向量后，将这个词向量跟根节点的参数向量进行相乘，再加以Sigmoid后，显然得到了一个概率值。这其实就是一个二元回归，或者是二分类。
接着，以这个概率值是否大于0.5为界，利用哈夫曼树的性质，大于0.5则视为预测结果是1，往右走；否则视为结果为0，往左走。
这样周而复始，最终可以通过`logn`的时间，到达一个叶子节点，即label词。
这种算法，将原先一口气对vocab中`n`个词全部计算softmax的做法，转换成树路径上一层一层的二分类，所以称为阶级化的softmax。
上面分析了参数量的空间复杂度，而显然，原来需要`O(n)`时间进行的Linear+softmax在这里也只需要`O(logn)`就行了。



以上，介绍了两种优化。两种优化其实都可以加在两种模型架构上。所以，具体的带优化的word2vec模型，总共有4种形式。