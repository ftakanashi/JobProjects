## RNN的梯度消失问题

一个简单的RNN图示如下：

<img src="https://camo.githubusercontent.com/10d2d0e08a9630c4d3ed22321236b65ee949756112f1cfe9516120e437ef55f4/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31322d32342d3036353733332e6a7067" alt="img" style="zoom:50%;" />

从这个图中，可以看到，任意时刻`t`中，RNN都使用了同一组参数（$W_x, W_s, W_o$）来进行正向传播的计算。
这里面就有一个问题，既然如我在图中的`t+1`时刻进行损失计算并反向传播，在这三个时间步上来说，梯度的计算如下所示：

<img src="https://camo.githubusercontent.com/0bc534b8c441715c96ed2e3eb9c4a5d496cb9ae6f810784aa6674aee66548e7a/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31322d32342d3036353733312e6a7067" alt="img" style="zoom:50%;" />

显然，对于每个时刻都在重复使用，并且还要参与反向传播的参数$W_s$与$W_x$而言，其梯度值是一个连加。而连加项越靠后（即越早的时间步时），连乘的项越多。

实际上，以$W_s$的式子为例，一个连乘项可以被表示为 $d_{out}W_sW_sW_s\cdots$ 因为连乘了$W_s$矩阵，就会导致矩阵中原先小于1的值越来越小，大于1的值越来越大。后者容易引起梯度爆炸问题，但是可以通过clipping的方法进行解决（当参数的某个指标如L2 Norm大于规定的threshold之后，所有参数乘以$\dfrac{threshold}{L2 Norm}$），而梯度消失就没那么好解决了。

这里还需要额外指出，RNN中的梯度消失不是绝对的梯度消失。在一般线性网络中，因为深度过大而梯度消失，指接近输入的参数梯度太小从而无法学到信息。彼时，接近输入的参数和接近输出的参数是不同具有不同梯度。
而在RNN中，因为各个时刻用同一组参数，所以参数在当前时间步，至少是可以学到有用的信息的。然而随着时间方向往回传播，梯度越来越小，所以RNN语境中的梯度消失，指的是时间维度上的梯度消失，换言之这也使得RNN无法很好学习远距离的数据关系。

## LSTM对RNN梯度消失的改善

LSTM通过引入门控机制，对RNN的梯度消失进行一定程度的改善。
这是为什么？泛泛来说，LSTM的门控在不同时刻可以取不同的值，从而对从前方传递回来的梯度进行当前时刻个性化的取舍。



## LSTM和GRU的区别

总体而言，GRU是对LSTM的一个简化，所以其参数更少，更容易收敛，当然表达能力上也稍弱一些。

具体来说，LSTM有三个门（forget，input，output），且除了隐状态之外，还额外插入了一层记忆细胞，以记忆细胞作为核心，对各个时间步的隐状态进行进一步的修正。

而GRU只有两个门（update，reset），并且只进行隐状态传递，单纯用这两个门直接对隐状态进行修正。