# Python中的基本数字表达
>参考：https://zhuanlan.zhihu.com/p/91967268?utm_source=wechat_session

## 二进制码
在数学中，我们可以将一个十进制数转化为一串0和1变成二进制数。而正负号是额外加的。
比如5的二进制表示是`101`。那么-5可以写作`-101`。

而在实际的计算机中，因为要做到在计算机中区分正负数，通常选择将固定位数表示整数时，最高位当做符号位。
自然的，符号位0表示正数，1表示负数。

这么一来，在计算机中表示的二进制数就有一个"机器数"和"真值"之间的对应关系。
比如5和-5在计算机中用8位表示分别可以写作`0000 0101`，`1000 0101`。
对于-5，其虽然表示成`1000 0101`，但是其真值是`-5`，因为考虑了符号。而不是机器数的`133`。

### 原码、反码、补码的定义以及转换规则
以上定义的，最高位作为符号位，其余位表绝对值的表示形式，称为"原码"。
比如1的原码是`0000 0001`。
比如-1的原码是`1000 0001`。
原码还是比较接近人类的表达习惯，看起来并不难懂。

反码是计算机中原码之外另一种二进制表示整数的方式。（不要问为什么需要这么一种拐弯抹角的表示方式，下面会说。
具体的，反码分类讨论。如果是正数，则保留其原码形式。
如果是负数，则是在原码的基础上保留符号位，反转其余所有位置得到的结果。
比如1的反码是`0000 0001`。
比如-1的反码是`1111 1110`。

补码是原码和反码外的第三种方式。
仍然分类讨论，如果是整数，保留原码形式。
如果是负数，则在原码的基础上保留符号位，反转其余所有位置，再+1。即在反码的基础上再加一。
比如1的补码是`0000 0001`。
比如-1的补码是`1111 1111`。

简单总结一下，
对于正数，三码合一。
对于负数，反码是原码的"真值部分取反"，补码是原码的"真值部分取反+1"。

#### 补码的小性质
在n位表示的前提下，若一个负数`a`，将其补码的符号位置零得到一个正数`b`之后，有如下关系：
`b = a + 2^(n-1)`。
比如8位的-5是`1111 1011`。通过置零符号位得到`0111 1011`即123。
-5 + 2^7 = 123 或者表示为 123 - 2^7 = -5。

另一方面，8位整数中`-5`的表示若放到更高的位数比如32位中，显然这就是一个正数了。
这个数显然就是`0111 1011` + `1000 0000`，即123 + 2^7 = 251。

以上几个数之间的关系整理如下， 最好能运用的滚瓜烂熟：
```text
8位整数的前提下、
-5 和 123 是差一个符号位。
-5 = 123 - 2^7
-5 和 251 是8位和更高位定义的同一个二进制表示， 251 = 123 + 2^7
```


### 为什么要用反码、补码
本来，如果可以愉快地使用易懂的原码表示整数并且进行运算，那么最好了。

但是人们发现用原码表示计算机中的整数并不是很方便。
这也主要是因为受到了计算机的机能限制。计算机的计算有两个特点。
第一，只计算加法，因此减去一个数其实是当做加上一个负数来计算的。
第二，算一个加法可以说是计算机中最最简单的操作，为了计算效率应该让电路设计的十分简单。

于是，用原码保存数据并进行计算的缺点出现了。由于第二个特点，我希望不要在电路上多余设计一个模块来分割判断符号。
同时，减去一个数是加上其相反数的操作。
如果使用原码，则做`1 - 1 = 0`的计算时，会发生以下情况：
```text
1是 0000 0001
-1是 1000 0001
两者相加得到 1000 0010 = -2
```
于是聪明的科学家尝试采用反码计算。如果用反码，上述计算可以变成：
```text
0000 0001 + 1111 1110 = 1111 1111 == -0
```
结果是`1111 1111`，别忘了这也是个反码所以转换成原码后我们发现其恰好是`-0`，从数学意义上来说已经得到正确答案了。

但是显然，使用反码也不是最佳方案。究其根本，反码的0对应了两种编码`0000 0000`和`1111 1111`。
我们当然希望数和编码时一一对应的。

于是，最终方案，补码登场。
用补码计算上述`1-1=0`时，由于-1的补码，是原码"真值部分取反+1"，是`1111 1111`：
```text
0000 0001 + 1111 1111 = 1 0000 0000
由于我们限定做8位整数的加法，因此截除首位1，得到0000 0000，这也恰好是0的补码。 
```
另一方面，看`-1 + -127`这个计算：
```text
1111 1111 + 1000 0001 = 1 1000 0000 => 1000 0000
```
这个计算正确结果应该是-128，绝对值超出了7个真值位可表达的范围，本来算溢出了。
而在补码形式中，`1000 0000`这个编码又不对应任何数。
（补码转回原码时要"真值部分-1取反"，可此处真值已经是0，不能再减小）

于是，将`1000 0000`这个编码定义为`-128`，一举两得。
一方面解决了`-0`问题，另一方面还扩大了一个可表示的数字范围。

## Python中的整数表示
Python中提供了int和long两种整数类型，但是并没有明确位数，这也就带来了很多比较迷惑的问题。

通常，在确定某个数字可以用n位带符号整数表示的时候，可以通过类似以下的办法来获得其二进制表达：
```python
def show_binary(num, bit):
    res = []
    for i in range(bit):
        res.append(num << i & 1)
    res.reverse()
    return ''.join(str(i) for i in res)
```
换言之，如果以上函数输出的二进制表达，首位符号与输入的num的实际符号不匹配，则说明出现了溢出的情况。

另一方面，从二进制去构造一个数的时候也要注意限制条件的位数。比如告诉你要构造一个32位整数的时候，
当遍历到第32位二进制位，就该意识到这个是个符号位。
如果其值是0还好办，如果是1，那么就该意识到要求的输出是个负数，不能直接`|= 2**31`（因为那样会变成更大的正数），
而是采用上面那条补码的性质，`-= 2**31`
上述可见`LC.137`作为练习题。

## Python中的浮点数表示
>参考https://zhuanlan.zhihu.com/p/58731780

计算机中所有数据都得用二进制存储。
整数数据总体来说比较方便理解，毕竟二进制和整数是一一对应的。

可带有小数点的浮点数该如何处理呢。

其实这和Python也没太大关系，而是所有计算机语言实现时，都统一采用了IEEE.754这个标准。
这个标准，其实就是一个二进制和浮点数之间互相映射的协议，大家都遵守。

这个标准中规定了两种类型的浮点数，占32位字节的单精度型(float)和占64位字节的双精度型(double)。

两者大同小异，下面默认以双精度型进行说明。

### 二进制转换浮点数
下面看这样一串64位二进制字符串：
```text
0100000000101001000101111000110101001111110111110011101101100100
```
顺便一提，如果这串字符串表示的是一个整数（因为是64位，还是个long型整数），那么他是
`4623252388170382180`。
转换规则很简单，首位是0说明其是一个正数，完了一位一位解析即可。

现在，我们要把这串字符串当成一个double型浮点数进行解析。
我们将其分成下面三个部分：
```text
0               (第1位)
10000000010     (第2位-第12位，共计11位)
1001000101111000110101001111110111110011101101100100    （剩余所有位）
```
为什么按照这样的长度分成这样三部分？这是IEEE规定的。

#### 每一部分的解析规则
这三部分分别表示了组成一个浮点数的三部分，分别是sign(符号位)，指数(exponent)，小数(fraction)。
我们只要从字符串中将这三个要素解析出来，然后进行简单的计算组合就可以得到浮点数的值。

先来说说三个部分分别如何解析。
首先，符号位，很好理解，和符号整型数一样，0代表正数、1代表负数。

然后，指数部分表示的是一个不带符号的整形数`c`，但`c`并不直接表示指数，而是取用`c - 1023`表示指数exponent。
其中，1023称为"指数偏移量"。
因为10位表示的不带符号的整数`c`的范围是`[0, 2047]`，所以此处`c-1023`即exponent的取值范围是`[-1023, 1024]`。
在最后计算结果的时候，乘以`2 ^ exponent`，这也是这部分被称为指数部分的理由。

最后，fraction表示的是一个范围在`[1,2)`间的小数。和指数部分一样，fraction中的二进制并不直接表示什么内容。
这52位的解析方式是这样的：
如果将其写作`m1 m2 m3 ... m52`，那么其表示的数是`sum(2 ** (-i) for i in {1...52} if mi == '1')`。
也就是说，第`i`位（注意i不是下标）如果是1，那么结果就加上`1 / (2^i)`。
显然，如果这52位都是1，那么得到的结果是`1 - 2**(-52)`，这是一个很接近1的数了。
另外别忘了定义的这个fraction是一个`[1,2)`之间的数，所以还要再加上1，才得到fraction。
没有加1的这部分小于1的数被称为尾数，也写做`m`。
根据上面描述，fraction部分的取值范围显然是`[1, 2-2**(-52)]`。

得到上面三部分的结果之后，最后计算总的结果，公式如下：
```text
res = (-1)^(sign) * 2^(exponent) * fraction
```
显然，当exponent最大，fraction也最大时，能取到double型数据的上限大约是`2*10^(308)`
当exponent最小，fraction也最小的时候，能取到double型表示的最小正数大约是`2*10^(-308)`
另外，当上述`c=0,m=0`的情况，即整个二进制表示都是0的时候，此时虽然数学值是`2^(-1023)`，但是为了和浮点数定义统一，这种情况定义为0。

#### 实际操作
好的，根据以上规则我们来实际演示一下如何解析。
再次重申一下，最初的64位二进制字符串，被分成了如下三部分：
```text
0               (第1位)
10000000010     (第2位-第12位，共计11位)
1001000101111000110101001111110111110011101101100100    （剩余所有位）
```
我们看到，符号位是0，说明这是一个正数。

接着解析指数部分。`10000000010`解析出来是`c = 1026`。
得到`exponent = c - 1023 = 3`。

再解析尾数部分。可以用下列代码解析：
```python
m, base = 0, 1
for c in s:
    if c == '1':
        m += 2**(-base)
    base += 1
```
运行上面代码得到`m = 0.56825`

接着拼凑各个部分：
```python
res = -1**(0) * 2**(3) * (1 + m)
```
也就得到了最终结果res是`12.546`。
即，上面这个二进制字符串如果被解析成double型数据，将会是12.546。

#### 解析用Python代码
```python
def bin2double(s):
    assert len(s) == 64
    sign, expo, frac = s[0], s[1:12], s[12:]

    sign = 1 if sign == '0' else -1
    expo = int(expo, 2) - 1023

    def str2mant(subs):
        m, base = 0, 1
        for c in subs:
            if c == '1':
                m += 2**(-base)
            base += 1
        return m

    frac = str2mant(frac) + 1

    return sign * 2**(expo) * frac
```

### 单精度型float以及浮点数的精度问题
以上例子以双精度型为例，其实单精度型大同小异。
只不过单精度型中，exponent和fraction的位数分别是8位和23位。

从上面的实操中可以看到，即使double尾数有52位，但是其表达的精度始终是有限的。
如果一个数的精细度低于`2^(-52)`这个量级，那么double也是能对其做一个近似而非准确的表达。
这也是浮点数大部分情况下都是近似而非准确表达的原因。
当然因为尾数的位数double比float要多，所以其精度自然也就高。

这种思想倒和量子力学有点像。我们的世界看似是连续的，其实是一个个非常小的普朗克单位的时间和能量叠加起来的。
在double的世界中，一个"普朗克"量是`2 ** (-52)`约等于`2.22 * 10^(-16)`。
所以用double表示数据时，只保证前15位有效数字，而第16位及以后只能做到部分精确。
类似的，float是前7位有效数字，往后是部分精确。

### 浮点数转换二进制
具体的算法这里先不提了，先给个工具：
>http://www.binaryconvert.com/convert_double.html
>这个网站可以基于IEEE协议，将某个小数转化成相应形式的二进制数据。

# Python3中的基础数据类型

Python(3)中的基础数据类型包括了int, float, str, tuple, list, set, dict这些。（注意，Python3中不再区分int和long）
这些基础数据类型的使用以及一般常用的接口早就滚瓜烂熟了，就不多说了。这章更多的是想说说这些基础数据类型的底层实现原理相关的一些内容。

## List的动态扩容机制

相比于传统数组，Python中的列表好用就好用在于，1.他可以装任意类型的数据， 2.他不用在声明时声明固定长度并且使用时不能超过。

第一点，因为Python中所有东西都是对象，所以列表只需要维护对象的指针，就可以做到装任意东西（事实上列表是一个指针的数组）。
第二点，则是因为Python的List的动态扩容机制了。动态扩容机制十分简单易懂，在标准CPython实现中是这样的：

空列表时，只申请保留一些列表meta信息的空间，大概在50-60字节。之后持续的append元素进来。
当长度到达4时，第5个进来前，扩容一倍，长度变成8。又来4个后，再扩容一倍，长度变成16。
之后第17个元素进来前，16扩容9个单位，变成25。之后每次扩容，扩容步长+1，所以整个需要扩容的长度形成的一个序列是：`0 4 8 16 25 35 46 58...`。

更具体的，上面==每一个单位，其实是一个指针的长度，64位系统的话就是8个字节==。所以，运行下面这段程序可以得到所示输出：

```python
import sys
lst = []
for _ in range(27):
	print(f'length = {len(lst)}, size = {sys.getsizeof(lst)}')
  lst.append(None)
  
'''
length = 0, size = 56
length = 1, size = 88
（略）
length = 4, size = 88
length = 5, size = 120
（略）
length = 8, size = 120
length = 9, size = 184
（略）
length = 16, size = 184
length = 17, size = 256
（略）
length = 25, size = 256
length = 26, size = 336
'''
```

另外，==所谓的“大小从m扩容n”的过程，其实是指在内存中申请一个全新的n大小的空间，然后将原空间中的m个元素复制到这片空间的前m个位置==。非常单纯的做法。

## Dict实现原理

我们知道，字典就是一个哈希表。

### ⭐️哈希表

来复习一下基础数据结构知识。哈希表是内存中一片连续的空间。但是和数组不同，哈希表的存储方式并不是从前往后一个个存储的。
构成一个哈希表，除了这片空间，还==需要指定一个哈希函数以及哈希冲突解决方案两个东西==。

哈希函数的作用，是将一个key映射到一个当前哈希表合法的index值。当然，通常`|key| >> |index|`，即key的数量总是很多的，于是就不可避免地出现哈希冲突，即不同的key经过哈希函数处理，得到的输出index相同。此时第二个要素，哈希冲突的解决法就发挥作用了。

哈希冲突的解决大的，可以分成内消解和外消解两种。我们先看内消解。

#### 哈希冲突的内消解

内消解，顾名思义，就是将哈希冲突在存储区内部想办法解决。通常的思路就是当哈希冲突，就找数组中其余还空着的空间，插入进去。
这种方法也被称为开地址法。
显然，开地址法还需要确定，找剩余空着的空间的策略，称为探查方式。下面是两种比较有代表性的探查方式：

- 线性探查法

  顾名思义，哈希冲突后，从冲突位置开始向右扫描，找到第一个空格就填入数据。
  线性探查虽然实现简单，但是由于总是遇到空格就填上，就会导致整个数组的前半部分比较拥挤，随着元素增加，探查时间将越来越长。

- 双散列探查法

  双散列探查，首先在原哈希函数的基础上，设置第二个辅助哈希函数。
  当哈希冲突发生时，即`hash(x)`下标出已经有值，此时将尝试把x放入`hash(x) + sub_hash(x)`中。

  双散列探查利用了第二个哈希函数，尽可能地减少线性移动以及哈希码的局部堆积。

#### 哈希冲突的外消解

- 桶散列（链表哈希）

  这里，线性空间中每一项不再是简单的单个value，而是values，values是发生哈希冲突的各个value形成的链表。
  桶散列中，只要不是脸特别黑，所有元素都哈希冲突了的话，性能就还可以。同时实现起来也很方便。
  值得一提的是，桶散列除了链表实现，每个哈希冲突的哈希值下的东西，除了链表，还可以用顺序表，或者一个小哈希表等方式实现。

- 外部溢出区

  这里，哈希冲突的解决方法就是不解决。发生哈希冲突后，就将这个元素给扔到外部溢出的区域中。外部溢出区可以是一个简单的顺序表，或者链表，或者又一个哈希表等等。

#### 平均检索长度

显然，评价一个冲突机制好不好，得看在这个机制下一个key可以再计算多少次之后找到相应的value。而具体的量化指标，就是称为平均检索长度（ASL）的指标。

这里我们先看检索一个key的情况。通过哈希函数，可以计算得到其在哈希表中的下标，然后检查下标中保存的key值是否与之匹配；若否，说明哈希冲突，则根据哈希冲突处理方法进行相应的下一步计算，以此类推。假设这个key在最终找到值或者认定哈希表中没有当前key的时候，进行了`c`次计算，即进行了`c`次key值匹配比较。

对于哈希表中的所有key，都会有一个`c`。另外还需要考虑哈希表中各个key是否会等概率地被检索。通常认为等概率。假设共有`n`个key。在这个情形下，这个哈希表中，检索一个key时需要的key值匹配比较次数的期望是

$ASL=\dfrac{1}{n}\sum_{i=0}^{n-1}c_i$

这也就是所说的这个哈希表的平均检索长度ASL了。

### 至于Dict…

其实以上哈希表是对标set讲的，因为只出现了value，没出现key。至于Dict，无非就是把上面用来哈希的输入数据，从value变成了key而已。另一方面，表中每一格除了key本身，还需要存储哈希值和value。

#### 内建hash函数、哈希值对数组长取余

上面叙述中，哈希函数到底是什么样的始终没有提。另外，我们直接假定哈希函数的输出就是下标了。实际上中间还有一些细节。
首先，Python中所有用到散列的地方，基于的哈希函数都是内建的`hash`函数。这个函数可以将Python中任意不可变对象（int, float, tuple, str等）都转换成一个整数，这个过程就是哈希函数本身的工作。

事情还没完，若尝试

```python
>>> hash('hello')
1618767383509003088
```

你会发现，raw的哈希值是很大的。我们不可能真的申请这么大一个数组来保存这个哈希值吧。所以通常，还需要用这个值对数组长度进行取余。而这个`hash(x) % len`才是最终放入数组的下标值。

> 内建hash函数内部到底怎么计算哈希值，目前我还没研究过。泛泛来说，哈希函数计算哈希值可以有折叠法、平方法、除余法、基数转换法等等、这些方法可以参考《Python数据结构与算法》的P.276。

#### 自动扩容机制

因为Dict的底层还是一个数组，那么和List一样，就面临着，随着保存的项越来越多，会导致数组长度不够用，此时就需要自动扩容。
Dict因为是哈希表，一般不会像List那样真的等到一点空间都没有再扩容，那样的话在那之前就会有太多哈希冲突发生了。

Python规定，Dict之类的底层是哈希表的东西，==在哈希表中有`2/3`容量被占据后进行扩容。扩容时现申请新的大空间，然后将原空间中的数据根据新空间的长度进行重新计算哈希值并填入==。
初始情况下，Dict底层哈希表有8个格子供容纳数据。当填入5个数据，试图填入第6个时，Dict发生第一次扩容。==扩容默认扩大当前实际已有元素的4倍==，即`5 * 4 = 20`个格子。

此后若继续填充，则会在尝试填入第14个数据时进行扩容，扩成80个格子。
当哈希表内数据超过50000个后，扩容倍数将从4倍变成2倍，以防止过度浪费内存。

注意，上面所说的一个“格子”，正如之前所说，其实是一个`hash(key), key, value`的三元组。所以一个格子的大小，其实是三个指针的大小，即24字节。

> 以上是书上的描述，实际上，模仿List扩容机制的实验，做了下实验发现好像又不是这样。仅供参考吧。

### 自定义类的哈希

在Python自带的数据类型中，只有不可变数据类型满足可以被哈希，即`hash(x)`不报错。
对于自定义的类，只要能实现`__hash__`这个魔法方法，那么`hash(instance)`的时候就去调用这个方法，返回值就行了。

另一方面，能不能哈希和是不是不可变对象之间不能画等号。内建类型只允许不可变对象被哈希，是考虑到哈希存储后，后续检索时如果对象本身改变了那么就无法检索到了，这个问题。而对于你自己的自定义类，系统不会考虑这一点。所以一定要注意自己的类如果想让其可以被哈希，就一定得保证其是不可变的（除非你确定他不会被拿去放到set，dict中去）




# Python中的语法糖

迭代器和生成器都是让我们自定义的可迭代对象。

## 迭代器
严格来说，实现了`__next__`方法的类都是迭代器类。
通常，迭代器还会实现`__iter__`方法。
比如
```python
class MyIterator:
    def __init__(self, n):
        self.n = n
        self.i = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.i == self.n:
            raise StopIteration
        self.i += 1
        return self.i
```

在说明上面这段代码之前，先来看看迭代器是怎么用的。通常，迭代器总是用在for循环中，即
`for item in XX`(`XX`是`MyIterator`的实例)。

现在深入看一下这句话。这句话的实质其实是`for item in iter(XX)`。其中`iter`是Python内置的迭代器化函数。
又因为`MyIterator`类实现了`__iter__`方法，所以上述语句等价于
`for item in XX.__iter__()`。这也就要求了`__iter__`返回的必须是一个迭代器对象。

接着，开始迭代的时候，其实python不断调用上述返回的迭代器对象的`__next__`方法直到`StopIteration`这个异常发生。

上面这个类，将两个魔法方法都实现了。因为实现了`__next__`，所以他是一个迭代器。
又因为实现了`__iter__`并且返回的是`self`，所以他可以直接被写在`for`循环中。

事实上完全可以外包一层，先写某个类实现`__iter__`，然后在这个方法中返回`MyIterator(10)`（MyIterator实现了`__next__`）。

但是这里显然没什么必要，可以将两个角色组合在同一个类中。

在运行一次for循环的过程中，`__iter__`只被调用一次，而`__next__`被调用n次直到遇到StopIteration。

## 生成器
生成器就简单多了。就记住两种。

第一种函数式生成器，只需要在函数中将本来遍历取出的写上yield关键字即可。
有了yield之后，函数就是一个生成器了。大多数时候yield不和return混用。

第二种是表达式式生成器，和列表生成式很类似，只不过用小括号。如`(i for i in range(10))`。

## 装饰器
装饰器：最基本的形式是一个返回函数对象的函数。
作用：构建通用的"函数增强模板"。

总之记住，当定义带修饰器的函数如下之后：
```python
@xxx
def yyy():
    # do some thing
```
接下来调用`yyy()`时，调用的实质上是：
```python
xxx(yyy)()
```
这里的`xxx`，既可以是一个简单的函数名，也可以是一个带参数的函数返回，还可以是一个类名等。

下面从零开始梳理一下装饰器。加入我们有函数`bar`。
下面我想给他增加一个功能，在运行bar的业务逻辑之前打印`bar is running`这条信息。
再不修改bar函数本身的代码的前提下，可以这么干：
```python
def bar():
    print('bar!')

def en_bar():
    print('bar is running')
    bar()

bar = en_bar
```

如果此时还有一个`foo`函数，需要加强类似的功能，一模一样写一个`en_foo`就略显繁琐。
一个可行的做法是：
```python
def bar():
    print('bar!')
def foo():
    print('foo!')

def logged(func):
    def en_func(*args, **kwargs):
        print(f'{func.__name__} is running')
        func(*args, **kwargs)
    return en_func

bar = logged(bar)
foo = logged(foo)
```
logged实质上就是一个装饰器，其本身是一个函数并且返回的是一个函数对象。这个函数对象就是一个通用的功能增强模板。
通常为了最大程度支持被增强的函数，内层的`en_func`通常接收任意参数即`*args, **kwargs`。

再省一步，将上述`x = logged(x)`给去掉，就可以使用装饰器的语法表达：
```python
@logged
def bar():
    print(bar)
```
这样定义出来的bar函数与上面等价。

以上就是最简单的装饰器了。

### 带有参数的装饰器
注意到在装饰器语法表达中，`@xxx`中的`xxx`其实是一个函数对象。
掰开揉碎讲，其逻辑过程是，当你给出`@xxx`来修饰下方定义的`yyy`函数后，运行`yyy`函数时其实是运行了`xxx(yyy)`。

比如上面的logged本身就是一个函数对象。
既然如此，我们就可以通过定义一个"装饰器的装饰器"，即返回的函数对象本身是一个装饰器，来实现带有参数的装饰器功能。

比如：
```python
def level_logged(level):
    def logged(func):
        
        def en_func(*args, **kwargs):
            print(f'[{level}]{func.__name__} is running')
            return func(*args, **kwargs)
        
        return en_func
    return logged

@level_logged(level='INFO')
def bar():
    print('bar!')
```

### 类装饰器
类装饰器指的是不是通过一个"返回函数的函数"实现，而是通过一个类实现。这就提供了更多灵活性给自定义装饰器。

具体来说，其实就是Python中类的魔法方法来实现类实例的函数化，即实现类中的`__call__`方法即可。
注意类定义时`__init__`方法需要接收被增强的函数对象作为参数。

就像上面所说的，`@xxx`修饰`yyy`函数时，执行`yyy(*args, **kwargs)`运行`yyy`函数就相当于执行`xxx(yyy)(*args, **kwargs)`。
当`xxx`是个类名时，显然`xxx(yyy)`就是在实例化，因此需要在`__init__`的输入参数中加上函数对象。

### functools.wraps
上面讲过，修饰器表达`@xxx`下面定义`yyy`函数，其实是代替了`yyy = xxx(yyy)`这句话。
而这个语句有一个小问题，就是原先的`yyy`函数被覆盖了。虽说函数功能通常会被完整地抄到`xxx`的定义中，但是其他一些元信息会丢失。

比如定义如下：
```python
def logged(func):
    def wrapper(*args, **kwargs):
        # do some thing
        func(*args, **kwargs)
    return wrapper

@logged
def bar():
    print('bar!')
```
此时如果查看`bar.__name__`，看到的将会是`wrapper`。因为实质上bar已经被狸猫换太子了。

如果想要留下原来的元信息，可采用如下方法：
```python
from functools import wraps
def logged(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        # do some thing
        func(*args, **kwargs)
    return wrapper
```

### 多重嵌套装饰器
一句话。
```python
@a
@b
@c
def f():
```
等价于
```python
f = a(b(c(f)))
```


# ⭐️Python 多线程与GIL锁

## 进程和线程
一般意义上，我们知道进程是运行的程序的抽象。为了运行程序除了程序本身他还要维护很多其他信息。

在任意时刻一个CPU上只能运行一个进程，而由于CPU可以高速切换不同进程实例，所以即便只有一个CPU，对于用户来说似乎还是多进程在运行。

线程则是对进程的进一步轻量化。
一个进程可以包含多个不同的线程。同一个进程下不同线程间可以享用同一块地址空间，相同的全局变量等等。相对的，每个线程也有一些自己独有的信息，比如
程序计数器，堆栈等等。

进程和线程可维护的数据区别如下：
![](https://pic.leetcode-cn.com/1612667575-cWbpmB-os8-7.png)

需要注意的是，虽然不同线程持有的信息可以分化，但是这种分化的独立性并不是很强（至少比不同进程间的分化弱）。
因此，一个线程理论上是可以读取、写入甚至删除另一个线程的所有信息。
这一块需要程序员自己控制，这也是为什么线程安全如此被重视。

相对的，线程-进程关系和进程-OS关系其实很像，但是进程间独立性更强。
对于进程间共享的内存内容，一定是只读的；而可写的内存内容，一定不是共享的。

## 一般意义上的多线程
一个操作系统可以有多个进程，一个进程又可以有多个线程。
而每个CPU在任意一个瞬间只能运行一个进程。这句话更具体一些，其实每个瞬间一个CPU只能运行一个进程的一个线程。

现代操作系统的线程调度通常是可以跨核的。即如果你有多个CPU，那么就可以真并行的运行同一个进程的几个不同线程。
>这个问题其实还牵扯到线程的实现模式。这里不深究，还需要进一步学习。


## Python的多线程与GIL锁
一句话说Python，但是实际上Python有好多种不同的实现。
其中最著名也是官方的实现就是基于C语言的实现即CPython。

CPython的一个特点就是，所有程序都通过Python解释器这个进程执行，而且其规定了，任意一个时刻Python解释器只能执行一个线程。
这个其实就可以完全类比单CPU执行多进程的场景。虽然看起来是并发的，但是只是CPU切换得快，任意一个时刻都只有单个进程在跑。

而保证任意一个瞬间只有一个线程在跑的机制，就是GIL锁。
使用Python解释器运行一次Python代码时，只会产生一个GIL锁。
换句话说，Python官方使用了最粗暴的机制保证了线程安全，即通过全局锁，只让一个线程运行。

具体来说，Python解释器运行步骤如下：
```text
1. 将GIL锁给到某个线程
2. 将上下文切换到对应线程
3. 运行
4. 结束运行，修改线程状态
5. 拿回GIL锁
```
在拿回GIL锁之前，线程状态可能会被修改为就绪态或者阻塞态。
如果是阻塞态，说明此时线程要进行I/O操作，而在其进行I/O操作的同时，拿回GIL锁后的解释器可以着手进行另一个线程的运行。

因此，对于I/O密集型任务如Web访问、文件读取等，即线程经常会变成阻塞态的情况，Python的多线程机制可以起到一定的促进效率的作用。
但是对于CPU计算密集型任务，线程即使运行完成也只是变成就绪态，还要等下一次运行，那么这其实和单线程没有本质区别，因此发挥不了很大作用。

## 简单的总结
在CPython实现的前提下，如果想要实现真正的多线程运行，一种解决办法是从更高层级进行并发，比如多进程。
比如利用Python中的`multiprocessing`库可以进行多进程的并发，每个进程相当于一个独立的解释器，因此可以实现真多线程。

另一种可能的解决办法是使用C语言编写线程程序并且通过链接库的形式在Python代码中引用。
据说解释器会检测到某段程序是C而非Python，从而在这段程序作为线程执行的时候不上GIL锁。
这一块并不熟悉就不多说了。

总的来说，泛泛的多线程因为可以做到 1.CPU并行 和 2.IO并行 所以提高了程序运行的效率。
但是Python由于GIL的原因，即便你的CPU有100个核，只要只有一个解释器进程，每个时刻就只会有一个核在跑一个线程。
因此Python是放弃了CPU并行的。但是IO并行的特点仍然存在。

## 为什么要GIL？
一个简单的知见：Python开发初期用了很多C/C++库，而这些库都是线程不安全的。
因为Python的初衷是要简单快速地编程，因此就粗暴地利用一把全局锁GIL强行实现了线程安全。
这样后续的开发者就不需要再考虑线程不安全的风险。当然这么做的代价就是如上所说，其通过多线程可以提升的效率受到限制。

## Python协程

笼统地说，协程是线程内部的调度单位，又称为“用户级线程”因为协程层面的讨论都不涉及用户态和内核态的切换。通常可以用生成器来理解协程的运作流程。比如下面这段代码

```python
def generator():
    for num in range(3):
        print(f'Yielding Number {num}')
        yield num

def main():
    print('Start of Main...')
    for i in generator():
        print(f'Number is {i}')
    print('End of Main...')

main()
'''
STDOUT:
Start of Main...
Yielding Number 0
Number is 0
Yielding Number 1
Number is 1
Yielding Number 2
Number is 2
End of Main...
'''
```

上述代码很好理解，但是如果仔细分析一下运行流程，到底是怎样的？
首先，`main`函数开始执行后，来到了`for i in generator`语句。此时要求生成器对象返回一个数字。这时必然要转入`generator`函数运行。此函数开始执行后，先进入`num == 0`的第一轮循环，`yield`了`0`。但是请注意，此时generator函数执行就被挂起了，主线程又回过头去执行`main`函数。

换言之，==`generator`函数和`main`函数的两个执行逻辑可以视为同一个线程中的两个协程==。假设`main`和`generator`的两个协程称为A和B，最开始执行的是协程A，当要生成器返回数字时挂起协程A转到协程B。当协程B遇到`yield`，可以返回数字时，记录数字并线程再次回到协程A。

于是可以总结协程的几个特点：

- 协程就像是C语言中的todo关键字的作用，可以改变以往程序一条路走到底的执行方式，有秩序地在多个协程间反复横跳。
- 线程内的多个协程在任意同一时刻只有一个执行，其余被挂起。==所以本质上，协程是串行的，只不过提供了时间上非线性串行的可能==。
- 协程的切换仍需要切换寄存器上下文、栈等内容，但是和线程切换不同，协程切换不需要进入内核态，全程在用户态进行。因此协程的切换也最快。

更多协程与线程、进程的比较可以参考操作系统篇笔记。

### 为什么协程切换不用进入内核态

这个问题其实可以反过来问，即为什么线程切换需要进入内核态。
首先应该明确，线程切换和用户内核态转换之间并不是对等关系。==准确的定义应该是，当程序发生了系统调用，就需要从用户态进入内核态；系统调用完成后就从内核态返回用户态==。

线程的切换，发生的原因有很多，比如时间片用完，或者外部中断之类的。无论发生的原因是什么，触发线程切换的函数是实现在内核中的，是一个内核程序，因此调用这个函数是系统调用，因此会发生用户态->内核态的过程。

而协程的切换，是由程序员自己的代码触发的，比如上面的`yield`关键字。这不属于内核程序，因此自然就不用进入内核态。


# ⭐️Python垃圾回收机制
## 引用计数
在比较底层的语言比如C/C++中，程序员有时需要手动对变量进行整理，将不再使用的变量手动回收。
这里，回收是指去除变量名和数据之间的联系，并且将保存数据的那一部分内存空间清空。

在Python中显然不需要这么干。这主要是因为Python内部采用了引用计数机制来实时地维护所有程序中的变量。

所谓引用计数，是PyObject类的固有属性。我们知道Python中所有的对象，都是基于object这个基类，而这个类其实又是由C语言
实现的PyObject这个类。这个类中带有一个属性`obj_refcnt`进行引用计数的维护。

那么引用计数具体怎么用。==具体来说，当一个对象在代码中发生如下事件时，其引用计数会加一==：

- ==被创建（对于不可变对象来说是初次建立引用关系），如 a = 1==
- ==被引用，如 b = a==
- ==作为参数被传入函数==
- ==被放入某种容器数据结构比如list,dict等==

反之，如果发生如下事件则引用计数减一：

- ==被del==
- ==离开作用域==
- ==所在容器被销毁或者被从容器中删除==

使用内置的`sys.getrefcount`函数可以查看某个对象当前的引用计数。因为任意一个对象传入这个函数时根据规则计数都会+1，所以实际上返回的结果总是比实际的计数大1。

## 垃圾回收
上面我们知道了每个对象都自带引用计数。

当某个对象引用计数减小为0，那就说明已经没有任何指针指向这个对象，此时可以放心地回收这部分空间。
这就是基于引用计数进行垃圾回收的原理。

利用引用计数进行垃圾回收，好处在于逻辑很简单，同时具有实时性。但是也有一些缺点，比如需要额外维护引用计数，当容器引用层级较深时可能遍历其中对象比较耗时。
然而其最大的一个缺点是无法处理循环引用。这也意味着Python不能只用引用计数作为垃圾回收的依据。

### 循环引用
顾名思义，当a,b两个对象互相引用对方就是循环引用。循环引用也包括自己引用自己的情况。
这么搞会出什么问题？

首先，两个对象创建后的引用计数都是1，而在互相引用对方之后都变成了2。
此时如果del掉a,b两个对象名，那么计数减1。然而他们互相引用形成一个环，这个环外部没有对他们的任何引用但是因为这个环的存在，他们的计数无法减为0。
因为计数不能减为0，所以不能回收，因为不能回收，所以就发生了内存泄漏。

## 解决方案：标记-清除算法
如上所述，单纯参考引用计数进行垃圾回收无法解决循环引用的问题，因此这里导入标记-清除算法作为补充。

这个算法的描述见参考
>https://zhuanlan.zhihu.com/p/83251959

简单来说，其实整体的引用可以看做一个以对象为节点，以引用为边的有向图。
标记-清除算法，顾名思义，分成标记和清除两个阶段。
标记阶段，GC会将所有“可达”和“不可达”对象通过算法标记区别。清除阶段，则是将那些“不可达”的对象作为垃圾进行回收。

具体工作流程是这样的：

1. 针对所有对象，先为其赋予一个`gc_ref`变量作为GC过程中使用的引用计数变量（因为现在只是在GC，不能去修改真的引用计数值），初始化为当前对象的引用计数值`ref_count`。初始化两个区域（本质上是两个链表），一个是可达的Object to Scan区，另一个是不可达的unreachable区。

2. 遍历每个对象，==将其引用的所有对象的`gc_ref -= 1`。注意，是将其引用的所有对象，而不是其自己的`gc_ref`减去1==。上述两个过程可以描述为下面图的变化过程：

   ![](https://pic1.zhimg.com/80/v2-0d5071093adaa02bc03fa3dfd91aa5bc_1440w.jpg)

   ![](https://pic2.zhimg.com/80/v2-d7314ead6b303f08a91687577c045585_1440w.jpg)

   3. 将所有`gc_ref`归零的对象移入unreachable区。

   4. ==从`gc_ref`尚未归零的对象出发进行图扫描。对于可以通过扫描到达的对象，即使其`gc_ref`归零并且被移入了unreachable区，还是将其移会object to scan区==。如图：

      ![](https://pic3.zhimg.com/80/v2-6fd40c055a6633c654acaf05f472c1b2_1440w.jpg)

   5. 此时，所有对象就被正确分成了可达与不可达两类。将不可达的那些进行GC即可。`gc_ref`这个GC用各个对象的临时变量可以回收。

## 分代回收
分代回收并不是一个独立的方案，可以看做是对标记-清除以及其他一些方法的优化。

由于程序的局部性，内存中的对象通常有80%-90%都是"短命"的，即被使用没多久后就被回收。
分代回收机制将内存块分成0，1，2三个世代。数字越大的世代，其对应的内存块越长寿。长寿的定义是，经历过的gc扫描次数多。
比如某个0世代的内存块经历一次gc扫描后没被回收，那么他就被归入1世代；同理，1世代再经过一次扫描仍然健在，就归入2世代。

显然，长寿的内存块，经历了这么多次gc扫描都没被当垃圾回收，其是垃圾的概率就相应小。
在这个前提下，==每次gc扫描可以优先扫描小的世代，从而提高扫描效率，这也是分代回收机制的核心所在==。

更具体的，分代回收为每个世代设置一个threshold，并且开始GC。
每当某个世代新增内存块数目超过threshold之后，gc就开始扫描这个世代以及所有更小世代，进行垃圾回收。

# 其他

## Python2和Python3的区别

这个问题一下子就能想到的两个答案

1. print从语句变成了函数
2. 定义字符串时从Python2中混乱的各种编码格式，统一成了Python3中的Unicode

除此之外还有呢？下面再来补充几个

3. 迭代器相关

   ```
   Python2中一些内建的函数比如range, map, zip, dict.keys(), dict.items()等都是直接返回列表的，而Python3中这些东西返回的是迭代器。显然迭代器更好一些。
   此外，Python2的迭代器要求类实现next方法，而在Python3中，方法名变成了__next__，即，将迭代方法定义为魔法方法了。
   ```

4. nonlocal关键字

   ```
   这点确实今天才知道。。
   nonlocal关键字用于函数内部的内函数中。内函数开头处，若用nonlocal修饰一个变量x，则表明这个x不取用内函数内部的局部定义域，而取用外部的变量值。
   另一个类似的，声明全局变量的global倒是2，3里都有。
   ```

5. True和False的根本

   ```
   这点也是今天才知道。Python2中的True和False，本质上是两个解释器范围的全局变量。既然是全局变量，如果在代码中对他们的值进行改变，那么整个代码中的True或者False的意义就会改变，这很不合理吧。
   所以Python3中，True和False正式称为两个关键字，代表bool类型的值。
   ```

6. 程序文件的默认编码

   ```
   依稀记得Python2的脚本中，如果你什么都不干而在注释中直接写中文，是会报错的。因为Python2默认程序本身的编码格式是ASCII。
   Python3中默认编码改成了UTF-8，所以可直接写中文了。
   默认编码格式用  sys.getdefaultencoding()  查看。
   ```

7. 格式化输出字符串的形式

   ```
   Python2中还是类似于 'Name: %s' % name
   Python3中则引入了format函数，更新的3.7还支持了f-string。
   ```

8. 整数除法

   ```
   Python2中，整数除法沿用了其他C系语言的做法，即 1/2=0
   而Python3中，自动会转浮点数，所以 1/2=0.5
   ```

有这么多条差不多够了吧。其他还有一些，就不写了。

## 【TODO】实例方法、类方法、静态方法



## `__new__`和`__init__`的联系与区别

先来说一下两个方法定义的一些规矩。
`__init__`见得太多了，它的第一个参数必须是`self`，代表的是实例自己。剩余参数可以由程序员自己指定。另外`__init__`方法可以但没必要返回任何东西。
另一方面，==`__new__`方法的第一个参数必须是`cls`。这也意味着`__new__`方法其实是一个类方法（类方法也是静态方法的一种）而不是普通的实例方法==（当然这么解释有点本末倒置，因为`cls`只是一个参数名，但是惯例上，第一个参数是`cls`的那就是类方法啦）。==同时，`__new__`方法还必须返回一个对象==。至于这个对象是什么。下面会说。

总体而言，下面是一个`__new__`和`__init__`定义的例子：

```python
#!/usr/bin/env python
class Person:
    
    def __new__(cls, *args, **kwargs):
        return_obj = super(Person, cls).__new__(cls)
        return return_obj

    def __init__(self, name, age):
        self.name = name
        self.age = age

if __name__ == '__main__':
    p = Person('Frank', 21)
```

上述代码可以按照预期正确运行。当在主体代码中调用`Person`类试图创建一个示例，其步骤是这样的：

1. 在创建示例之前，调用`Person.__new__`方法。因为这是一个类方法，所以在创建示例前就可以调用了。根据上面要求，这个方法会返回一个对象。这里的代码返回的，是Person父类的new出来的实例对象。因为没有显式指定Person父类，所以其继承与基类object类。
2. ==`__new__`中返回的对象，会被赋值给`__init__`的`self`参数，将其作为此次创建Person类实例的返回==。`__init__`中可以对这个实例进行进一步的初始化如定义属性，给各个属性赋初值等。

正如两个方法的名字所暗示的那样，`__new__`才是真正的创建方法，而`__init__`是初始化方法。两者的区别主要在于：

|                | `__new__`                                                    | `__init__`                                    |
| -------------- | ------------------------------------------------------------ | --------------------------------------------- |
| 调用时机       | 在实例创建前                                                 | 在示例创建后，接受`__new__`返回的内容作为self |
| 方法性质       | 类方法                                                       | 示例方法                                      |
| 是否可以不定义 | 可以，若不定义`__new__`方法，则自动调用父类的该方法，也就是`super(Class, cls).__new__(cls)`就是上面代码中干的事。 | 一般类必须要有`__init__`                      |

### `__new__`到底有什么用

一直以来的开发都只用了`__init__`，那么为何还需要有个`__new__`呢？
一些场景，比如想要实现单例模式的时候就可以用`__new__`。

> 单例模式
>
> 通常一个类通过多次实例化可以获得多个不同的互相独立的实例。单例模式则是指多次实例化同一个类，得到的实例始终只有一个。这样，可以在不同的地方实例化同个类来访问同一个实例，实现数据的共享等功能。

```python
class Singleton:
	def __new__(cls):
		if not hasattr(cls, 'instance'):
      cls.instance = super(Singleton, cls).__new__(cls)
    return cls.instance

obj1 = Singleton()
obj2 = Singleton()
print(obj1 is obj2)
```

按照一般思路,`obj1`和`obj2`是两次实例化得到的实例，最后一行肯定输出False。但是实际上，`__new__`方法在实例生成前被运行，而这个方法内部，又将返回的实例对象每次都指向了类`Singleton`内部保存的instance属性（第一次因为还没有，所以会用`Singleton`的父类的`__new__`创建一个）。
这就导致，无论你调用多少次`Singleton()`尝试去示例化这个类，最终拿到的实例都是同一个。