自我介绍

平常有用过transformer架构吗？

使用transformer架构做翻译的时候，你觉得对数据有什么要求？

你的研究中有对transformer做什么改动吗？取得什么样的效果？
（提了下崔哥的mixed-attention 结果还继续问了…我也不熟悉啊）

你说local attention，local attention相比于普通attention有什么区别？为何性能更好？

你有什么想问的？