## 题目描述
有个内含单词的超大文本文件，给定任意两个不同的单词，找出在这个文件中这两个单词的最短距离(相隔单词数)。如果寻找过程在这个文件中会重复多次，而每次寻找的单词不同，你能对此优化吗?

示例：
```
输入：words = ["I","am","a","student","from","a","university","in","a","city"], word1 = "a", word2 = "student"
输出：1
```

提示：
```
words.length <= 100000
```

### 解法1 哈希表 + 二分查找
一个最简单粗暴的思路。
首先用一个`defaultdict(list)`将所有单词以及其对应的所有下标形成的列表，用哈希表维护起来。
由于这个过程可以在从左到右的一次遍历中实现，所以列表一定是有序的。

接着找到word1和word2分别对应的两个有序的下标列表，计算这些下标中彼此间最接近的两个即可。
虽然双指针遍历也可。但是由于是有序的，所以还可以用二分优化。

即，以较长的那段为被搜索列表，然后依次代入另一个列表中的每个下标，寻找`pos = bisect.bisect(indices, i)`。
然后计算`indices[pos] - i`以及`indices[pos-1] - i`即可。注意判断pos是否越界。

值得一提的是，这题官方给出的解法是一次遍历，总复杂度是O(n)。
而上述算法的复杂度应该是O(n + xlogy)，其中x和y表示word1和word2的出现次数。
找理由应该是官方算法更快，但是实际上还是我这个更快…就是我这个用了额外的空间O(n)。